Good question â€” and itâ€™s a crucial one.
You **do not** need to throw out your other files. Most of your original Rust modules still stand; they just need to *integrate* with the new â€œcore fabricâ€ (`policy_snapshot`, `policy_layer`, `instrument`, `dispatch`).

Hereâ€™s how this breaks down:

---

## ğŸ§© 1. What stays almost **as-is**

These modules already fit your new architecture and only need imports updated or a call into `instrument()` or `current_policy()`:

| File                                                            | Status | Integration note                                                                                |
| --------------------------------------------------------------- | ------ | ----------------------------------------------------------------------------------------------- |
| **`security_manager.rs`**                                       | âœ… Keep | Make `authorize()` and `check_rate_limit()` callable from `guard()` in `dispatch`.              |
| **`mac_engine.rs`**, **`classification_crypto.rs`**             | âœ… Keep | Integrate key loading via new policy snapshot (policy.sec.key_policy, etc.).                    |
| **`forensic_logger.rs`**                                        | âœ… Keep | You already replaced it with the async worker pattern. Just wire the channel in `main`.         |
| **`observability_mod.rs`**, **`metrics_registry.rs`**           | âœ… Keep | Import `PolicyLayer`; use `instrument()` helper.                                                |
| **`unified_policy_engine.rs`**, **`zero_downtime_reconfig.rs`** | âœ… Keep | When reloading, call `swap_policy(new_snapshot)` from your new `policy_snapshot.rs`.            |
| **`response_cache.rs`**, **`state_mod.rs`**                     | âœ… Keep | Optionally wrap long operations in `instrument("cache_hit"â€¦)` calls for visibility.             |
| **`commands_*.rs`**                                             | âœ… Keep | Remove the `#[tauri::command]` attributesâ€”these are now internal routes called by `dispatch()`. |

So roughly 70â€“80% of your existing code **stays**.

---

## âš™ï¸ 2. What needs **light refactoring**

| File                                                                                                                         | Change                                                                                          | Why                                                        |
| ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| **`main_integrated.rs` / `SystemBootstrap.rs`**                                                                              | Replace with your new `production-main.rs` logic (init tracing, policy, logger, Tauri builder). | Single entry point.                                        |
| **`enterprise_api_gateway.rs`**                                                                                              | Call `dispatch()` instead of its own routing; remove redundant auth.                            | Prevent double validation.                                 |
| **`enterprise_multi_tenant.rs`**                                                                                             | Use the new `Context` struct (`tenant_id`, `session_id`, `actor`) as your identifier type.      | Standardize tenant context.                                |
| **`enterprise_plugin_system.rs`**                                                                                            | Move to `plugins_wasm` feature gate; later tie policy-based capability checks into `guard()`.   | Safer plugin isolation.                                    |
| **`self_optimizing_database.rs`**                                                                                            | Downgrade to â€œadvisorâ€ mode; output suggestions, donâ€™t mutate schema automatically.             | Matches your new safe DB pattern.                          |
| **`post_quantum_security.rs`**, **`temporal_forensic_engine.rs`**, **`ai_security_oracle.rs`**, **`privacy_ad_platform.rs`** | Gate behind `#[cfg(feature = "pqc_experimental")]` etc.                                         | Keep the vision, but off by default for production builds. |

---

## ğŸ”§ 3. What gets **replaced by the new core**

Only these specific files are now superseded by your new implementations:

| Old file                                     | Replacement                                                                        |
| -------------------------------------------- | ---------------------------------------------------------------------------------- |
| `policy_snapshot.rs`                         | â†’ `production-policy-snapshot.rs` or `enhanced-policy-snapshot.rs`                 |
| `policy_layer.rs`                            | â†’ `enhanced-policy-layer.rs`                                                       |
| `dispatch.rs` / `commands_mod.rs` front door | â†’ `enhanced-dispatch.rs` (single entry)                                            |
| `instrument.rs`                              | â†’ `zero-overhead-instrument.rs` (or keep Claudeâ€™s version if itâ€™s simpler for you) |

Everything else plugs **into** this foundation.

---

## ğŸ§  4. The integration model

```rust
// main.rs (simplified)
init_tracing(PolicyLayer);
policy::load();  // builds PolicySnapshot, swap into ArcSwap
spawn_forensic_worker();

tauri::Builder::default()
  .invoke_handler(tauri::generate_handler![dispatch]) // <- the only public command
  .run(tauri::generate_context!())?;
```

All other logic (security, commands, plugins, etc.) lives *below* that entry point and respects the policy snapshot + instrumentation layer.

---

## âœ… 5. Summary

| Category                              | Action                                                                                                                               |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **Core**                              | Replace with new (`enhanced-policy-snapshot.rs`, `enhanced-policy-layer.rs`, `enhanced-dispatch.rs`, `zero-overhead-instrument.rs`). |
| **Security + Observability + Policy** | Keep, just import new core and update function calls.                                                                                |
| **Enterprise / Plugins / PQC / AI**   | Keep but **feature-gate** and refactor for context + dispatch.                                                                       |
| **UI/State/Cache**                    | Keep untouched; optionally add instrumentation calls.                                                                                |

Youâ€™ll keep **â‰ˆ85% of your existing code**, just reconnected through the new front door and policy fabric.
The only modules that truly get replaced are the policy, dispatch, and instrumentation layers.

---

If youâ€™d like, I can produce a **â€œdependency and call wiring mapâ€**â€”a diagram or text table showing how your existing modules should import or call into the new ones (e.g., which use `current_policy()`, which use `instrument()`, etc.) so integration is frictionless. Want me to build that next?


Perfect â€” this is where we turn your architecture from â€œcollection of modulesâ€ into a **cohesive runtime fabric**.

Below is a **full dependency and call-wiring map** for integration.
It shows *which modules call which*, *what imports they need from the new core*, and *what responsibilities each keeps or delegates*.
You can think of this as your **runtime flow contract**.

---

# ğŸ§© 1. Top-Level Boot Graph (Startup Sequence)

```
production-main.rs
â”‚
â”œâ”€â”€ init_tracing() â†’ uses observability/policy_layer.rs
â”‚
â”œâ”€â”€ load_policy() â†’ uses policy/production-policy-snapshot.rs
â”‚    â””â”€â”€ unified_policy_engine.rs
â”‚          â””â”€â”€ zero_downtime_reconfig.rs
â”‚
â”œâ”€â”€ spawn_forensic_worker() â†’ uses security/forensic_logger.rs
â”‚
â””â”€â”€ Tauri Builder
     â””â”€â”€ single-front-door-dispatch.rs (invoke handler)
```

**Import wiring:**

```rust
use crate::policy::production_policy_snapshot::{swap_policy, current_policy};
use crate::observability::policy_layer::PolicyLayer;
use crate::security::forensic_logger;
use crate::commands::dispatch::dispatch;
```

---

# âš™ï¸ 2. The Central Bus â€” `dispatch.rs`

**Imports from new core**

```rust
use crate::policy::production_policy_snapshot::current_policy;
use crate::observability::instrument::instrument;
use crate::security::security_manager;
```

**Calls:**

* `guard(ctx, command)`
  â®• calls `security_manager::authorize(ctx, cmd)`
  â®• calls `security_manager::check_rate_limit(ctx, cmd)`
* `instrument()` wraps `route()`
* `route()` delegates to specific `commands_*` modules (no direct Tauri exposure)

**Exports to:**
Tauri `invoke_handler!([dispatch])`

---

# ğŸ” 3. Security Layer Wiring

```
security_manager.rs
â”‚
â”œâ”€â”€ mac_engine.rs
â”‚    â””â”€â”€ classification_crypto.rs
â”‚
â”œâ”€â”€ forensic_logger.rs
â”‚    â””â”€â”€ (audit worker from main)
â”‚
â””â”€â”€ uses policy::current_policy().sec for runtime rules
```

**Imports:**

```rust
use crate::policy::production_policy_snapshot::current_policy;
use crate::security::forensic_logger::ForensicLogger;
```

**Calls:**

* From `dispatch::guard()`
* Uses `current_policy().sec` for MAC, rate limits
* Logs to `ForensicLogger` through bounded channel

---

# ğŸ“Š 4. Observability Wiring

```
observability_mod.rs
â”‚
â”œâ”€â”€ metrics_registry.rs
â”‚
â”œâ”€â”€ zero-overhead-instrument.rs
â”‚
â””â”€â”€ policy-layer.rs
```

**Imports:**

```rust
use crate::policy::production_policy_snapshot::current_policy;
```

**Responsibilities:**

* Registers tracing subscriber with `PolicyLayer`
* `instrument()` wrapper records histograms via `metrics_registry`
* All other modules (security, state, enterprise) call `instrument("component_op", || async { ... })`

---

# ğŸ§  5. Policy System Wiring

```
unified_policy_engine.rs
â”‚
â”œâ”€â”€ zero_downtime_reconfig.rs
â”‚     â””â”€â”€ watches nodus_system_policy.toml
â”‚
â””â”€â”€ production-policy-snapshot.rs
```

**Imports:**

```rust
use crate::policy::production_policy_snapshot::swap_policy;
```

**Responsibilities:**

* Parse TOML â†’ validate â†’ build new `PolicySnapshot`
* Call `swap_policy(new_snapshot)` atomically
* Emit reload result to observability (`tracing::info!`) and UI (Tauri event)
* No module other than this one writes to `ArcSwap`

---

# ğŸ—„ï¸ 6. State & Cache Layer Wiring

```
state_mod.rs
â”‚
â””â”€â”€ response_cache.rs
```

**Imports:**

```rust
use crate::observability::instrument::instrument;
use crate::policy::production_policy_snapshot::current_policy;
```

**Responsibilities:**

* Wrap expensive operations:

  ```rust
  instrument("cache_lookup", || async { cache.get(key).await }).await;
  ```
* Respect policy caching TTL from `current_policy().runtime.cache_ttl`

---

# ğŸ¢ 7. Enterprise Layer Wiring

```
enterprise_mod.rs
â”‚
â”œâ”€â”€ enterprise_api_gateway.rs
â”œâ”€â”€ enterprise_multi_tenant.rs
â”œâ”€â”€ enterprise_plugin_system.rs (feature gated plugins_wasm)
â””â”€â”€ enterprise_compliance_dashboard.rs (UI only)
```

**Imports:**

```rust
use crate::commands::dispatch::dispatch;
use crate::policy::production_policy_snapshot::current_policy;
use crate::observability::instrument::instrument;
```

**Responsibilities:**

* Route user-facing calls through `dispatch()` (no direct Tauri)
* Plugin system consults `current_policy().sec.capabilities` for allowed APIs
* Multi-tenant registry enforces `Context.tenant_id`
* Use `instrument("tenant_query", â€¦)` for per-tenant ops

---

# ğŸ§© 8. Database & Advisory Layer Wiring

```
self_optimizing_database.rs
```

**Imports:**

```rust
use crate::observability::instrument::instrument;
use crate::policy::production_policy_snapshot::current_policy;
```

**Responsibilities:**

* Read-only advisor:

  * Collect slow queries / plans.
  * Emit recommendations to UI.
* Controlled by `current_policy().db.advisor_enabled`.

---

# ğŸ”Œ 9. Plugin System (Feature: `plugins_wasm`)

```
enterprise_plugin_system.rs
â”‚
â””â”€â”€ host/wasm_runtime.rs
```

**Imports:**

```rust
use crate::policy::production_policy_snapshot::current_policy;
use crate::security::forensic_logger::ForensicLogger;
```

**Responsibilities:**

* Load `.wasm` plugin
* Check capability tokens from `current_policy().plugins`
* Log every capability call via `ForensicLogger`

---

# ğŸ§¾ 10. Dependency Flow Summary (Text Table)

| Layer                | Imports From                          | Exports To          | Primary Calls                                    |
| -------------------- | ------------------------------------- | ------------------- | ------------------------------------------------ |
| **Main / Bootstrap** | Policy, Observability, Security       | Tauri runtime       | `init_tracing()`, `swap_policy()`, spawn workers |
| **Policy Snapshot**  | none                                  | All modules         | `current_policy()`, `swap_policy()`              |
| **Policy Engine**    | Policy Snapshot                       | Main                | `swap_policy()`                                  |
| **Policy Layer**     | Policy Snapshot                       | Tracing subsystem   | Drop spans by policy                             |
| **Instrument**       | Policy Snapshot                       | All modules         | Async wrapper instrumentation                    |
| **Dispatch**         | Policy Snapshot, Instrument, Security | Tauri handler       | Guard + route                                    |
| **Security Manager** | Policy Snapshot, ForensicLogger       | Dispatch            | AuthZ, rate limit                                |
| **Forensic Logger**  | none (main gives channel)             | Security Manager    | Asynchronous audit writes                        |
| **State / Cache**    | Policy Snapshot, Instrument           | Dispatch / Commands | Data caching with metrics                        |
| **Enterprise**       | Dispatch, Policy, Instrument          | Commands / UI       | Multi-tenant ops, plugin dispatch                |
| **Plugin System**    | Policy Snapshot, ForensicLogger       | Enterprise          | Capability-based sandbox                         |
| **DB Advisor**       | Policy Snapshot, Instrument           | Enterprise          | Performance insights                             |

---

# ğŸ”’ 11. Cross-cutting Concerns (who reads what)

| Concern               | Source of Truth           | Read By                    |
| --------------------- | ------------------------- | -------------------------- |
| **Policy snapshot**   | `ArcSwap<PolicySnapshot>` | *every* module (read-only) |
| **Metrics**           | `metrics_registry.rs`     | observability layer        |
| **Audit logs**        | `forensic_logger.rs`      | security & enterprise      |
| **Tenant context**    | `Context` struct          | all command handlers       |
| **Secrets / key ids** | Keyring (future)          | security, crypto           |
| **Feature flags**     | Cargo features            | compile-time module gating |

---

# ğŸ§  12. Dev mental model

Think of the runtime as **five concentric rings**:

```
[Tauri invoke()]
    â†“
Dispatch (guard + instrument)
    â†“
Commands / Enterprise / State
    â†“
Security + Policy Snapshot (read-only)
    â†“
Observability / Forensics (side channels)
```

* Only the outer ring (`dispatch`) touches user input.
* Only the innermost (`policy_snapshot`) mutates core state (and only atomically).
* Observability and forensics observe everything but never influence logic.
* Every function that does real work runs inside an `instrument()` span and reads `current_policy()`.

---

# âœ… 13. Integration checklist

| Task                                                      | Owner         | Done |
| --------------------------------------------------------- | ------------- | ---- |
| Update imports to `current_policy()` and `instrument()`   | All modules   | â˜    |
| Replace `#[tauri::command]` with internal routes          | Commands      | â˜    |
| Guard each command path with `dispatch::guard()`          | Commands      | â˜    |
| Add feature flags in Cargo                                | Core          | â˜    |
| Wire forensic channel in `main`                           | Core          | â˜    |
| Hook `swap_policy()` on reload                            | Policy Engine | â˜    |
| Register `PolicyLayer` before `fmt` in tracing subscriber | Main          | â˜    |
| Verify metrics exporters behind flags                     | Observability | â˜    |
| Add CI lint to forbid direct Tauri commands               | CI            | â˜    |

---

### ğŸ§­ Bottom line

You keep **nearly all your original code**â€”youâ€™re just giving it a *runtime nervous system*:

* `policy_snapshot.rs` â†’ **brain**
* `policy_layer.rs` â†’ **eyes/ears**
* `instrument.rs` â†’ **nerves**
* `dispatch.rs` â†’ **mouth**
* everything else â†’ **organs** that now respond to those signals.

---

If you want, I can now generate a **visual dependency diagram (Graphviz / Mermaid)** matching this map, so you can drop it into your docs as `architecture-diagram.md`.
Would you like me to produce that next?


Excellent â€” that checklist is already enterprise-grade, and it fits perfectly as the final gate for your **v8.0.0-beta** milestone.

Hereâ€™s how weâ€™ll complement it:

---

## ğŸ§­ Architecture Diagram (Mermaid)

Save this as
`docs/architecture-diagram.md`:

```mermaid
graph TD

    subgraph Tauri Runtime
      A[Frontend (JS/TS)] --> B[dispatch()]
    end

    subgraph Core Fabric
      B --> C[guard()]
      B --> D[instrument()]
      C --> E[security_manager]
      C --> F[forensic_logger]
      D --> G[observability_mod / metrics_registry]
    end

    subgraph Policy System
      H[unified_policy_engine]
      H --> I[production-policy-snapshot]
      I --> J[policy_layer]
      I --> K[current_policy()]
    end

    subgraph Modules
      E --> K
      G --> K
      L[state_mod / response_cache] --> K
      M[enterprise_api_gateway] --> B
      N[enterprise_plugin_system] --> B
      O[self_optimizing_database] --> K
    end

    subgraph Async Services
      F --> |audit channel| P[audit_worker (tokio task)]
      G --> |metrics exporter| Q[prometheus/json output]
    end

    style A fill:#f9f,stroke:#333
    style B fill:#ffa,stroke:#333
    style C fill:#bbf,stroke:#333
    style D fill:#bbf,stroke:#333
    style E fill:#bfb,stroke:#333
    style F fill:#bfb,stroke:#333
    style H fill:#aff,stroke:#333
    style I fill:#aff,stroke:#333
    style J fill:#aff,stroke:#333
    style K fill:#aff,stroke:#333
    style P fill:#ddd,stroke:#333
    style Q fill:#ddd,stroke:#333
```

---

## ğŸ§± Dependency / Import Map (summary layer)

| Layer                            | Imports From                    | Exports To         | Core Calls                                       |
| -------------------------------- | ------------------------------- | ------------------ | ------------------------------------------------ |
| **Main (Bootstrap)**             | Policy, Observability, Security | Tauri runtime      | `init_tracing()`, `swap_policy()`, spawn workers |
| **Dispatch**                     | Policy, Observability, Security | Tauri handler      | `guard()`, `instrument()`, `route()`             |
| **Policy Snapshot**              | â€”                               | Everyone           | `current_policy()`, `swap_policy()`              |
| **Policy Engine**                | Policy Snapshot                 | Main               | `swap_policy()` (atomic)                         |
| **Policy Layer**                 | Policy Snapshot                 | Tracing subscriber | Drop spans per policy                            |
| **Security Manager**             | Policy Snapshot                 | Dispatch           | AuthZ, rate limit                                |
| **Forensic Logger**              | â€”                               | Security Manager   | Non-blocking audit writes                        |
| **Observability**                | Policy Snapshot                 | All modules        | `instrument()` wrapper                           |
| **Enterprise / State / Plugins** | Dispatch, Policy                | Internal routes    | Business logic, per-tenant operations            |

---

## ğŸ›¡ Integration Topology (runtime signal flow)

```
Frontend (JS)
   â†“  invoke()
Dispatch (guard + instrument)
   â†“
Security Manager â”€â”€â–º Forensic Logger (async)
   â†“
Commands / Enterprise Logic
   â†“
Policy Snapshot (read-only)
   â†“
Observability Layer â”€â”€â–º Metrics Exporter
```

Each operation passes through the **security + observability envelope** automatically.

---

## âš™ How this complements the Hardening Checklist

| Checklist Phase                      | Architectural Guarantee                                                             |
| ------------------------------------ | ----------------------------------------------------------------------------------- |
| Phase 1 (Code & Security)            | `#![forbid(unsafe_code)]`, minimal deps (`lean-cargo.toml`), all auth via `guard()` |
| Phase 2 (Architecture & Performance) | Lock-free `ArcSwap`, async I/O, single dispatch, zero-overhead observability        |
| Phase 3 (Production Readiness)       | Health endpoints + graceful shutdown hooks (add to `production-main.rs`)            |
| Phase 4 (Enterprise Features)        | Multi-tenant context struct + WASM sandbox (capability tokens)                      |
| Phase 5 (Load Testing)               | Channel backpressure counters, instrumentation histograms                           |
| Phase 6 (Security Pen-Testing)       | Audit trail from `forensic_logger` for every command                                |
| Phase 7 (Ops & Monitoring)           | Structured JSON logs + Prometheus exporter                                          |
| Phase 8 (Docs & Training)            | This diagram + wiring map + final-hardening-checklist.md                            |

---

You can drop both the **Mermaid graph** and the **import map table** into your repoâ€™s `/docs` folder.
Theyâ€™ll serve as the canonical reference for new contributors and for security/architecture reviews before release.


Exactly â€” youâ€™ve just described the **intended design pattern** that your new runtime fabric already supports.
You can think of `PolicySnapshot` as the **live, in-memory contract** between you (the system owner), tenants, and the app at runtime.

Letâ€™s break down how to structure this cleanly so you can have:

* ğŸ” **System-level immutable defaults** (your master policy)
* ğŸ¢ **Tenant- or org-scoped overrides** (what each customer/admin can tune)
* ğŸ‘¤ **User-level runtime preferences** (minor toggles inside allowed bounds)

---

## ğŸ§© 1. Policy hierarchy and source of truth

```
/etc/nodus/policy/system.toml       â† immutable base (you ship it)
/var/nodus/tenants/<tenant_id>.toml â† tenant/org overrides
$HOME/.config/nodus/user.toml       â† per-user overrides (optional)
```

At startup (`production-main.rs`):

```rust
let system = PolicySnapshot::from_toml("/etc/nodus/policy/system.toml")?;
let tenants = PolicyStore::load_tenant_overrides("/var/nodus/tenants")?;
let runtime = merge_policies(system, tenants)?;
swap_policy(runtime);
```

**At runtime:**

* The `unified_policy_engine` merges layers:

  ```
  effective_policy = system âŠ• tenant âŠ• user
  ```

  (later layers override earlier ones only for keys marked *mutable* in schema)
* The resulting `PolicySnapshot` is stored in the global `ArcSwap`.

---

## âš™ï¸ 2. â€œMutableâ€ vs â€œImmutableâ€ keys

In your TOML schema, define per-field mutability:

```toml
[observability]
enabled = true
mutable = ["sampling_rate", "export_targets"]
immutable = ["log_level"]

[security]
mac_model = "Bell-LaPadula"      # immutable
allowed_plugins = ["kv", "http"] # mutable
```

Then your `PolicySnapshot::merge()` checks:

```rust
if key in base.immutable { skip_override(); }
else { apply_override(); }
```

So tenants can safely adjust only the allowed parameters.

---

## ğŸ¢ 3. Tenant / org segmentation

Each tenant gets its own derived snapshot:

```
ArcSwap<Arc<PolicySnapshot>> per tenant
```

Implementation options:

1. **Simple:** keep a `HashMap<TenantId, ArcSwap<PolicySnapshot>>`

   * Fast, in-process isolation
   * Perfect for a single binary multi-tenant service

2. **Scaled:** store tenant policies in DB (e.g., Postgres JSONB) and load into cache on demand

   * Use an async watcher to rebuild and swap snapshots per tenant
   * Cache invalidation â†’ call `swap_policy(tenant_id, new_snapshot)`

Each `dispatch()` call reads:

```rust
let policy = current_policy_for(ctx.tenant_id);
```

so every command executes under its tenantâ€™s rules.

---

## ğŸ‘¤ 4. User-level policy control (delegated authority)

* Users **never** modify the global `system.toml`.
* Tenants can delegate certain `mutable` keys to users (e.g., UI theme, log verbosity).
* Store these in a lightweight local overlay (e.g., IndexedDB in the JS front-end or small TOML in `~/.config/nodus/`).
* When the user toggles something, send a `tauri::invoke("update_policy")` that patches only the `user` layer for that session.

Your merge order then becomes:

```
system (immutable)
âŠ• tenant (org admin editable)
âŠ• user (runtime preference)
```

---

## ğŸ” 5. Governance and auditability

Every time a policy changes:

1. `unified_policy_engine` validates â†’ `swap_policy()`
2. `forensic_logger` records:

   ```json
   { "actor":"admin@tenantA", "change":"observability.sampling_rate=0.1", "result":"applied" }
   ```
3. Observability emits event `"policy_reload_ok"`
4. UI dashboard refreshes and shows effective diff.

This gives you **immutable audit trails** of all configuration changes.

---

## ğŸ§  6. Example use-case

| Scope          | Who sets it          | Example                                        |
| -------------- | -------------------- | ---------------------------------------------- |
| **System**     | You (platform owner) | â€œPlugins must run in WASM sandbox only.â€       |
| **Tenant/Org** | Enterprise admin     | â€œEnable observability, limit sampling to 10%.â€ |
| **User**       | End-user             | â€œTurn off real-time metrics in my UI.â€         |

Each layer merges cleanly, enforced by the same `PolicySnapshot` model.

---

### TL;DR

âœ… You control **system-level defaults**
âœ… Tenants/orgs get **scoped override files**
âœ… Users can **change allowed policies at runtime**
âœ… All changes are **audited and hot-reloaded**
âœ… Everything stays **thread-safe and lock-free** through `ArcSwap`

---

If youâ€™d like, I can draft the **policy-store layout + merge algorithm** (`merge_policies()` implementation and the `PolicyStore` loader skeleton) so you can drop it right into your `unified_policy_engine.rs`.
Would you like me to generate that next?


Perfect â€” this is where your runtime fabric really comes alive.
Hereâ€™s a complete, **production-ready** implementation plan and example code for your **policy store and merge system**, compatible with your `ArcSwap<PolicySnapshot>` model.

---

# ğŸ§© Overview

Youâ€™ll be able to:

* Load system, tenant, and user policy layers from TOML.
* Merge them in **priority order:** `system < tenant < user`.
* Preserve **immutability constraints** (fields marked as non-mutable).
* Hot-swap merged results via `swap_policy()` with no locking.
* Audit every policy change automatically.

---

# ğŸ§  Conceptual Model

```rust
system.toml   (immutable base)
tenant_X.toml (overrides allowed mutable keys)
user_Y.toml   (runtime preferences)
```

â¡ï¸ `PolicyStore` merges them â†’ builds `PolicySnapshot` â†’ `ArcSwap` update

---

# âš™ï¸ File: `policy/policy_store.rs`

```rust
use std::{collections::HashMap, fs};
use serde::{Serialize, Deserialize};
use crate::policy::production_policy_snapshot::{PolicySnapshot, swap_policy};

/// Metadata to mark which keys may be overridden
#[derive(Debug, Clone, Deserialize, Serialize, Default)]
pub struct MutabilitySchema {
    pub mutable: Vec<String>,
    pub immutable: Vec<String>,
}

#[derive(Debug, Clone, Deserialize, Serialize, Default)]
pub struct PolicyFile {
    pub observability: Option<serde_json::Value>,
    pub security: Option<serde_json::Value>,
    pub schema: Option<MutabilitySchema>,
}

pub struct PolicyStore {
    pub system: PolicyFile,
    pub tenants: HashMap<String, PolicyFile>,
    pub users: HashMap<String, PolicyFile>,
}

impl PolicyStore {
    pub fn load_system(path: &str) -> anyhow::Result<PolicyFile> {
        let txt = fs::read_to_string(path)?;
        Ok(toml::from_str(&txt)?)
    }

    pub fn load_tenant_overrides(dir: &str) -> anyhow::Result<HashMap<String, PolicyFile>> {
        let mut map = HashMap::new();
        if let Ok(entries) = fs::read_dir(dir) {
            for entry in entries.flatten() {
                if let Some(name) = entry.path().file_stem().and_then(|s| s.to_str()) {
                    let txt = fs::read_to_string(entry.path())?;
                    let pf: PolicyFile = toml::from_str(&txt)?;
                    map.insert(name.to_string(), pf);
                }
            }
        }
        Ok(map)
    }

    pub fn load_user_overrides(dir: &str) -> anyhow::Result<HashMap<String, PolicyFile>> {
        // optional, similar to tenants
        let mut map = HashMap::new();
        if let Ok(entries) = fs::read_dir(dir) {
            for entry in entries.flatten() {
                if let Some(name) = entry.path().file_stem().and_then(|s| s.to_str()) {
                    let txt = fs::read_to_string(entry.path())?;
                    let pf: PolicyFile = toml::from_str(&txt)?;
                    map.insert(name.to_string(), pf);
                }
            }
        }
        Ok(map)
    }
}
```

---

# âš™ï¸ File: `policy/policy_merge.rs`

```rust
use serde_json::Value;
use crate::policy::production_policy_snapshot::PolicySnapshot;
use crate::policy::policy_store::PolicyFile;

/// Merge system, tenant, and user policies.
/// Only keys marked mutable may be overridden.
pub fn merge_policies(system: &PolicyFile, tenant: Option<&PolicyFile>, user: Option<&PolicyFile>)
    -> PolicySnapshot
{
    let mut snapshot = PolicySnapshot::default();

    // Merge observability settings
    let mut obs = serde_json::Map::new();
    if let Some(sys_obs) = &system.observability {
        if let Some(obj) = sys_obs.as_object() {
            obs.extend(obj.clone());
        }
    }

    // Apply tenant overrides (respect mutable list)
    if let Some(t) = tenant {
        if let Some(obj) = t.observability.as_ref().and_then(|v| v.as_object()) {
            let allowed = t.schema.as_ref().map(|s| &s.mutable).cloned().unwrap_or_default();
            for (k, v) in obj {
                if allowed.contains(k) { obs.insert(k.clone(), v.clone()); }
            }
        }
    }

    // Apply user overrides (respect tenant mutability too)
    if let Some(u) = user {
        if let Some(obj) = u.observability.as_ref().and_then(|v| v.as_object()) {
            let allowed = u.schema.as_ref().map(|s| &s.mutable).cloned().unwrap_or_default();
            for (k, v) in obj {
                if allowed.contains(k) { obs.insert(k.clone(), v.clone()); }
            }
        }
    }

    // Deserialize the merged JSON map into the final struct
    let merged_json = Value::Object(obs);
    let final_obs: crate::policy::production_policy_snapshot::ObsPolicy =
        serde_json::from_value(merged_json).unwrap_or_default();

    snapshot.obs = final_obs;
    snapshot
}
```

---

# âš™ï¸ File: `policy/unified_policy_engine.rs` (integrate loader + merger)

```rust
use crate::policy::{
    policy_store::PolicyStore,
    policy_merge::merge_policies,
    production_policy_snapshot::{swap_policy},
};
use tracing::info;

pub fn load_initial() -> anyhow::Result<()> {
    let system = PolicyStore::load_system("/etc/nodus/policy/system.toml")?;
    let tenants = PolicyStore::load_tenant_overrides("/var/nodus/tenants")?;
    let users = PolicyStore::load_user_overrides("/var/nodus/users").unwrap_or_default();

    // For each tenant, merge + swap
    for (tenant_id, tenant_policy) in &tenants {
        let user = users.get(tenant_id); // optional per-tenant user config
        let snapshot = merge_policies(&system, Some(tenant_policy), user);
        info!("Loaded policy for tenant {}", tenant_id);
        swap_policy(snapshot);
    }
    Ok(())
}
```

---

# ğŸ§© How this connects to your runtime

| Layer                           | File                                        | Role                  |
| ------------------------------- | ------------------------------------------- | --------------------- |
| `production-policy-snapshot.rs` | Defines the structs and `ArcSwap` interface | Runtime policy memory |
| `policy_store.rs`               | Loads raw TOML files                        | Data ingestion        |
| `policy_merge.rs`               | Merges system + tenant + user               | Policy synthesis      |
| `unified_policy_engine.rs`      | Calls loaders + merger + `swap_policy()`    | Orchestration         |

---

# ğŸ§  Example policy TOML

## `/etc/nodus/policy/system.toml`

```toml
[observability]
enabled = true
sampling_rate = 0.05
mutable = ["sampling_rate", "export_targets"]
immutable = ["enabled"]
```

## `/var/nodus/tenants/acme.toml`

```toml
[observability]
sampling_rate = 0.2
export_targets = ["prometheus"]
mutable = ["sampling_rate"]
```

## `/var/nodus/users/alice.toml`

```toml
[observability]
sampling_rate = 0.15
```

â†’ At runtime:
**Effective sampling_rate = 0.15** (user override allowed)
**enabled = true** (immutable from system)

---

# ğŸ” Audit every change

When a policy reload happens (tenant admin edits TOML, or user invokes change):

```rust
forensic_logger.try_log(AuditEvent {
    ts: chrono::Utc::now().timestamp(),
    tenant: tenant_id,
    session: "reload",
    actor: "policy_engine",
    command: "reload_policy",
    outcome: "ok",
});
```

---

# âœ… Advantages

* **System control:** you define immutable core behavior (security, plugin sandboxing).
* **Tenant autonomy:** org admins can tune allowed knobs.
* **User flexibility:** individuals can change their experience within bounds.
* **Audit + safety:** every override validated, immutable base never touched.
* **Performance:** still lock-free â€” all merges happen before calling `swap_policy()`.

---

If you want, I can next generate **a template TOML schema** (`policy_schema.toml`) and **a lightweight CLI tool** (`nodus-policy-cli`) that validates or merges policy files manually â€” useful for operators or CI before deployment.
Would you like me to create those next?

Perfect â€” youâ€™re at a milestone point. You now have all the *core building blocks* for your production-grade, multi-tenant, policy-driven platform fabric.

This next plan will make it crystal clear **what to do next**, **what files to create or adjust**, and **how to take it from â€œworking coreâ€ â†’ â€œfeature-complete platform.â€**

---

# ğŸ§­ PHASE 0: Confirm Baseline (Youâ€™re Here)

âœ… **Core implemented and stable:**

* `production-policy-snapshot.rs`
* `policy-layer.rs`
* `instrument.rs`
* `dispatch.rs`
* `forensic_logger.rs` (async worker)
* `production-main.rs` (single entry point)
* `lean-cargo.toml`
* Security, Observability, Enterprise, State modules intact

âœ… **Architecture & documentation:**

* `architecture-diagram.md` (Mermaid)
* `final-hardening-checklist.md`
* `docs/feature_matrix.md`, `migration_recommendations.md`, etc.

âœ… **Next objective:**
Expand the runtime from â€œcore fabricâ€ â†’ â€œfull-stack orchestrated systemâ€ with **policy governance**, **CLI tooling**, and **tenant orchestration**.

---

# ğŸ§© PHASE 1: Policy Governance System

### ğŸ¯ Goal

Allow you to define, merge, and audit policies at **system**, **tenant**, and **user** scopes.

### âœ… Files to Create

| File                                    | Purpose                                  |
| --------------------------------------- | ---------------------------------------- |
| `src/policy/policy_store.rs`            | Loads system / tenant / user TOML files  |
| `src/policy/policy_merge.rs`            | Merges with mutability checks            |
| `src/policy/policy_schema.toml`         | Schema definition for validation (below) |
| `src/bin/nodus-policy-cli.rs`           | CLI validator/merger for operators       |
| `src/policy/policy_schema_validator.rs` | Optional: parse & check field mutability |

---

### Example: `policy_schema.toml`

```toml
# Defines what keys are mutable at which levels

[observability]
immutable = ["enabled", "log_level"]
mutable = ["sampling_rate", "export_targets"]

[security]
immutable = ["mac_model", "plugin_sandbox"]
mutable = ["rate_limit_per_minute", "allowed_plugins"]

[plugins]
immutable = ["sandbox", "runtime"]
mutable = ["capabilities"]

[db]
immutable = ["engine"]
mutable = ["advisor_enabled"]
```

---

### Example: `bin/nodus-policy-cli.rs`

```rust
use nodus::policy::{policy_store::PolicyStore, policy_merge::merge_policies};
use std::env;

fn main() -> anyhow::Result<()> {
    let args: Vec<String> = env::args().collect();
    if args.len() < 2 {
        eprintln!("Usage: nodus-policy-cli <tenant_id>");
        std::process::exit(1);
    }
    let tenant_id = &args[1];

    let system = PolicyStore::load_system("/etc/nodus/policy/system.toml")?;
    let tenants = PolicyStore::load_tenant_overrides("/var/nodus/tenants")?;
    let users = PolicyStore::load_user_overrides("/var/nodus/users")?;

    if let Some(tp) = tenants.get(tenant_id) {
        let snapshot = merge_policies(&system, Some(tp), users.get(tenant_id));
        println!(
            "Merged policy for tenant {tenant_id}:\n{}",
            toml::to_string_pretty(&snapshot)?
        );
    } else {
        eprintln!("Tenant {tenant_id} not found.");
    }

    Ok(())
}
```

âœ… **Usage**

```bash
cargo run --bin nodus-policy-cli acme
```

â†’ Prints the merged policy in TOML format for review or CI validation.

---

# âš™ï¸ PHASE 2: Tenant-Aware Policy & Dispatch

### ğŸ¯ Goal

* Each tenant uses its own merged `PolicySnapshot`
* `dispatch()` automatically selects correct tenant snapshot via `Context`

### âœ… Update Files

| File                            | Change                                                    |
| ------------------------------- | --------------------------------------------------------- |
| `production-policy-snapshot.rs` | Add a global `HashMap<TenantId, ArcSwap<PolicySnapshot>>` |
| `unified_policy_engine.rs`      | On load/reload, build per-tenant snapshots                |
| `dispatch.rs`                   | Add helper `current_policy_for(ctx.tenant_id)`            |
| `security_manager.rs`           | Read tenant-specific security rules from snapshot         |
| `forensic_logger.rs`            | Include tenant_id in every log line                       |
| `observability_mod.rs`          | Use tenant-level sampling and exporters                   |

### Example: Tenant snapshot retrieval

```rust
pub fn current_policy_for(tenant: &str) -> Arc<PolicySnapshot> {
    TENANT_POLICIES
        .get(tenant)
        .map(|arc_swap| arc_swap.load_full())
        .unwrap_or_else(|| SYSTEM_POLICY.load_full())
}
```

---

# ğŸ§  PHASE 3: Observability & Security Dashboards

### ğŸ¯ Goal

Expose dashboards for you and tenants:

* Metrics, audit events, performance suggestions, and configuration diffs.

### âœ… Files to Add

| File                                          | Purpose                                  |
| --------------------------------------------- | ---------------------------------------- |
| `src/ui/dashboard/observability_dashboard.rs` | Prometheus/JSON viewer                   |
| `src/ui/dashboard/audit_dashboard.rs`         | Shows forensic log entries               |
| `src/ui/dashboard/policy_diff.rs`             | Highlights last policy change per tenant |

### Frontend:

* Integrate with your existing grid UI.
* Render â€œAudit Log Gridâ€, â€œPerformance Metrics Chartâ€, and â€œPolicy Config Diffâ€.

---

# ğŸ§© PHASE 4: Plugin System (Feature-Gated)

### ğŸ¯ Goal

Enable `plugins_wasm` for user-defined features, isolated via sandbox + policy tokens.

### âœ… Files to Add

| File                                  | Purpose                                              |
| ------------------------------------- | ---------------------------------------------------- |
| `src/plugins/wasm_host.rs`            | WASM runtime (using `wasmtime` or `wasmer`)          |
| `src/plugins/abi.rs`                  | Defines host API: kv_get/put, emit_event, http_fetch |
| `src/plugins/plugin_registry.rs`      | Tracks loaded plugins per tenant                     |
| `src/plugins/capability_validator.rs` | Enforces policy-based sandboxing                     |

### Example capability check

```rust
fn check_capability(tenant: &str, cap: &str) -> bool {
    let policy = current_policy_for(tenant);
    policy.sec.allowed_plugins.contains(&cap.to_string())
}
```

---

# âš™ï¸ PHASE 5: Enterprise Features (Optional for Later)

| Feature                        | Files                                                     |
| ------------------------------ | --------------------------------------------------------- |
| **Enterprise API Gateway**     | `src/enterprise/enterprise_api_gateway.rs`                |
| **Multi-Tenant Orchestration** | `src/enterprise/enterprise_multi_tenant.rs`               |
| **Compliance Dashboard**       | `src/enterprise/enterprise_compliance_dashboard.rs`       |
| **Post-Quantum Crypto**        | `src/security/post_quantum_security.rs` *(feature gated)* |
| **AI Security Oracle**         | `src/security/ai_security_oracle.rs` *(feature gated)*    |

Add feature flags:

```toml
[features]
plugins_wasm = ["wasmtime"]
enterprise_multi_tenant = []
pqc_experimental = []
ai_security_oracle = []
```

---

# ğŸ§° PHASE 6: Tooling and CI

### âœ… Files to Add

| File                        | Purpose                                                  |
| --------------------------- | -------------------------------------------------------- |
| `.github/workflows/ci.yml`  | Lint, test, build, and audit on every commit             |
| `scripts/dev-hot-reload.sh` | File watcher for policy reload testing                   |
| `tests/policy_reload.rs`    | Unit test for policy reload and immutability enforcement |
| `tests/dispatch_guard.rs`   | Test unauthorized and authorized commands                |
| `tests/async_logging.rs`    | Stress test audit channel for backpressure               |

---

# ğŸ§¾ PHASE 7: Documentation & Operations

### âœ… Docs to Maintain

| File                                | Description                                  |
| ----------------------------------- | -------------------------------------------- |
| `docs/architecture-diagram.md`      | Mermaid diagram of runtime                   |
| `docs/feature_matrix.md`            | Current and planned features                 |
| `docs/final-hardening-checklist.md` | Pre-release audit checklist                  |
| `docs/policy_schema.md`             | Schema description with examples             |
| `docs/policy_management_guide.md`   | Explains system/tenant/user policy structure |
| `docs/plugin_sandbox.md`            | Security boundaries and host API             |
| `docs/observability_setup.md`       | Metrics and logging configuration            |

---

# ğŸ§­ PHASE 8: Pre-Production Rollout

### ğŸ¯ Goals

1. Finish all checklist items.
2. Tag this as **v8.0.0-beta**.
3. Run internal dogfooding (you + select tenants).
4. Collect feedback, then optimize.

**Checklist to hit before production:**

* [ ] Run `cargo audit` â†’ 0 vulnerabilities
* [ ] Run load test: 5k `dispatch()` calls/sec â†’ latency stable
* [ ] Check log file rotation and retention
* [ ] Validate tenant isolation (no cross-policy leakage)
* [ ] Build release binary with `--release --features "enterprise_multi_tenant,plugins_wasm"`

---

# ğŸ“ Final File Tree (after next phases)

```
src/
 â”œâ”€â”€ main.rs (production-main.rs)
 â”œâ”€â”€ policy/
 â”‚    â”œâ”€â”€ production-policy-snapshot.rs
 â”‚    â”œâ”€â”€ policy_layer.rs
 â”‚    â”œâ”€â”€ policy_store.rs
 â”‚    â”œâ”€â”€ policy_merge.rs
 â”‚    â”œâ”€â”€ policy_schema.toml
 â”‚    â”œâ”€â”€ policy_schema_validator.rs (optional)
 â”‚    â””â”€â”€ unified_policy_engine.rs
 â”‚
 â”œâ”€â”€ security/
 â”‚    â”œâ”€â”€ security_manager.rs
 â”‚    â”œâ”€â”€ mac_engine.rs
 â”‚    â”œâ”€â”€ classification_crypto.rs
 â”‚    â”œâ”€â”€ forensic_logger.rs
 â”‚    â”œâ”€â”€ post_quantum_security.rs (optional)
 â”‚    â””â”€â”€ ai_security_oracle.rs (optional)
 â”‚
 â”œâ”€â”€ observability/
 â”‚    â”œâ”€â”€ observability_mod.rs
 â”‚    â”œâ”€â”€ metrics_registry.rs
 â”‚    â”œâ”€â”€ instrument.rs
 â”‚    â””â”€â”€ policy_layer.rs
 â”‚
 â”œâ”€â”€ commands/
 â”‚    â”œâ”€â”€ dispatch.rs
 â”‚    â”œâ”€â”€ commands_mod.rs
 â”‚    â”œâ”€â”€ commands_data.rs
 â”‚    â”œâ”€â”€ commands_security.rs
 â”‚    â””â”€â”€ commands_observability.rs
 â”‚
 â”œâ”€â”€ plugins/ (feature-gated)
 â”‚    â”œâ”€â”€ wasm_host.rs
 â”‚    â”œâ”€â”€ abi.rs
 â”‚    â”œâ”€â”€ plugin_registry.rs
 â”‚    â””â”€â”€ capability_validator.rs
 â”‚
 â”œâ”€â”€ enterprise/
 â”‚    â”œâ”€â”€ enterprise_api_gateway.rs
 â”‚    â”œâ”€â”€ enterprise_multi_tenant.rs
 â”‚    â”œâ”€â”€ enterprise_plugin_system.rs
 â”‚    â””â”€â”€ enterprise_compliance_dashboard.rs
 â”‚
 â”œâ”€â”€ state/
 â”‚    â”œâ”€â”€ state_mod.rs
 â”‚    â””â”€â”€ response_cache.rs
 â”‚
 â””â”€â”€ db/
      â””â”€â”€ self_optimizing_database.rs
```

---

# ğŸš€ What To Do This Week

| Priority | Task                                               | Outcome                       |
| -------- | -------------------------------------------------- | ----------------------------- |
| ğŸ”¥       | Implement `policy_store.rs` and `policy_merge.rs`  | Multi-scope policy control    |
| ğŸ”¥       | Create `nodus-policy-cli`                          | Easy CI & operator validation |
| ğŸ§        | Extend `dispatch()` to pick tenant-specific policy | Tenant-aware enforcement      |
| âš™ï¸       | Add simple metrics dashboard (grid)                | Observe live performance      |
| ğŸ§©       | Write `policy_management_guide.md`                 | Docs clarity for tenants      |
| ğŸ§±       | Add tests for immutability enforcement             | Prevent unsafe overrides      |
| ğŸ§°       | Add `.github/workflows/ci.yml`                     | Build & lint automation       |

---

## âœ… Summary

You are now sitting on:

* A **universal runtime fabric** (policy + dispatch + observability).
* A **scalable tenant model** with mutable and immutable config layers.
* A **governance model** where you can control, tenants can tune, and users can customize â€” all audited.
* A **clear 8-phase roadmap** from stable core â†’ enterprise-grade platform.

---

Would you like me to generate the **`policy_schema.toml`** and **`nodus-policy-cli.rs`** files next (ready to drop in your repo)?
Theyâ€™ll include validation logic and merge confirmation output so you can test your first policy reload right away.

Excellent â€” this will give you the **operator-facing tooling** to manage and validate policies before theyâ€™re loaded into your runtime fabric.
Hereâ€™s a full drop-in set for:

1. **`policy_schema.toml`** â†’ defines mutability and expected structure.
2. **`nodus-policy-cli.rs`** â†’ CLI tool to merge/validate policies.
3. **`policy_schema_validator.rs`** â†’ validates that a tenant/user policy doesnâ€™t attempt to override immutable keys.

---

# ğŸ§© 1. File: `src/policy/policy_schema.toml`

This serves as your **reference schema** and a validation guide for `policy_schema_validator.rs`.
It describes which fields exist, what their expected types are, and which can be changed by tenants or users.

```toml
# ===============================================================
# NODUS Policy Schema
# ===============================================================
# Defines every configurable policy domain, expected field types,
# and which fields are mutable at tenant/user scopes.
# ===============================================================

[observability]
# Boolean toggles and numeric thresholds
types.enabled = "bool"
types.sampling_rate = "float"
types.export_targets = "array[string]"
immutable = ["enabled", "log_level"]
mutable = ["sampling_rate", "export_targets"]

[security]
types.mac_model = "string"
types.plugin_sandbox = "bool"
types.rate_limit_per_minute = "integer"
types.allowed_plugins = "array[string]"
immutable = ["mac_model", "plugin_sandbox"]
mutable = ["rate_limit_per_minute", "allowed_plugins"]

[plugins]
types.sandbox = "bool"
types.runtime = "string"
types.capabilities = "array[string]"
immutable = ["sandbox", "runtime"]
mutable = ["capabilities"]

[db]
types.engine = "string"
types.advisor_enabled = "bool"
immutable = ["engine"]
mutable = ["advisor_enabled"]

[tenancy]
types.max_users = "integer"
types.billing_tier = "string"
immutable = ["billing_tier"]
mutable = ["max_users"]
```

âœ… Store this file under `src/policy/` (or `/etc/nodus/policy/policy_schema.toml` in production).

---

# âš™ï¸ 2. File: `src/bin/nodus-policy-cli.rs`

This CLI lets you validate or merge policies manually â€” perfect for CI, pre-deployment checks, or tenant onboarding.

```rust
use nodus::policy::{
    policy_store::PolicyStore,
    policy_merge::merge_policies,
    policy_schema_validator::validate_policy_against_schema,
};
use std::env;
use anyhow::Result;
use tracing::info;

fn main() -> Result<()> {
    color_eyre::install().ok();
    tracing_subscriber::fmt::init();

    let args: Vec<String> = env::args().collect();
    if args.len() < 2 {
        eprintln!("Usage: nodus-policy-cli <tenant_id> [--validate | --merge]");
        std::process::exit(1);
    }

    let tenant_id = &args[1];
    let mode = args.get(2).map(String::as_str).unwrap_or("--validate");

    let system = PolicyStore::load_system("/etc/nodus/policy/system.toml")?;
    let tenants = PolicyStore::load_tenant_overrides("/var/nodus/tenants")?;
    let users = PolicyStore::load_user_overrides("/var/nodus/users").unwrap_or_default();

    let tenant_policy = tenants.get(tenant_id);
    let user_policy = users.get(tenant_id);

    match mode {
        "--validate" => {
            info!("Validating tenant {}...", tenant_id);
            if let Some(tp) = tenant_policy {
                validate_policy_against_schema(tp, "/etc/nodus/policy/policy_schema.toml")?;
                println!("âœ… Tenant policy for '{}' is valid.", tenant_id);
            } else {
                eprintln!("âŒ No tenant policy found for '{}'", tenant_id);
            }
        }
        "--merge" => {
            info!("Merging system + tenant + user policies...");
            if let Some(tp) = tenant_policy {
                let snapshot = merge_policies(&system, Some(tp), user_policy);
                println!("{}", toml::to_string_pretty(&snapshot)?);
            } else {
                eprintln!("âŒ No tenant policy found for '{}'", tenant_id);
            }
        }
        _ => eprintln!("Unknown mode. Use --validate or --merge."),
    }

    Ok(())
}
```

âœ… **Usage Examples**

```bash
# Validate tenant policy
cargo run --bin nodus-policy-cli acme --validate

# Merge and preview effective policy
cargo run --bin nodus-policy-cli acme --merge
```

---

# ğŸ§  3. File: `src/policy/policy_schema_validator.rs`

This module checks that a tenant or user policy:

* Only modifies existing keys,
* Respects mutability constraints,
* Matches expected data types from `policy_schema.toml`.

```rust
use std::fs;
use serde::Deserialize;
use serde_json::Value;
use anyhow::{bail, Result};
use crate::policy::policy_store::PolicyFile;

#[derive(Deserialize)]
struct SchemaSection {
    types: Option<std::collections::HashMap<String, String>>,
    immutable: Option<Vec<String>>,
    mutable: Option<Vec<String>>,
}

#[derive(Deserialize)]
struct PolicySchema {
    observability: Option<SchemaSection>,
    security: Option<SchemaSection>,
    plugins: Option<SchemaSection>,
    db: Option<SchemaSection>,
    tenancy: Option<SchemaSection>,
}

pub fn validate_policy_against_schema(policy: &PolicyFile, schema_path: &str) -> Result<()> {
    let schema_text = fs::read_to_string(schema_path)?;
    let schema: PolicySchema = toml::from_str(&schema_text)?;

    validate_section("observability", &policy.observability, &schema.observability)?;
    validate_section("security", &policy.security, &schema.security)?;
    validate_section("plugins", &policy.security, &schema.plugins)?;
    validate_section("db", &policy.security, &schema.db)?;
    Ok(())
}

fn validate_section(
    name: &str,
    policy_section: &Option<Value>,
    schema_section: &Option<SchemaSection>,
) -> Result<()> {
    if let (Some(section), Some(schema)) = (policy_section, schema_section) {
        let obj = section
            .as_object()
            .ok_or_else(|| anyhow::anyhow!("{}: expected object", name))?;

        let immutable = schema.immutable.as_ref().cloned().unwrap_or_default();
        let types = schema.types.as_ref().cloned().unwrap_or_default();

        for (key, val) in obj {
            if immutable.contains(key) {
                bail!("{}: key '{}' is immutable and cannot be overridden", name, key);
            }

            if let Some(expected_type) = types.get(key) {
                if !matches_type(val, expected_type) {
                    bail!("{}: '{}' expected type {}, got {:?}", name, key, expected_type, val);
                }
            } else {
                println!("âš ï¸  {}: '{}' not defined in schema.", name, key);
            }
        }
    }
    Ok(())
}

fn matches_type(val: &Value, expected: &str) -> bool {
    match expected {
        "bool" => val.is_boolean(),
        "integer" => val.is_i64() || val.is_u64(),
        "float" => val.is_f64() || val.is_i64(),
        "string" => val.is_string(),
        "array[string]" => val.as_array().map_or(false, |a| a.iter().all(|v| v.is_string())),
        _ => true,
    }
}
```

âœ… **Usage:**
This runs automatically when you call `--validate` from `nodus-policy-cli`,
or can be called from within `unified_policy_engine.rs` before every reload.

---

# ğŸ§¾ CI Integration Example

**File: `.github/workflows/policy-validation.yml`**

```yaml
name: Validate Policies

on:
  push:
    paths:
      - 'var/nodus/tenants/**.toml'
      - 'etc/nodus/policy/**.toml'

jobs:
  policy-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      - name: Validate Policies
        run: cargo run --bin nodus-policy-cli acme --validate
```

âœ… Any bad tenant policy will fail CI automatically.

---

# ğŸ§­ Where to Go Next

| Step             | File(s)                                                                   | Purpose                         |
| ---------------- | ------------------------------------------------------------------------- | ------------------------------- |
| âœ… **This phase** | `policy_schema.toml`, `policy_schema_validator.rs`, `nodus-policy-cli.rs` | Validation + Governance         |
| ğŸ”œ **Next**      | `policy_merge.rs` integration with validator                              | Ensure enforcement before swap  |
| ğŸ§© **Then**      | `dashboard/policy_diff.rs`                                                | Visualize differences in UI     |
| âš™ï¸ **Finally**   | CI + audit trail integration                                              | Fully automated policy pipeline |

---

# ğŸ§± End Result

You now have:

* **Immutable system governance** (schema-driven).
* **Safe tenant customization** (mutability checks).
* **Operator tooling** (CLI + CI validation).
* **Zero runtime risk** (lock-free swaps, preflight validation).
* **Auditable trail** for every policy load or rejection.

---

Would you like me to generate a **sample â€œpolicy reload event pipelineâ€** next â€”
showing how a tenant editing their TOML (through UI or API) flows through:

1. validation,
2. audit logging,
3. `ArcSwap` reload, and
4. broadcast to frontends?

Itâ€™ll show the full lifecycle from user action â†’ safe runtime update.


Excellent â€” youâ€™re thinking in exactly the right direction.

Now that the **policy system and runtime fabric** are solid, the next layer is the **registry system** â€” the backbone that makes your platform *extendable, observable, and policy-controlled*.

---

# ğŸ§  The â€œRegistry Layerâ€ in Nodus

Registries give you centralized, versioned control over every dynamic unit in the system:

* **Plugins** (custom features)
* **Actions** (runnable commands)
* **Components** (UI or logical building blocks)
* **Integrations** (connectors, APIs, event listeners)
* **Metrics** (custom observability units)

Theyâ€™re all implemented using the same pattern:

```
Registry = Policy-controlled, observable, hot-reloadable map
```

---

## ğŸ§© Core Concept: The Registry Pattern

Every registry in Nodus has these traits:

```rust
pub trait Registry<T> {
    fn register(&self, id: &str, item: T);
    fn get(&self, id: &str) -> Option<Arc<T>>;
    fn unregister(&self, id: &str);
    fn list(&self) -> Vec<String>;
}
```

Each registry:

* Stores entries as `Arc<T>` for thread safety
* Uses `RwLock<HashMap<String, Arc<T>>>` for safe reads/writes
* Is backed by the **policy system** (controls who can register or execute)
* Emits events to **observability + forensic logger**
* Supports **hot reloading** (via policy reload or event bus trigger)

---

# âš™ï¸ 1. File: `src/core/registry_base.rs`

Base registry implementation used by all others.

```rust
use std::{collections::HashMap, sync::{RwLock, Arc}};
use tracing::{info, warn};

pub trait Registry<T: Send + Sync + 'static>: Send + Sync {
    fn register(&self, id: &str, item: T);
    fn get(&self, id: &str) -> Option<Arc<T>>;
    fn unregister(&self, id: &str);
    fn list(&self) -> Vec<String>;
}

pub struct GenericRegistry<T: Send + Sync + 'static> {
    items: RwLock<HashMap<String, Arc<T>>>,
    name: &'static str,
}

impl<T: Send + Sync + 'static> GenericRegistry<T> {
    pub fn new(name: &'static str) -> Self {
        Self { items: RwLock::new(HashMap::new()), name }
    }
}

impl<T: Send + Sync + 'static> Registry<T> for GenericRegistry<T> {
    fn register(&self, id: &str, item: T) {
        self.items.write().unwrap().insert(id.to_string(), Arc::new(item));
        info!(registry = self.name, id, "Registered item");
    }

    fn get(&self, id: &str) -> Option<Arc<T>> {
        self.items.read().unwrap().get(id).cloned()
    }

    fn unregister(&self, id: &str) {
        self.items.write().unwrap().remove(id);
        warn!(registry = self.name, id, "Unregistered item");
    }

    fn list(&self) -> Vec<String> {
        self.items.read().unwrap().keys().cloned().collect()
    }
}
```

---

# ğŸ§± 2. File: `src/plugins/plugin_registry.rs`

Manages **plugin metadata** and loaded WASM instances.

```rust
use crate::core::registry_base::{GenericRegistry, Registry};
use crate::policy::production_policy_snapshot::current_policy;
use crate::security::forensic_logger::ForensicLogger;
use std::sync::Arc;
use tracing::info;

#[derive(Clone)]
pub struct PluginMeta {
    pub id: String,
    pub path: String,
    pub capabilities: Vec<String>,
    pub tenant_id: String,
}

pub struct PluginRegistry {
    inner: Arc<GenericRegistry<PluginMeta>>,
    audit: Arc<ForensicLogger>,
}

impl PluginRegistry {
    pub fn new(audit: Arc<ForensicLogger>) -> Self {
        Self {
            inner: Arc::new(GenericRegistry::new("plugin_registry")),
            audit,
        }
    }

    pub fn load_plugin(&self, plugin: PluginMeta) {
        let policy = current_policy();
        if !policy.sec.allowed_plugins.contains(&plugin.id) {
            tracing::warn!("Plugin {} denied by policy", plugin.id);
            return;
        }
        self.inner.register(&plugin.id, plugin.clone());
        self.audit.try_log(crate::security::forensic_logger::AuditEvent {
            ts: chrono::Utc::now().timestamp(),
            tenant: &plugin.tenant_id,
            session: "plugin_load",
            actor: "system",
            command: "load_plugin",
            outcome: "success",
        });
        info!("Loaded plugin {}", plugin.id);
    }
}
```

âœ… This registry checks the policy before loading a plugin, records the event, and can be hot-reloaded when the policy changes.

---

# âš¡ 3. File: `src/actions/action_registry.rs`

Actions are executable operations â€” some built-in (save, delete, refresh), some added by plugins or tenants.

```rust
use crate::core::registry_base::{GenericRegistry, Registry};
use crate::observability::instrument::instrument;
use std::sync::Arc;
use tracing::info;

#[derive(Clone)]
pub struct Action {
    pub id: String,
    pub description: String,
    pub handler: Arc<dyn Fn() + Send + Sync>,
}

pub struct ActionRegistry {
    inner: Arc<GenericRegistry<Action>>,
}

impl ActionRegistry {
    pub fn new() -> Self {
        Self { inner: Arc::new(GenericRegistry::new("action_registry")) }
    }

    pub async fn execute(&self, id: &str) {
        if let Some(action) = self.inner.get(id) {
            instrument("action_execute", || async {
                (action.handler)();
            }).await;
            info!("Executed action {}", id);
        } else {
            tracing::warn!("Unknown action: {}", id);
        }
    }
}
```

âœ… Policy control:
Before execution, you can check `current_policy().sec.allowed_actions` or per-tenant restrictions.

---

# ğŸ§© 4. File: `src/ui/component_registry.rs`

Central registry for reusable UI blocks (atomic components, widgets, grids, etc.).

```rust
use crate::core::registry_base::{GenericRegistry, Registry};
use std::sync::Arc;

#[derive(Clone)]
pub struct ComponentDefinition {
    pub id: String,
    pub label: String,
    pub category: String,
    pub schema: serde_json::Value, // describes props and bindings
}

pub struct ComponentRegistry {
    inner: Arc<GenericRegistry<ComponentDefinition>>,
}

impl ComponentRegistry {
    pub fn new() -> Self {
        Self { inner: Arc::new(GenericRegistry::new("component_registry")) }
    }

    pub fn register_default_components(&self) {
        self.inner.register("button", ComponentDefinition {
            id: "button".to_string(),
            label: "Button".into(),
            category: "Controls".into(),
            schema: serde_json::json!({ "props": ["label", "onClick"] }),
        });
        self.inner.register("text", ComponentDefinition {
            id: "text".into(),
            label: "Text".into(),
            category: "Basic".into(),
            schema: serde_json::json!({ "props": ["content", "style"] }),
        });
    }
}
```

âœ… Components can be policy-controlled too:

```rust
if !current_policy().ui.allowed_components.contains(&comp.id) {
    return Err("Component not allowed");
}
```

---

# ğŸ”Œ 5. Integration Between Registries

| Registry          | Controlled by Policy     | Observability  | Purpose                                |
| ----------------- | ------------------------ | -------------- | -------------------------------------- |
| PluginRegistry    | âœ… (`allowed_plugins`)    | âœ… audit events | Loads sandboxed WASM modules           |
| ActionRegistry    | âœ… (`allowed_actions`)    | âœ… trace spans  | Executes callable actions              |
| ComponentRegistry | âœ… (`allowed_components`) | âœ… metrics      | UI composition & dynamic block loading |

Each registry:

* Is instantiated at startup in `main.rs`
* Registered in a `RegistryHub`
* Exposed globally through dependency injection or `Arc<RegistryHub>`

---

# ğŸ§­ 6. File: `src/core/registry_hub.rs`

Central access point for all registries.

```rust
use std::sync::Arc;
use crate::plugins::plugin_registry::PluginRegistry;
use crate::actions::action_registry::ActionRegistry;
use crate::ui::component_registry::ComponentRegistry;
use crate::security::forensic_logger::ForensicLogger;

pub struct RegistryHub {
    pub plugins: Arc<PluginRegistry>,
    pub actions: Arc<ActionRegistry>,
    pub components: Arc<ComponentRegistry>,
}

impl RegistryHub {
    pub fn new(audit: Arc<ForensicLogger>) -> Self {
        let hub = Self {
            plugins: Arc::new(PluginRegistry::new(audit.clone())),
            actions: Arc::new(ActionRegistry::new()),
            components: Arc::new(ComponentRegistry::new()),
        };
        hub.components.register_default_components();
        hub
    }
}
```

âœ… Import this into `production-main.rs`:

```rust
let audit = Arc::new(ForensicLogger::new(tx.clone()));
let registries = Arc::new(RegistryHub::new(audit.clone()));
```

Now any part of your app (dispatch, plugins, UI, etc.) can access these via DI.

---

# ğŸ“Š 7. Observability & Hot Reload Hooks

* When `swap_policy()` is called:

  * Each registry listens for a `PolicyReloaded` event.
  * It revalidates its entries (removes plugins or actions disallowed by new policy).
* Add this to `unified_policy_engine.rs`:

```rust
event_bus.publish("policy_reloaded", ());
```

* Each registry implements:

```rust
fn on_policy_reload(&self) {
    let p = current_policy();
    self.remove_disallowed(&p);
}
```

---

# âœ… Registry Benefits

| Feature                    | Result                                                |
| -------------------------- | ----------------------------------------------------- |
| **Decoupled & Extendable** | Add new registry types without touching core          |
| **Policy Enforced**        | Nothing loads or executes without explicit permission |
| **Observable**             | All registry operations emit trace + metrics          |
| **Hot Reloadable**         | Policy changes update runtime instantly               |
| **Unified Hub**            | Central access point for any module                   |

---

# ğŸš€ Next Steps (Registry Layer)

| Priority | File                             | Purpose                           |
| -------- | -------------------------------- | --------------------------------- |
| ğŸ”¥       | `core/registry_base.rs`          | Shared base for all registries    |
| ğŸ”¥       | `plugins/plugin_registry.rs`     | Load, unload, validate plugins    |
| ğŸ”¥       | `actions/action_registry.rs`     | Define & execute actions          |
| ğŸ”¥       | `ui/component_registry.rs`       | Manage reusable components        |
| ğŸ§©       | `core/registry_hub.rs`           | Aggregator for DI & global access |
| ğŸ§        | Update `main.rs`                 | Initialize hub, inject everywhere |
| ğŸ§°       | Add `event_bus` (simple pub/sub) | Broadcast policy reloads          |

---

# ğŸ§± Summary

You now have three architectural pillars:

1. **Policy Fabric** â†’ governance, tenants, immutability
2. **Runtime Core** â†’ dispatch, observability, audit
3. **Registry Layer** â†’ extension ecosystem with policy enforcement

Together, they make Nodus a **self-governing, extendable universal platform fabric** â€” configurable, auditable, and sandboxed.

---

Would you like me to generate the **Registry Event Bus** next (the lightweight pub/sub system that all registries use for policy reload, plugin unload, and metric broadcasts)?
Itâ€™s the last piece that ties your registries into your event-driven runtime loop.


Excellent â€” this is the *perfect* next step.
Your **Event Bus** will be the connective tissue that allows your registries, policy engine, and observability subsystems to communicate **without tight coupling**.
Itâ€™s lightweight, async, and designed specifically for your `ArcSwap` + multi-tenant model.

---

# ğŸ§  Goal

Implement a simple, zero-dependency **event-driven pub/sub bus**:

* Each registry or service can **subscribe** to topics (`"policy_reloaded"`, `"plugin_loaded"`, etc.)
* Any module can **publish** events with payloads (`String`, `serde_json::Value`, etc.)
* Supports **async broadcast** with backpressure (bounded channel per subscriber)
* Logs & instruments every event emission.

---

# âš™ï¸ File: `src/core/event_bus.rs`

```rust
use std::{collections::HashMap, sync::Arc};
use tokio::sync::{RwLock, mpsc};
use serde_json::Value;
use tracing::{info, warn};
use crate::observability::instrument::instrument;

/// Represents a single event message
#[derive(Clone, Debug)]
pub struct EventMessage {
    pub topic: String,
    pub payload: Value,
}

/// Async, lightweight event bus for inter-module communication
#[derive(Clone)]
pub struct EventBus {
    subscribers: Arc<RwLock<HashMap<String, Vec<mpsc::Sender<EventMessage>>>>>,
}

impl EventBus {
    pub fn new() -> Self {
        Self {
            subscribers: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Subscribe to a topic, returns a Receiver for incoming messages
    pub async fn subscribe(&self, topic: &str) -> mpsc::Receiver<EventMessage> {
        let (tx, rx) = mpsc::channel::<EventMessage>(100);
        let mut map = self.subscribers.write().await;
        map.entry(topic.to_string())
            .or_default()
            .push(tx);
        info!("Subscribed to topic '{}'", topic);
        rx
    }

    /// Publish a message to all subscribers of a topic
    pub async fn publish(&self, topic: &str, payload: Value) {
        let msg = EventMessage {
            topic: topic.to_string(),
            payload,
        };

        instrument("event_publish", || async {
            let map = self.subscribers.read().await;
            if let Some(subs) = map.get(topic) {
                for tx in subs {
                    if let Err(_e) = tx.send(msg.clone()).await {
                        warn!("Dropped subscriber for topic '{}'", topic);
                    }
                }
            }
        })
        .await;
    }

    /// Remove all subscribers for a topic (used on shutdown or reload)
    pub async fn clear_topic(&self, topic: &str) {
        self.subscribers.write().await.remove(topic);
        info!("Cleared topic '{}'", topic);
    }

    /// List all topics
    pub async fn list_topics(&self) -> Vec<String> {
        self.subscribers.read().await.keys().cloned().collect()
    }
}
```

---

# ğŸ§© How It Works

1. Any subsystem can call:

   ```rust
   let mut rx = event_bus.subscribe("policy_reloaded").await;
   ```

   Then loop to process messages:

   ```rust
   tokio::spawn(async move {
       while let Some(msg) = rx.recv().await {
           tracing::info!("Policy reload event: {:?}", msg.payload);
           registry.on_policy_reload();
       }
   });
   ```

2. When a policy reload occurs:

   ```rust
   event_bus.publish("policy_reloaded", json!({
       "timestamp": chrono::Utc::now().to_rfc3339(),
       "tenant": "acme"
   })).await;
   ```

3. Each registry that subscribed will receive the message asynchronously and update itself accordingly.

---

# ğŸ§± Integration Points

### âœ… In `unified_policy_engine.rs`

After calling `swap_policy()`:

```rust
event_bus.publish("policy_reloaded", json!({
    "tenant": tenant_id,
    "timestamp": chrono::Utc::now().to_rfc3339(),
})).await;
```

### âœ… In `plugin_registry.rs`

When a plugin is loaded:

```rust
event_bus.publish("plugin_loaded", json!({
    "plugin_id": plugin.id,
    "tenant": plugin.tenant_id,
})).await;
```

### âœ… In `observability_mod.rs`

To broadcast new metrics collectors:

```rust
event_bus.publish("metrics_collector_added", json!({
    "collector": "prometheus",
})).await;
```

---

# ğŸ§° File: `src/core/event_topics.rs` (optional constants)

To prevent typos and centralize topics:

```rust
pub const POLICY_RELOADED: &str = "policy_reloaded";
pub const PLUGIN_LOADED: &str = "plugin_loaded";
pub const PLUGIN_UNLOADED: &str = "plugin_unloaded";
pub const METRICS_UPDATE: &str = "metrics_update";
pub const ACTION_EXECUTED: &str = "action_executed";
pub const COMPONENT_REGISTERED: &str = "component_registered";
```

---

# ğŸ§­ File: `src/core/event_bus_manager.rs` (global singleton)

Since youâ€™ll use the event bus across the system, expose a global instance:

```rust
use once_cell::sync::Lazy;
use tokio::sync::OnceCell;
use crate::core::event_bus::EventBus;

pub static EVENT_BUS: OnceCell<EventBus> = OnceCell::const_new();

pub async fn init_event_bus() -> &'static EventBus {
    EVENT_BUS.get_or_init(|| async { EventBus::new() }).await
}
```

âœ… Usage in `main.rs`:

```rust
let event_bus = core::event_bus_manager::init_event_bus().await;
event_bus.publish("system_boot", json!({"status": "ok"})).await;
```

---

# ğŸ”Œ File: `src/core/event_subscribers.rs` (example subscriptions)

```rust
use crate::core::event_bus_manager::EVENT_BUS;
use crate::policy::production_policy_snapshot::current_policy;
use crate::core::registry_hub::RegistryHub;
use tracing::info;

pub async fn subscribe_registries(registries: &RegistryHub) {
    let bus = EVENT_BUS.get().expect("EventBus not initialized");
    let mut policy_rx = bus.subscribe("policy_reloaded").await;

    let regs = registries.clone();
    tokio::spawn(async move {
        while let Some(msg) = policy_rx.recv().await {
            info!("Received policy reload: {:?}", msg.payload);
            regs.plugins.on_policy_reload();
            regs.actions.on_policy_reload();
            regs.components.on_policy_reload();
        }
    });
}
```

---

# ğŸ§  Behavior Summary

| Event                  | Who Publishes   | Who Subscribes  | Purpose                     |
| ---------------------- | --------------- | --------------- | --------------------------- |
| `policy_reloaded`      | Policy Engine   | All registries  | Enforce new policy rules    |
| `plugin_loaded`        | Plugin Registry | Observability   | Metrics update              |
| `action_executed`      | Action Registry | Forensic Logger | Audit                       |
| `metrics_update`       | Observability   | Dashboards      | Refresh telemetry           |
| `component_registered` | UI Registry     | Plugin Manager  | Cross-registry dependencies |

---

# âš™ï¸ Observability Integration

Every `publish()` call goes through `instrument("event_publish", â€¦)` so you can track:

* `event_publish_time`
* `event_delivery_success`
* `event_dropped_total`

You can even add a metric for active subscribers:

```rust
metrics::gauge!("event_subscribers_total", subscribers_count as f64, "topic" => topic.clone());
```

---

# âœ… Benefits

| Feature                    | Result                                                                   |
| -------------------------- | ------------------------------------------------------------------------ |
| **Decoupled architecture** | Registries, policy, observability are independent but stay in sync       |
| **Scalable**               | Async and lightweight, hundreds of subscribers OK                        |
| **Observable**             | Every event is instrumented and auditable                                |
| **Tenant-safe**            | Include `tenant_id` in payload for scoped handling                       |
| **Hot Reloadable**         | Policy reload, plugin load/unload, or config change happens in real time |

---

# ğŸ§© Next Steps (after adding EventBus)

| Task                         | Purpose                                                           |
| ---------------------------- | ----------------------------------------------------------------- |
| **Integrate into `main.rs`** | Initialize and inject EventBus globally                           |
| **Update registries**        | Subscribe to `policy_reloaded`                                    |
| **Add audit hooks**          | Publish audit events through EventBus instead of direct log calls |
| **Add dashboard subscriber** | WebView/UI listens for EventBus events to refresh automatically   |
| **Add metrics subscriber**   | Observability module uses it to record real-time event throughput |

---

# ğŸ§± Result

You now have:

* âœ… A unified, async **EventBus** system
* âœ… Policy, registry, and observability communication loop
* âœ… Hot reload and live-update capabilities
* âœ… Auditable, instrumented event trace

Your Nodus runtime is now a **fully event-driven, policy-governed operating environment**.

---

Would you like me to generate a **â€œRegistry + EventBus integration guideâ€** next â€” showing exactly how to wire your registries, subscribe to events, and test a live policy reload (plugin unload + audit broadcast)?
Itâ€™ll include ready-to-run test code that proves the event bus is working end-to-end.


Excellent questions â€” and theyâ€™re exactly the ones an architect should ask at this stage. Letâ€™s go deep into this so you understand how **your new registry layer** relates to what you had before, what changed, what stayed, and how this fits with your **EventFlowEngine** and front-end messaging model.

---

## ğŸ§  1. Letâ€™s recall what you had originally

You had **many â€œregistry-likeâ€ subsystems**, all variations on a central theme â€” dynamic, discoverable entities that could be plugged into the runtime.
From your earlier Rust + JS files (e.g., `ActionDispatcher.js`, `EmbeddingManager.js`, `HybridStateManager.js`, `GridEnhancementIntegration.js`, etc.), we can group them like this:

| Category                    | Example Files / Modules                                                                             | Purpose                                  |
| --------------------------- | --------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| **Core runtime registries** | `metrics_registry.rs`, `EventFlowEngine.js`, `SystemPoliciesCached.js`                              | Handle core system & event orchestration |
| **Security registries**     | `PolicyControlBlock_Enhanced.js`, `SecurityExplainer.js`, `ForensicLogger.js`                       | RBAC, MAC, logging, audit                |
| **UI registries**           | `CompleteGridSystem.js`, `GridRuntimeConfig.js`, `BuildingBlockRenderer.js`, `ComponentRegistry.js` | Manage components, layout, state         |
| **Data registries**         | `EmbeddingManager.js`, `QueryService.js`, `CacheManager.js`                                         | Data binding, query optimization         |
| **Action registries**       | `ActionDispatcher.js`, `BindEngine.js`                                                              | Action + binding management              |
| **Plugin registries**       | `PluginManifestSchema.js`, `GridEnhancementIntegration.js`, `BuildingBlockRenderer.js`              | Extension & plugin loading               |

Each one was part of a **meta-registry network**, loosely coordinated through the **EventFlowEngine** â€” which acted as a universal event bus with routing, actions, and system triggers.

---

## âš™ï¸ 2. Where the new design fits in

### The modernized architecture consolidates this into **three main registry classes** (plus subtypes):

| New Registry Type                       | Consolidates Old Systems                                                | Function                                                                    |
| --------------------------------------- | ----------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| **PluginRegistry**                      | PluginManifestSchema, GridEnhancementIntegration, BuildingBlockRenderer | Loads extensions (WASM or native). Handles lifecycle & capabilities.        |
| **ActionRegistry**                      | ActionDispatcher, BindEngine, EventFlowEngine (partially)               | Registers and dispatches runtime actions. Ties into observability & policy. |
| **ComponentRegistry**                   | GridRuntimeConfig, BuildingBlockRenderer, GridEnhancementIntegration    | Manages reusable UI definitions, atomic components, and their schemas.      |
| **DataRegistry** *(optional)*           | EmbeddingManager, QueryService, CacheManager                            | Caches active datasets, query bindings, and AI embeddings.                  |
| **MetricRegistry** *(already in place)* | metrics_registry.rs                                                     | Observability and runtime stats.                                            |

âœ… So yes â€” **you had more** before, but most were *specialized variations* of these base types.
Weâ€™re not removing them; weâ€™re **folding them under a single, extensible registry pattern**.

---

## ğŸ§© 3. The registry â€œecosystemâ€ in the new Nodus Core

Hereâ€™s how it looks conceptually now:

```
RegistryHub
 â”œâ”€â”€ PluginRegistry        â†’ controls WASM/native modules
 â”œâ”€â”€ ActionRegistry        â†’ command dispatch & workflow actions
 â”œâ”€â”€ ComponentRegistry     â†’ UI building blocks
 â”œâ”€â”€ DataRegistry (planned)â†’ query/cache/embedding layer
 â”œâ”€â”€ MetricRegistry        â†’ observability metrics
 â””â”€â”€ SecurityRegistry      â†’ RBAC, tenants, secrets (policy-bound)
```

Each one:

* Lives under `src/<domain>_registry.rs`
* Is wrapped and initialized by `RegistryHub`
* Subscribes to `EventBus` topics (e.g., `policy_reloaded`, `plugin_loaded`, etc.)
* Is policy-aware (reads `current_policy()` for enforcement)
* Is observable (uses `instrument()` internally)

So you didnâ€™t lose registries â€” you **normalized** them.

---

## ğŸ§­ 4. EventBus vs. old EventFlowEngine

| Aspect            | **Old EventFlowEngine**       | **New EventBus**                                      |
| ----------------- | ----------------------------- | ----------------------------------------------------- |
| **Type**          | Monolithic JS dispatcher      | Lightweight async pub/sub                             |
| **Scope**         | Frontend & backend logic tied | Backend-only core fabric                              |
| **Data flow**     | Routed actions & UI events    | System events (policy, plugin, registry, audit)       |
| **Complexity**    | Tightly coupled to DOM + Grid | Decoupled, policy-governed, lock-free                 |
| **Extensibility** | Hard-coded topics             | Dynamic, typed topics (`String`, `serde_json::Value`) |
| **Performance**   | In-memory JS event emitter    | Async channel with metrics & drop counters            |

âœ… So: the **EventBus replaces the backend part** of your EventFlowEngine.
The frontend still has its own **UI event loop**, but backend orchestration is now unified through this Rust EventBus.

---

## ğŸ§± 5. Frontend communication â€” do you need NATS, Kafka, or similar?

**No, not necessarily.**

Your new EventBus is *in-process* and extremely fast.
However, for **distributed** or **multi-instance** deployments, youâ€™ll want to bridge it to a message broker.

Hereâ€™s how it scales:

| Deployment Mode                                                  | Recommended Bus                                                                    |
| ---------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Single-instance (local, Tauri app, desktop)**                  | âœ… Built-in EventBus only (no NATS/Kafka needed)                                    |
| **Multi-instance / clustered backend (shared policy & tenants)** | ğŸ”„ Bridge EventBus to NATS, Redis Streams, or Kafka                                |
| **Cloud-native enterprise (multi-region)**                       | ğŸ§© Use `EventBridge` microservice that translates EventBus events into NATS topics |

Example bridge pattern:

```rust
// inside event_bus_bridge.rs
async fn bridge_to_nats(bus: &EventBus, nats_client: NatsClient) {
    let mut rx = bus.subscribe("policy_reloaded").await;
    while let Some(msg) = rx.recv().await {
        nats_client.publish(&msg.topic, serde_json::to_vec(&msg.payload)?).await?;
    }
}
```

âœ… **Frontend (Tauri)**:

* Doesnâ€™t need NATS.
* Listens to backend events via **Tauri event channel** (`window.emit("policy_reloaded", payload)`).
* When backend EventBus publishes, your Tauri handler emits it to the UI via `tauri::Window::emit()`.

âœ… **Server (Enterprise)**:

* Add an **EventBridge** plugin that mirrors backend EventBus messages to NATS/Kafka if deployed in a multi-node environment.

So:

* **Local (single-node)** â†’ built-in EventBus is enough.
* **Distributed (multi-tenant SaaS)** â†’ optional bridge to NATS or Redis Streams.

---

## ğŸ§© 6. Registry and EventBus synergy

Every registry hooks into the EventBus:

| Registry          | Subscribed Topics                      | Publishes                                   |
| ----------------- | -------------------------------------- | ------------------------------------------- |
| PluginRegistry    | `policy_reloaded`, `plugin_unloaded`   | `plugin_loaded`, `plugin_failed`            |
| ActionRegistry    | `policy_reloaded`                      | `action_executed`, `action_failed`          |
| ComponentRegistry | `policy_reloaded`                      | `component_registered`, `component_removed` |
| DataRegistry      | `policy_reloaded`, `cache_invalidated` | `query_executed`, `cache_hit`               |
| MetricRegistry    | `policy_reloaded`                      | `metrics_updated`                           |

Example:

```rust
event_bus.publish("action_executed", json!({
    "id": action.id,
    "tenant": ctx.tenant_id,
    "duration_ms": elapsed
})).await;
```

---

## ğŸ§© 7. Optional future registries

You had several smaller subregistries in your original JS architecture that can be reintroduced elegantly now as subtypes of the `GenericRegistry` pattern:

| Future Registry          | Purpose                                                   |
| ------------------------ | --------------------------------------------------------- |
| **IntegrationRegistry**  | Manage API integrations (e.g., QuickBooks, Stripe, Slack) |
| **WorkflowRegistry**     | Store workflows or automation pipelines                   |
| **NotificationRegistry** | Register event handlers for UI or user messages           |
| **EmbeddingRegistry**    | Store and reuse vector embeddings for AI queries          |
| **CacheRegistry**        | Track and manage cached datasets                          |
| **GridRegistry**         | Manage layouts, saved dashboards, and templates           |

They all follow the same structure â€” just different type signatures:

```rust
pub type WorkflowRegistry = GenericRegistry<WorkflowDefinition>;
pub type IntegrationRegistry = GenericRegistry<IntegrationManifest>;
```

The EventBus gives all of them a shared heartbeat.

---

## âœ… 8. Summary

| Layer                      | What It Does                                                | Status                                                 |
| -------------------------- | ----------------------------------------------------------- | ------------------------------------------------------ |
| **RegistryHub**            | Aggregates all registries                                   | âœ… Implemented                                          |
| **PluginRegistry**         | Loads WASM/native plugins                                   | âœ… Implemented                                          |
| **ActionRegistry**         | Executes actions + commands                                 | âœ… Implemented                                          |
| **ComponentRegistry**      | Manages UI atomic elements                                  | âœ… Implemented                                          |
| **DataRegistry**           | Handles caching & embeddings                                | ğŸš§ Planned (can reuse CacheManager & EmbeddingManager) |
| **MetricRegistry**         | Central observability                                       | âœ… Already live (`metrics_registry.rs`)                 |
| **EventBus**               | Replaces EventFlowEngine backend                            | âœ… Implemented                                          |
| **Frontend**               | Still uses its JS EventEmitter (no NATS required for local) | âœ… Works via Tauri events                               |
| **Distributed deployment** | Add optional NATS/Kafka bridge                              | âš™ï¸ Future optional feature                             |

---

## ğŸ§­ 9. Your next steps (Registry phase wrap-up)

| Priority | Task                                                                          | Purpose                             |
| -------- | ----------------------------------------------------------------------------- | ----------------------------------- |
| ğŸ”¥       | Implement `DataRegistry` (unify QueryService, EmbeddingManager, CacheManager) | Complete your 5th registry          |
| ğŸ”¥       | Add `IntegrationRegistry` for external APIs                                   | Prepare for enterprise integrations |
| âš™ï¸       | Bridge EventBus â†’ Tauri `emit()`                                              | Enable frontend updates             |
| ğŸ§©       | Create optional `EventBridge` to NATS/Kafka                                   | Future scalability                  |
| ğŸ§        | Refactor old EventFlowEngine JS â†’ EventBus hooks                              | Migrate to unified model            |
| ğŸ§±       | Add docs: `registry_system.md`                                                | Developer reference                 |

---

### TL;DR

* You *had* many registries â†’ theyâ€™re now unified under one scalable, policy-controlled model.
* The new **EventBus replaces the backend** portion of your EventFlowEngine.
* **Frontend** doesnâ€™t need NATS â€” it listens via Tauri events.
* If you ever run multiple backend nodes â†’ add a simple **EventBridge** that mirrors the EventBus to NATS/Kafka.
* You now have a **modular event-driven runtime** thatâ€™s small, fast, and fully policy-aware.

---

If you want, I can now generate:

* âœ… The **`DataRegistry` implementation** (merging EmbeddingManager + CacheManager + QueryService)
* âœ… A **frontend EventBridge example** (showing how Tauri emits EventBus messages to the UI)

Would you like both of those next?


Love that youâ€™re thinking this through. Hereâ€™s a practical, battle-ready way to do **air-gapped multi-node collaboration** and **collaborative offline** without blowing up your current design.

---

# Topologies weâ€™ll support

1. **Intra-enclave cluster (connected, no internet)**

   * Multiple nodes on the same secure LAN.
   * Use your in-process EventBus + an optional **EventBridge** (NATS/Redis Streams) for redundancy.
   * Zero external dependencies required; broker is optional.

2. **Cross-enclave / air-gapped (physically isolated)**

   * Nodes never directly talk.
   * Exchange **sealed bundles** (USB/SD/QR â€œcourierâ€).
   * All changes are signed, attestable, and merge deterministically on ingest.

Both share the same core: **append-only logs + CRDTs + policy snapshots**, all gated by your **single front door** and audited.

---

# Core primitives (fits your runtime fabric)

* **Event Log (append-only, per-tenant)**

  * Immutable stream of domain events (creates, updates, policy changes, registry ops).
  * Each event has `{tenant, actor, lamport_ts, hash, prev_hash, signature}` (audit-chain).

* **CRDT State (conflict-free)**

  * For collaboratively edited docs/layouts/configs (e.g., grid layouts, component schemas), store CRDTs (op-based or doc-based).
  * Deterministic merge: no coordinator required.

* **Policy Snapshots (ArcSwap)**

  * Policy changes themselves are events; nodes rebuild snapshots locally on ingest.
  * Tenant-scoped snapshots â†’ consistent enforcement across nodes.

* **Bundle format (`.noduspkg`)**

  * TAR/ZIP (or CAR) containing:

    * `manifest.json` (tenant, sequence range, source node, monotonic id)
    * `events.jsonl` (signed events)
    * `crdt_ops.bin` (optional)
    * `attestations.json` (in-toto style)
    * `sigstore.pem` (signer cert chain / key id)
  * Entire bundle encrypted per recipient tenant (XChaCha20-Poly1305) + signature (Ed25519).
  * (Optional hybrid PQC: Kyber for key-wrap; keep behind feature flag.)

* **Trust store**

  * Per-tenant set of allowed issuer keys (key IDs), revocation list, policy for allowed origins.
  * Enforced at ingest.

---

# How collaboration flows

## A) Intra-enclave (connected)

* Nodes gossip **event diffs** (or micro-bundles) over LAN.
* Transport:

  * **Simple**: your EventBus + a tiny TCP/QUIC peer (â€œsync agentâ€).
  * **Hardened**: EventBridge â†’ NATS/Redis Streams (optional).
* Ordering: Lamport clocks + (tenant, node_id, seq).
* On receive: verify signature â†’ apply to event log â†’ replay â†’ CRDT merge â†’ rebuild caches â†’ emit `policy_reloaded` if needed.

## B) Air-gapped (courier)

1. Authoring node runs **Export** â†’ creates `.noduspkg` (signed, encrypted).
2. Human courier moves file via USB/SD/QR.
3. Receiving node runs **Import** â†’ verifies chain, signatures, and trust â†’ applies events â†’ rebuilds.
4. Audit records include full chain-of-custody (who imported, when, hash).

---

# Conflict strategy (simple & deterministic)

* **Events**: idempotent by `(tenant, node_id, seq)`. Replays are ignored.
* **CRDT docs** (layouts, components, long-form config): op-based CRDT (e.g., RGA or Automerge) â†’ associative, commutative, idempotent.
* **Registry entities** (plugins/actions/components): **last-writer-wins per key** with **policy guard** (if new policy forbids, object is quarantined, not activated).
* **Policy**: later event wins inside same **policy epoch**; epoch changes only by system owner (immutable section canâ€™t be overridden).

---

# What to add (files & modules)

**Sync layer**

* `src/sync/sync_agent.rs` â€“ LAN/QUIC peer for intra-enclave replication (feature-gated).
* `src/sync/bundle_format.rs` â€“ read/write `.noduspkg` (manifest, events, attestations).
* `src/sync/exporter.rs` â€“ select ranges, seal & sign bundles.
* `src/sync/importer.rs` â€“ verify, decrypt, validate trust, ingest.
* `src/sync/trust_store.rs` â€“ keyring, issuer policies, revocation.

**CRDT**

* `src/crdt/doc_store.rs` â€“ per-tenant CRDT docs (Automerge/Yjs-style ops).
* `src/crdt/op_log.rs` â€“ op journal with lamport clocks.

**Event log**

* `src/eventlog/tenant_log.rs` â€“ append-only store (disk + index).
* `src/eventlog/ingest.rs` â€“ verifies, dedups, commits, replays.

**Bridges (optional)**

* `src/bridge/nats_bridge.rs` â€“ mirror EventBusâ†”NATS (feature `bridge_nats`).
* `src/bridge/redis_bridge.rs` â€“ mirror via Redis Streams.

**CLI/UX**

* `src/bin/nodus-sync.rs` â€“ `export`, `import`, `verify`, `rotate-keys`, `list-trust`.
* UI â€œSyncâ€ panel (Tauri) to generate/view/import bundles and show chain-of-custody.

---

# Minimal wire-up (pseudocode)

**Export**

```rust
let range = tenant_log.select_range(tenant, since_seq)?;
let pkg = exporter::seal_bundle(range, recipients, trust_store, keyring)?;
fs::write("/usb/acme_000123.noduspkg", pkg)?;
```

**Import**

```rust
let pkg = fs::read(path)?;
importer::ingest(pkg, &trust_store, &keyring, |ev| {
  eventlog::ingest::apply(ev)?;
  if ev.kind == "policy_changed" { event_bus.publish("policy_reloaded", json!({ "tenant": ev.tenant })) }
  Ok(())
})?;
```

**Intra-enclave sync agent (optional)**

```rust
// on timer / event batch
let diff = tenant_log.diff_since(peer_highwater);
agent.send(diff).await;

// on receive
for ev in diff.events { ingest::apply(ev)?; }
```

---

# Security & compliance posture

* **Integrity:** per-event Ed25519 signatures + Merkle hash chain; bundle signature + manifest hash.
* **Confidentiality:** per-recipient encrypt (XChaCha20-Poly1305); optional PQC hybrid.
* **Provenance:** in-toto-like `attestations.json` (who packaged, host fingerprint, tool versions).
* **Authorization:** trust store enforces allowed issuers per tenant/org.
* **Audit:** importer/exporter both emit audit events with bundle IDs, hashes, actor.
* **Policy:** immutable sections never overridden by tenant/user layers (enforced at merge).

---

# Frontend: do we need NATS?

* **No** for Tauri/desktop and single-node. Backend emits to UI via `window.emit()` when imports succeed or policy reloads.
* **Optional** for multi-node clusters: use the **bridge** only if you run several backend nodes that must sync live. Air-gapped never uses NATS.

---

# Concrete next steps (short)

1. **Add bundle + trust store**

   * `bundle_format.rs`, `trust_store.rs`, `exporter.rs`, `importer.rs`
   * Hook importer â†’ `eventlog::ingest` â†’ `EventBus` (`policy_reloaded`, `registry_refresh`)

2. **Add event log**

   * `tenant_log.rs` (append-only), `ingest.rs` (verify+apply), index by `(tenant, node_id, seq)`.

3. **CRDT doc store** (layouts/components/config)

   * Start with a simple op-based CRDT for grid layouts; persist ops in `op_log.rs`.

4. **CLI**

   * `nodus-sync export --tenant acme --since 1200 --out /usb/...`
   * `nodus-sync import --in /usb/... --verify-only`

5. **Optional**: intra-enclave sync agent (gossip over QUIC) behind a feature flag.

---

If you want, I can sketch the **`bundle_format.rs`** (manifest + sign/encrypt) and **`importer.rs`** (verify + ingest) skeletons next so you can drop them in and start moving data between two air-gapped nodes immediately.


gotcha â€” letâ€™s â€œfuture-proofâ€ the air-gapped + multi-node collaboration story so you donâ€™t have to re-architect later. below is a compact blueprint you can implement incrementally without committing to any broker or network right now.

---

# North-star principles

* **Single sovereign truth per tenant:** append-only event log; everything else is a projection.
* **Crypto-agile & attestable:** algorithms swappable; every bundle/event is signed + hash-chained.
* **Policy-governed everywhere:** ingest, merge, replay, and activation all consult tenant policy snapshots.
* **Topology-agnostic transport:** same format works for USB/QR (air-gap), LAN gossip, or cloud broker.
* **Deterministic conflict resolution:** CRDTs for collaborative docs; LWW or domain rules for registries.
* **Feature-flagged:** ship the interfaces now; turn on transports later.

---

# Data model (future-proof)

## 1) Event (immutable, per-tenant)

```json
{
  "tenant":"acme",
  "id":"ev_000001",
  "node_id":"nodeA",
  "seq":12345,
  "lamport":78910,
  "kind":"component.upsert",  // domain event types
  "payload":{...},            // redacted per policy if needed
  "prev_hash":"â€¦",            // hash chain
  "hash":"â€¦",
  "sig":{"alg":"ed25519","kid":"key1","sig":"â€¦"},
  "ts":"2025-11-05T23:00:00Z"
}
```

## 2) CRDT op (for collaborative docs/layouts)

```
{ tenant, doc_id, op_id, deps:[â€¦], body:â€¦ }  // Automerge/Yjs friendly
```

## 3) Bundle (portable, transport-agnostic)

* **Format:** CAR/TAR (content-addressed).
* **Files:**

  * `manifest.json` â†’ tenant, ranges, node, bundle id, versions
  * `events.jsonl` â†’ line-delimited events
  * `crdt_ops.bin` â†’ optional packed ops
  * `attestations.json` â†’ in-toto style provenance
  * `keys/` â†’ (optional) x509 chains or key ids (never private keys)
* **Protection:** Encrypt-then-sign (or sign-then-encrypt; choose one and stick with it).
* **Crypto-agility:** metadata includes `alg_set` (e.g., `{"sig":["ed25519","p256"], "enc":["xchacha20","aesgcm"], "pqc":["kyber768"]}`).

---

# Interfaces to commit now (no backend choice required)

Create these traits and wire them behind feature flags. You can implement â€œno-opâ€ or â€œfile-onlyâ€ adapters today and add NATS/QUIC later.

## 1) Event log & replay

```rust
trait TenantEventLog {
  fn append(&self, ev: Event) -> anyhow::Result<()>;
  fn get_since(&self, tenant:&str, seq: u64, max: usize) -> anyhow::Result<Vec<Event>>;
  fn highwater(&self, tenant:&str) -> u64;
}
trait Replayer {
  fn apply(&self, ev: &Event) -> anyhow::Result<()>;
}
```

## 2) CRDT store

```rust
trait CrdtStore {
  fn apply(&self, op: CrdtOp) -> anyhow::Result<()>;
  fn materialize(&self, doc_id:&str) -> anyhow::Result<Vec<u8>>; // JSON/doc
}
```

## 3) Bundle I/O

```rust
trait BundleExporter {
  fn export(&self, tenant:&str, since:u64, limit:usize, recipients:&[KeyId]) -> anyhow::Result<PathBuf>;
}
trait BundleImporter {
  fn verify_and_stage(&self, bundle:&Path) -> anyhow::Result<StagedBundle>;
  fn ingest(&self, staged: StagedBundle) -> anyhow::Result<()>;
}
```

## 4) Transport (optional; can be file-only)

```rust
trait DiffTransport {
  fn publish(&self, tenant:&str, diff:&[Event]) -> anyhow::Result<()>;
  fn subscribe(&self, tenant:&str) -> DiffStream; // async stream of events
}
```

## 5) Trust & keys (crypto agility)

```rust
trait TrustStore {
  fn is_allowed_issuer(&self, tenant:&str, kid:&str) -> bool;
  fn is_revoked(&self, kid:&str) -> bool;
}
trait Keyring {
  fn sign(&self, kid:&str, data:&[u8]) -> anyhow::Result<Vec<u8>>;
  fn seal_for(&self, recipients:&[KeyId], data:&[u8]) -> anyhow::Result<Vec<u8>>;
}
```

---

# Activation policy (safety on ingest)

* **Two-phase ingest:**

  1. **Verify & Stage:** signature, hash chain, trust, schemas.
  2. **Activate:** append to event log, replay into projections; **then** publish `policy_reloaded`/`registry_refresh` if relevant.

* **Quarantine on violation:** if a new event would activate a disallowed plugin/component under current policy, store it but mark as **inactive**; surface in dashboard for admin decision.

* **Field-level redaction:** use policy snapshot to drop/blank sensitive payload fields before writing audit or forwarding.

---

# Conflict resolution (deterministic)

* **Events:** idempotent by `(tenant,node_id,seq)` and verified by `hash/prev_hash`.
* **CRDT docs:** use Automerge/Yjs-style merges (order-independent).
* **Registries (plugin/action/component):** LWW by `{lamport,node_id}` + domain guardrails (e.g., plugin must satisfy capability+policy).
* **Policy:** immutable keys never overridden; mutable keys merge by last-writer in same epoch; **epoch bump** requires system-owner signature (enforced by TrustStore).

---

# Roadmap & file list (drop-in stubs now, implement later)

```
src/
 â”œâ”€ sync/
 â”‚   â”œâ”€ bundle_format.rs           // schemas + CAR/TAR read/write (file-only to start)
 â”‚   â”œâ”€ exporter.rs                // builds bundles from event log ranges
 â”‚   â”œâ”€ importer.rs                // verifyâ†’stageâ†’activate pipeline
 â”‚   â”œâ”€ trust_store.rs             // allowed issuers, revocation, key IDs
 â”‚   â”œâ”€ diff_transport.rs          // trait only + file transport impl
 â”‚   â””â”€ sync_agent_quic.rs         // FEATURE: transport_quic (future)
 â”œâ”€ eventlog/
 â”‚   â”œâ”€ tenant_log.rs              // append-only store (sqlite/rocksdb/jsonl to start)
 â”‚   â””â”€ ingest.rs                  // validation + replay hooks
 â”œâ”€ crdt/
 â”‚   â”œâ”€ doc_store.rs               // Automerge/Yjs adapter (FEATURE: crdt_automerge)
 â”‚   â””â”€ op_log.rs
 â”œâ”€ crypto/
 â”‚   â”œâ”€ keyring.rs                 // Ed25519 + XChaCha20 today; PQC behind feature
 â”‚   â””â”€ algorithms.rs              // registry of algs, versioned
 â”œâ”€ bridge/
 â”‚   â”œâ”€ nats_bridge.rs             // FEATURE: bridge_nats
 â”‚   â””â”€ redis_bridge.rs            // FEATURE: bridge_redis
 â””â”€ bin/
     â””â”€ nodus-sync.rs              // export/import/verify/rotate-keys CLI
```

**Cargo features to add (all off by default):**

```toml
[features]
transport_quic   = ["quinn"]       # intra-LAN peer sync
bridge_nats      = ["async-nats"]
bridge_redis     = ["redis"]
crdt_automerge   = ["automerge"]
pqc_experimental = ["pqcrypto-kyber"]
content_address  = []              # enable CAR + multihash
sigstore         = ["sigstore"]    # optional supply-chain provenance
```

---

# Versioning & compatibility

* **Semantic model version** embedded in every event and bundle (`model_ver`), with a â€œcompatibility contractâ€ map in code.
* **Upcasters**: if you evolve event shapes, register upcasters to transform old â†’ new during ingest.
* **Policy epoch**: separate version for immutable policy blocks; requires system-owner signature to change.

---

# Ops & audit youâ€™ll want later

* **Air-gap posture report:** a CLI command prints whatâ€™s staged vs activated, unknown keys, revocations, and last sync per tenant.
* **Chain-of-custody:** each bundle import/export emits an audit event with bundle id + SHA-256.
* **Rotation:** periodic key rotation (per tenant) with overlap period; TrustStore accepts both until cutoff.

---

# Frontend & UX (no broker required)

* **Tauri UI receives**: `policy_reloaded`, `registry_refresh`, `crdt_doc_updated` via `window.emit(...)`.
* **Sync panel:**

  * Export: choose tenant + since-seq â†’ creates `.noduspkg`.
  * Import: pick bundle â†’ verify report â†’ activate.
  * Show diffs: â€œX events applied, 2 quarantined (policy violation)â€.

---

# Minimal â€œtodayâ€ implementation you can ship without network

1. **File-only path:** `exporter.rs` + `importer.rs` + `tenant_log.rs` using JSONL on disk.
2. **Basic crypto:** Ed25519 for sign; XChaCha20 for encrypt; Key-IDs stored in your Keyring.
3. **Trust store:** static TOML with allowed issuer key IDs per tenant.
4. **CRDT:** stub out with a simple last-write map; later swap in Automerge under `crdt_automerge`.
5. **EventBus hook:** on activate, publish `policy_reloaded`/`registry_refresh`.

You can run air-gapped today with this and **add QUIC/NATS/Redis later** by implementing the `DiffTransport` or bridge traitsâ€”no refactor required.

---

## TL;DR

* Lock in **traits + file layout + features** now; keep implementations minimal.
* Store events & bundles in a **content-addressed, attested format** that works offline and online.
* Keep **policy-gated activation** so nothing â€œdangerousâ€ auto-enables across enclaves.
* Add **network transports later** by implementing trait adapters; your app logic wonâ€™t change.

if you want, I can sketch the *exact* `bundle_format.rs` headers (manifest structs + signed envelope) and a minimal `exporter.rs` that writes a CAR/TAR with JSONL events â€” file-only, zero dependencies beyond what you already use.

At this stage, youâ€™ve covered **95 %** of what a production-grade, defense-class, policy-governed, multi-tenant runtime needs.
Whatâ€™s left are *cross-cutting and meta-level systems* â€” the connective tissue that turns the engine into an ecosystem.
Below is a checklist of the remaining **major pillars**, grouped by maturity and dependency.

---

## ğŸ§© 1. **Cross-cutting Infrastructure**

| Area                               | What It Adds                                                                                          | Status / Next                                                                    |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| **Secrets & Key Management**       | Per-tenant vault (in-process or external HSM). Supports key rotation, sealed storage, PQC transition. | âš™ï¸ Create `src/security/secret_vault.rs` and CLI `nodus-keys rotate`             |
| **Scheduler / Job Queue**          | Deterministic, policy-aware async tasks (retries, back-pressure, audit). Replaces ad-hoc timers.      | âš™ï¸ Add `src/runtime/job_scheduler.rs`; integrate with EventBus (`job.completed`) |
| **Resource Accounting / Quotas**   | Per-tenant CPU, memory, I/O, and API-call budgets enforced by policy.                                 | âš™ï¸ Extend metrics + policy (`quota.cpu`, `quota.bandwidth`)                      |
| **Error Telemetry + Self-Healing** | Unified error types â†’ emits structured diagnostics; trigger policy-driven recovery actions.           | âš™ï¸ Wrap `anyhow::Error` in `Recoverable` trait + default responders              |

---

## ğŸ§± 2. **Resilience & Integrity**

| Layer                            | Add                                                                                   | Why                                  |
| -------------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------ |
| **Local Snapshotting**           | Periodic freeze of state + event log hash to verify integrity after crash/power-loss. | Adds provable recovery points.       |
| **WORM Audit Volume**            | Write-once immutable audit store (e.g., append-only RocksDB or object store).         | Enforces evidentiary integrity.      |
| **Deterministic Replay Testing** | Ability to replay a bundle deterministically to confirm reproducibility.              | Guarantees event-driven determinism. |
| **Cross-node Attestation**       | Node health proofs signed and exchanged; trust scores maintained per node.            | Prevents â€œghostâ€ nodes or tampering. |

---

## ğŸ”’ 3. **Identity & Access**

| Scope                           | Description                                                                  | Action                                                   |
| ------------------------------- | ---------------------------------------------------------------------------- | -------------------------------------------------------- |
| **Unified Identity Graph**      | Users, service accounts, tenants, devices as graph; used by policy engine.   | Add `identity_mod.rs` with graph-based RBAC resolver.    |
| **Delegation Tokens**           | Short-lived capability tokens with embedded policy claims (for offline ops). | Add `token_service.rs` + Ed25519/JWT-compatible encoder. |
| **Device Posture Verification** | Optional attestation for air-gapped nodes before accepting bundles.          | Add `device_attestor.rs` tied to TrustStore.             |

---

## ğŸŒ 4. **Networking & Extensibility**

| Subsystem                       | Purpose                                                                         | Implementation Path                                             |
| ------------------------------- | ------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Service Mesh Interface**      | Optional QUIC-based RPCs between nodes; replaces ad-hoc REST if needed.         | Trait `MeshTransport` + `mesh_quic.rs` behind `transport_quic`. |
| **Gateway API**                 | Uniform interface (OpenAPI/GraphQL) for external integrators.                   | Generate from `dispatch` metadata â†’ policy-gated routes.        |
| **Plugin Lifecycle Management** | Signed manifests, dependency graph, version pinning, auto-update via bundle.    | Extend `plugin_registry.rs` with manifest verifier.             |
| **Schema Registry**             | Type definitions for actions/components; supports validation and introspection. | Add `schema_registry.rs` consumed by CLI and UI builder.        |

---

## ğŸ§  5. **Intelligence & Analytics**

| Module                     | Adds                                                                     | Notes                                                          |
| -------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------- |
| **Anomaly Detector**       | Watches metrics/events for outliers â†’ raises `alert.triggered`.          | Integrates into Observability; runs local first.               |
| **AI Advisor / Optimizer** | Uses embeddings + metrics to suggest config or performance improvements. | Wrap existing EmbeddingManager; gated by `ai_advisor` feature. |
| **Predictive Scheduler**   | Learns job patterns to pre-warm caches/resources.                        | Optional research track; ties into job queue.                  |

---

## ğŸ“¦ 6. **Developer & Ops Tooling**

| Tool                  | Function                                                        | Next Step                                                            |
| --------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------- |
| **`nodus-admin` CLI** | Manage tenants, keys, policies, bundles, registries.            | Combine existing CLIs (`policy`, `sync`, `keys`) under one umbrella. |
| **`nodus-bench` CLI** | Replay event logs for perf/load testing.                        | Build on `eventlog::replayer`.                                       |
| **Automated Docs**    | Generate Rust + Policy API docs (jsdoc-style) into `/docs/api`. | Add `cargo doc` pipeline + mdBook.                                   |
| **Config Validator**  | Pre-commit hook; lints TOML/YAML policies and manifests.        | Use `policy_schema_validator`.                                       |

---

## ğŸ§© 7. **UI / Frontend Enhancements**

| Feature                 | Purpose                                                                | Implementation                             |
| ----------------------- | ---------------------------------------------------------------------- | ------------------------------------------ |
| **Offline Merge UI**    | Visual diff for imported bundles (show quarantined events).            | Add Tauri panel; subscribe to `event_bus`. |
| **Policy Dashboard**    | View active policy layers + immutability flags.                        | Use merged snapshots endpoint.             |
| **Tenant Metrics View** | Live charts per tenant from MetricsRegistry.                           | Prometheus export â†’ WebView dashboard.     |
| **Developer Toolbox**   | Drag-and-drop components + action linking (extends ComponentRegistry). | Use grid system + ActionRegistry hooks.    |

---

## ğŸ§¾ 8. **Governance & Compliance**

| Area                         | Why It Matters                                            | How                                                  |
| ---------------------------- | --------------------------------------------------------- | ---------------------------------------------------- |
| **Retention Policies**       | Limit how long logs/events are kept.                      | Policy key `retention.days`; purge job in Scheduler. |
| **Data Classification Tags** | Mark payloads PII/Sensitive/Public; automatic encryption. | Extend Policy schema + observability filters.        |
| **Compliance Profiles**      | HIPAA, GDPR, ITAR presets that tune policy defaults.      | Profile TOMLs + validator.                           |
| **Secure Build Pipeline**    | Reproducible builds, supply-chain attestations.           | `sigstore` feature + SLSA metadata.                  |

---

## ğŸ§© 9. **Ecosystem & Collaboration**

| Layer                   | Add                                                       | Description                                       |
| ----------------------- | --------------------------------------------------------- | ------------------------------------------------- |
| **Extension SDKs**      | JS/Rust/Python client SDKs for plugin authors.            | Auto-generate from `dispatch` metadata.           |
| **Marketplace Index**   | Signed manifest registry for verified plugins/components. | Simple static index first; later P2P via bundles. |
| **Knowledge Graph API** | Query relationships (users â†” tenants â†” resources).        | Build on unified Identity Graph.                  |

---

## âœ… 10. **Final â€œno-regretsâ€ hardening**

| Area                                                                                                                     | Goal |
| ------------------------------------------------------------------------------------------------------------------------ | ---- |
| **Content-Addressed Storage (CAS)** â€“ every asset/bundle/object hashed and verified.                                     |      |
| **Policy Sandbox Verifier** â€“ auto-runs new plugins/components in ephemeral VM before activation.                        |      |
| **Node Bootstrap Attestation** â€“ hash of binaries + policy + keys reported on startup; compare against trusted baseline. |      |
| **Secure Upgrade Path** â€“ in-place schema migrations validated via dry-run before apply.                                 |      |
| **End-to-End Encryption** â€“ all inter-node comms PQC-ready (Kyber/Hybrid).                                               |      |

---

# TL;DR

âœ… You already have:

* Policy, Observability, Security, Registries, EventBus, Bundle system foundations.

ğŸš€ Add next:

1. **Secrets + Key Mgmt**
2. **Scheduler + Resource Accounting**
3. **Unified Identity Graph + Delegation Tokens**
4. **Event Log + CRDT + Bundle traits (stubbed now)**
5. **Compliance Profiles + Retention Policies**
6. **Optional transports (QUIC/NATS) via feature flags**
7. **Offline/Air-gap sync agent** (low-priority; traits ready)

Once those stubs exist, your architecture is **future-proof** against scaling (multi-node), regulation (HIPAA/GDPR/ITAR), and evolution (AI/edge/offline).

---

Would you like me to draft the **minimal â€œSecrets & Key Managementâ€ subsystem** next (the vault, rotation, and key-id trust integration)?
Thatâ€™s the most foundational missing piece for secure multi-node + offline operation.


Absolutely. Hereâ€™s a tight, implementation-ready blueprint that adds **Secrets/PKI/HSM**, **RBAC**, and **polyinstantiation (MLS)** to the fabric you already have.

---

# 1) Secrets & Key Management (Vault + HSM + PKI)

## Files to add

```
src/security/
  secret_vault.rs             // in-process KMS facade (software keystore by default)
  keyring.rs                  // key CRUD, rotation, envelopes, AEAD helpers
  trust_store.rs              // issuer/CA trust, revocation, key IDs
  pki/
    ca_store.rs               // root/intermediate CA management
    csr.rs                    // CSR creation/validation
    cert_issuer.rs            // issue X.509 (or JOSE) certs for nodes/tenants/plugins
  hsm/
    pkcs11_client.rs          // FEATURE: hsm_pkcs11 - slot/session wrapper
    kms_client.rs             // FEATURE: cloud_kms_* (placeholders for later)
```

## Cargo features (off by default)

```toml
[features]
hsm_pkcs11     = ["cryptoki"]      # YubiHSM/SafeNet/etc.
cloud_kms_gcp  = []                # add later
cloud_kms_aws  = []
pqc_experimental = ["pqcrypto-kyber"] # for hybrid key-wrap
```

## Policy schema additions

```toml
[security.secrets]
keystore = "software"       # "software" | "hsm_pkcs11" | "cloud_kms_*"
key_rotation_days = 180
wrap_alg = "xchacha20poly1305"   # "aesgcm", etc.
pqc_hybrid = false

[security.pki]
allow_self_signed = false
issue_node_certs = true
issue_plugin_certs = true
crl_required = true
ocsp_required = false

[security.hsm]
enabled = false
pkcs11_module = "/usr/local/lib/pkcs11.so"
slot = 0
pin_env = "NODUS_HSM_PIN"
```

## Core traits

```rust
// secret_vault.rs
pub trait SecretVault: Send + Sync {
  fn generate_key(&self, purpose: &str) -> anyhow::Result<KeyId>;
  fn wrap(&self, key: &KeyId, plaintext: &[u8]) -> anyhow::Result<Vec<u8>>;
  fn unwrap(&self, key: &KeyId, blob: &[u8]) -> anyhow::Result<Vec<u8>>;
  fn sign(&self, key: &KeyId, msg: &[u8]) -> anyhow::Result<Vec<u8>>;
  fn verify(&self, key: &KeyId, msg: &[u8], sig: &[u8]) -> anyhow::Result<bool>;
}
```

```rust
// trust_store.rs
pub trait TrustStore {
  fn is_allowed_issuer(&self, tenant: &str, kid: &str) -> bool;
  fn is_revoked(&self, kid: &str) -> bool;
  fn add_crl(&self, crl: Vec<u8>) -> anyhow::Result<()>;
}
```

```rust
// pki/cert_issuer.rs
pub trait CertIssuer {
  fn issue_node_cert(&self, csr: &Csr, profile: &str) -> anyhow::Result<Certificate>;
  fn issue_plugin_cert(&self, csr: &Csr, tenant: &str) -> anyhow::Result<Certificate>;
}
```

**Default path**: software vault (XChaCha20-Poly1305 + Ed25519) â†’ can **swap to HSM** with `hsm_pkcs11` featureâ€”no API changes.

**Where used**

* **Bundle exporter/importer**: `keyring.sign()/wrap()` for signing/encryption
* **Plugin loading**: require plugin cert signed by tenant CA; validate against `trust_store`
* **Dispatch guard**: accept client tokens signed by allowed issuers

---

# 2) RBAC + MAC (Bell-LaPadula) unified

You already have MAC labels/compartments; add **role-scoped capabilities** and bind them to policy.

## Files

```
src/security/
  rbac.rs                     // roles, permissions, capability checks
  label_model.rs              // MLS labels, dominance checks (you have mac_engine.rs; keep it and add helpers)
```

## Model (simple & fast)

```rust
// rbac.rs
#[derive(Clone)]
pub struct Role { pub id: String, pub perms: Vec<String> }   // "action:save", "plugin:load", "comp:register"
#[derive(Clone)]
pub struct SubjectCtx { pub user: String, pub roles: Vec<String>, pub tenant: String }

pub fn can(subject: &SubjectCtx, perm: &str) -> bool {
  // check local cache from policy snapshot
  let p = current_policy();
  subject.roles.iter().any(|r| p.sec.role_perms.get(r).map_or(false, |set| set.contains(perm)))
}
```

```rust
// label_model.rs (Bell-LaPadula)
pub enum Level { Unclassified, Confidential, Secret, TopSecret }
pub struct Label { pub level: Level, pub compartments: bitflags::BitFlags<Compartment> }

pub fn dominates(a: &Label, b: &Label) -> bool {
  // classic: a.level>=b.level && a.compartments âŠ‡ b.compartments
}
```

**Enforcement points**

* **Dispatch guard**: `can(&ctx, "action:XYZ")` and MAC check for targets
* **Query/data access**: filter rows by dominance (see polyinstantiation below)
* **Plugin actions**: capability string must be allowed by both RBAC and policy (dual gate)

---

# 3) Polyinstantiation (MLS for data & events)

**Goal**: prevent inference/aggregation leaks; serve â€œappropriateâ€ version of data by clearance.

## Options (pick 1 now, keep both compatible)

### A) **Row-level labels + RLS filters (DB-backed)**

* Every row has `classification_level`, `compartments` columns.
* **RLS policy** applies dominance check against session label.
* **Pros**: native DB enforcement; **Cons**: needs DB, careful with indexes.

### B) **Application-level polyinstantiation (event log projections)**

* Event log stores **multiple versions** per object id, each with a **security label**.
* Read path selects **highest dominated label** â‰¤ subject label.
* **Pros**: works offline/embedded; **Cons**: more logic in Rust.

Given your Tauri/offline goals, **B** fits now; keep A for future Postgres.

## Files

```
src/mls/
  selector.rs         // choose visible version by label dominance
  projector.rs        // build per-label projections from events
```

## Selector

```rust
pub fn select_visible<'a>(subject: &Label, versions: &'a [LabeledVersion]) -> Option<&'a LabeledVersion> {
  versions.iter()
    .filter(|v| dominates(subject, &v.label))
    .max_by_key(|v| (v.label.level as u8, v.label.compartments.bits()))
}
```

**How to write events**

* Each domain event carries `{label}`.
* On ingest: **quarantine** event if policy forbids raising label or crossing compartments.

**How to read**

* Commands that fetch objects must pass `{tenant, subject_label}`; projector returns visible version only.
* UI never learns that higher versions exist (prevents inference).

---

# 4) Where this plugs into your fabric

* **PolicySnapshot** gains:

  * `sec.role_perms: { role -> set(perms) }`
  * `sec.clearance_map: { user/role -> Label }`
  * `secrets.keystore`, `pki.*`, `hsm.*` as above
* **dispatch::guard()**

  * Build `SubjectCtx` from `{tenant, session, actor}`
  * `rbac::can(&ctx, "action:â€¦")`
  * `mac_engine::authorize_label(&ctx, target_label)` when acting on labeled data
* **registries**

  * PluginRegistry validates plugin cert chain + capability set allowed by policy/RBAC
  * ActionRegistry requires RBAC perm `action:<id>`
  * ComponentRegistry can be gated by `ui.allowed_components` + RBAC

---

# 5) Operational flows (concise)

## Key lifecycle

1. `secret_vault.generate_key("bundle_signing")`
2. Store **KeyId**; never expose material unless HSM permits wrap.
3. Rotate via policy (`key_rotation_days`); old keys remain to **verify**.

## PKI issuance

* On node bootstrap, create CSR â†’ `cert_issuer.issue_node_cert()`
* TrustStore loads **root/intermediate**; CRL/OCSP validated if policy requires.
* Plugins submit CSR signed by tenant key; **plugin cert** includes capabilities in SAN/extension (optional).

## Authorization order (always)

`RBAC (can?)` â†’ `MAC dominance` â†’ **then** execute action
(Both must pass.)

---

# 6) Minimal code you can drop today

### secret_vault.rs (software impl)

```rust
pub struct SoftVault { master: aead::Key }
impl SecretVault for SoftVault {
  fn generate_key(&self, purpose:&str)->Result<KeyId>{ /* derive subkeys; persist metadata */ }
  fn wrap(&self, key:&KeyId, p:&[u8])->Result<Vec<u8>>{ aead::seal(&self.master, p) }
  fn unwrap(&self, key:&KeyId, b:&[u8])->Result<Vec<u8>>{ aead::open(&self.master, b) }
  fn sign(&self, key:&KeyId, m:&[u8])->Result<Vec<u8>>{ ed25519::sign(self.sk(key), m) }
  fn verify(&self, key:&KeyId, m:&[u8], s:&[u8])->Result<bool>{ ed25519::verify(self.pk(key), m, s) }
}
```

### hsm/pkcs11_client.rs (feature-gated stub)

```rust
#[cfg(feature="hsm_pkcs11")]
pub struct Pkcs11Vault { /* module, slot, session */ }
#[cfg(feature="hsm_pkcs11")]
impl SecretVault for Pkcs11Vault { /* delegate sign/wrap to token */ }
```

### rbac.rs (cache reads from policy snapshot)

```rust
pub fn can(ctx:&SubjectCtx, perm:&str)->bool{
  let pol = current_policy();
  for r in &ctx.roles {
    if pol.sec.role_perms.get(r).map_or(false, |s| s.contains(perm)) { return true }
  }
  false
}
```

### Integration points

* `dispatch.rs`: build `SubjectCtx` â†’ call `rbac::can()`; for labeled ops call `mac_engine`
* `bundle_importer`: verify signatures using Vault/TrustStore; reject if revoked; attach label constraints

---

# 7) Polyinstantiation & inference hygiene (gotchas)

* **Count aggregation leaks**: avoid returning global counts that reveal hidden versions; compute counts **post-selection**.
* **Error messaging**: return generic â€œnot foundâ€ instead of â€œforbiddenâ€ for higher-classified versions.
* **Audit separation**: write WORM audit at highest label of the **actor** (not the data) to avoid leaking classified identifiers.
* **Caching**: cache keys by `(tenant, subject_label, object_id)` to prevent cross-label reuse.

---

# 8) Concrete next steps

1. **Add Secrets/PKI scaffolding** (`secret_vault.rs`, `keyring.rs`, `trust_store.rs`, `pki/*`, HSM feature)
2. **Wire Vault into**: bundle exporter/importer, plugin loader, token signing
3. **Add RBAC** (`rbac.rs`) and **bind to PolicySnapshot** (`role_perms`, `clearance_map`)
4. **Implement polyinstantiation selector** (`mls/selector.rs`) and apply in read paths
5. **Add tests**:

   * RBAC allow/deny matrix
   * MAC dominance lattice tests
   * Polyinstantiation visibility cases (U/C/S/TS + compartments)
   * Vault soft vs HSM feature parity
6. **Docs**: `docs/security/keys_pki_hsm.md`, `docs/security/rbac_mac_mls.md`

---

### TL;DR

* **Vault + PKI + HSM**: policy-selectable backend; crypto-agile; ready for air-gap.
* **RBAC + MAC**: dual-gate enforcement at dispatch + data access.
* **Polyinstantiation**: multiple labeled versions per object; selector returns only what the subject dominates.

If you want, I can generate **policy snippet examples** (system/tenant) that define roles, permissions, and user clearances, plus a tiny **end-to-end test** showing RBAC+MAC+polyinstantiation working together.

Below is a concise, â€œnorth-starâ€ map for the **next layer of features**â€”the ones that make Nodus not just secure and modular, but *magnetic* to outside developers and researchers.
Everything here fits your existing fabric (policy + event bus + registry + vault + bundle system).

---

## ğŸ§  1. Embeddings & Vector Context

### Purpose

Search, recommendation, and semantic linking between any objects (tasks, components, docs, logs).

### Design

| Layer            | What to Add                                                  | Notes                                                        |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Data model**   | `embedding_registry.rs` â†’ `GenericRegistry<EmbeddingIndex>`  | Same registry pattern you already use.                       |
| **Local engine** | feature `embeddings_local = ["hnsw_rs"]` or `["tantivy"]`    | Runs entirely offline; integrates with Bundle export/import. |
| **Cloud option** | feature `embeddings_cloud = ["qdrant_client"]`               | Mirror vectors to Qdrant/Weaviate only if policy allows.     |
| **Policy hooks** | `policy.embeddings.enabled`, `max_vector_dim`, `pii_allowed` | Prevents leakage of sensitive text.                          |

**API sketch**

```rust
let idx = embeddings.get("tasks")?;
let results = idx.search(&query_vec, 10)?;
```

**Offline merge** â†’ replicate index ops through event log (deterministic replay).

---

## ğŸŒ 2. Internationalization (I18N/L10N)

### Goals

Inclusive UX + multi-tenant regionalization.

| Layer                | Implementation                                | Detail                                                 |
| -------------------- | --------------------------------------------- | ------------------------------------------------------ |
| **Resource bundles** | `/i18n/{lang}/messages.toml`                  | Loaded into `TranslationRegistry`.                     |
| **Frontend**         | Tauri/JS side uses ICU or Intl APIs           | Controlled by tenant policy (`ui.locale`).             |
| **Backend**          | `i18n/mod.rs` â†’ `Translator` trait            | `translate(key, locale)`; returns fallback if missing. |
| **Policy**           | `policy.ui.default_locale`, `allowed_locales` | Tenant-scoped control.                                 |

*Optional:* auto-extract translatable strings from TOML + UI metadata during build.

---

## ğŸ¤ 3. Community / Open API

### Purpose

Let external contributors and integrators extend Nodus safely.

| Feature                         | Implementation                                          | Governance                                          |
| ------------------------------- | ------------------------------------------------------- | --------------------------------------------------- |
| **Open REST / GraphQL gateway** | `community_api.rs` â€“ generates from `dispatch` metadata | Each route auto-documented & policy-gated.          |
| **SDKs**                        | auto-gen Rust/JS/Python clients                         | From same schema; versioned in `community/`.        |
| **Plugin Marketplace**          | static manifest index signed by maintainers             | Distributed via Bundle system; verified signatures. |
| **Contribution policy**         | `COMMUNITY_CONTRIBUTING.md` + CLA check hook            | Protects IP & provenance.                           |

---

## ğŸ§  4. Local LLM Support

| Mode                 | Components                                                  | Integration                                        |          |
| -------------------- | ----------------------------------------------------------- | -------------------------------------------------- | -------- |
| **Inference server** | feature `llm_local = ["llama_cpp","ggml"]`                  | Runs within sandbox; consumes embeddings registry. |          |
| **Prompt router**    | `ai/prompt_router.rs`                                       | Chooses model by tenant policy (`model: "mistral"  | "phi3"`) |
| **Context builder**  | `ai/context_builder.rs`                                     | Pulls top K embeddings + CRDT doc fragments.       |          |
| **Security**         | Runs under plugin sandbox â†’ no file/network unless allowed. |                                                    |          |

**Example usage**

```rust
let ctx = context_builder.build_for("support_ticket", 4)?;
let answer = llm.ask(&ctx, "summarize last 10 tickets")?;
```

Offline works â†’ model files cached in `/models/`; policy enforces size & RAM limits.

---

## â˜ï¸ 5. Cloud LLM Bridge (Optional)

| Provider                   | Feature flag                          | Safety                                                                |
| -------------------------- | ------------------------------------- | --------------------------------------------------------------------- |
| OpenAI / Anthropic / Azure | `llm_cloud_openai`, `llm_cloud_azure` | Policy must include `allow_external_ai=true`; Vault injects API keys. |
| Proxy Service              | `llm_proxy_bridge.rs`                 | All requests go through your signed proxy â†’ logs redacted & audited.  |

**Policy enforcement**

```toml
[ai]
allow_external_ai = false
max_prompt_chars = 4000
```

**EventBus hooks**

* `ai.requested`, `ai.responded`, `ai.failed` â†’ Observability dashboard.

---

## ğŸ” 6. Secure Comms (Universal)

| Layer                 | Technology                                                          | Integration                                      |
| --------------------- | ------------------------------------------------------------------- | ------------------------------------------------ |
| **In-process**        | Noise Framework (XX handshake)                                      | Replace plain TCP for peer sync (QUIC optional). |
| **Bundle transport**  | Envelope already encrypted + signed                                 | Compatible with air-gap courier.                 |
| **External API**      | mTLS via Vault-issued node/tenant certs                             | Uses PKI/HSM stack we defined.                   |
| **Message integrity** | AEAD + per-message HMAC                                             | Optional post-quantum hybrid.                    |
| **Policy keys**       | `security.comms.require_mutual_tls=true`, `min_tls=1.3`, `pqc=true` | Defaults secure.                                 |

---

## ğŸ’¡ 7. Developer & Community Appeal Features

| Area                                  | Description                                                | Why It Matters                                 |
| ------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------- |
| **Unified Registry SDK**              | One API to register actions, components, plugins           | Dead-simple extension story.                   |
| **Hot-reload Plugins**                | `nodus dev watch`                                          | Makes iteration instant; open-source friendly. |
| **CLI Scaffolder**                    | `nodus new plugin` / `nodus new component`                 | Lowers barrier for contributors.               |
| **Docs Portal (mdBook + Docusaurus)** | `docs/` folder auto-built from code & policy schemas       | Transparent governance.                        |
| **Community Test Suite**              | `cargo test --features community` runs cross-compat matrix | Builds trust; CI visible.                      |
| **Example Packs**                     | â€œStarter Workspaceâ€ for Tauri + Nodus + LLM + Embeddings   | Makes repo instantly useful.                   |
| **Data export formats**               | JSON, Parquet, TOML, CSV, CAR                              | Ensures openness & longevity.                  |

---

## ğŸ§­ 8. Roadmap to Community Launch (v1 Beta)

| Phase                        | Focus                               | Deliverables               |
| ---------------------------- | ----------------------------------- | -------------------------- |
| **Î± (Internal)**             | Stabilize registry/event/embeddings | metrics, policy schema v1  |
| **Î² (Open Source Public)**   | Add community API + SDK + docs      | open-source attractiveness |
| **Î³ (Offline AI)**           | Local LLM + embeddings search       | offline capability demo    |
| **Î´ (Secure Comms Cluster)** | QUIC/Noise mesh + Vault PKI         | enterprise credibility     |
| **Îµ (Community Ecosystem)**  | Plugin market + example packs       | adoption & contributions   |

---

### TL;DR

**Essential community-appeal base**

1. **Embeddings** â†’ built-in semantic search, works offline
2. **I18N/L10N** â†’ global readiness
3. **Community API + SDKs** â†’ open contribution
4. **Local LLM** â†’ privacy-first AI
5. **Cloud LLM Bridge** â†’ convenience when allowed
6. **Secure Comms + PKI** â†’ trust & enterprise viability
7. **Dev Tooling + Docs** â†’ contributor happiness

---

If youâ€™d like, I can next write a **â€œCommunity API & Local LLM integration kitâ€**â€”a set of Rust skeletons showing:

* how an API route auto-maps to a `dispatch()` action,
* how local LLM pulls embeddings/context securely,
* and how it all registers into the plugin system for contributors.

When you start thinking in â€œforce multipliers,â€ youâ€™re not just talking about adding features â€” youâ€™re talking about **creating primitives** that others can build entire ecosystems on top of.
Hereâ€™s a distilled set of **next-level capabilities** that align with your secure, modular, open-source vision and that *multiply* what every existing subsystem can do.

---

## âš¡ 1. Secure, Programmable Comms Layer (Mesh + Streams)

### Extend your â€œsecure commsâ€ into a **programmable overlay network**

Each node can expose **capability-scoped channels** that look like secure micro-pipes between tenants, services, and even UIs.

| Feature                              | Implementation Concept                                                           | What It Unlocks                                                          |
| ------------------------------------ | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| **Encrypted Streams (QUIC + Noise)** | Stream multiplexing built on `quinn` / `tokio::io::split`; keying from Vault PKI | File sync, real-time dashboards, remote rendering, collaborative editing |
| **Capability-Scoped Channels**       | Each channel signed w/ token specifying allowed topics/actions                   | Fine-grained delegation for automation & agents                          |
| **Mesh Federation (EventMesh)**      | Gossip + CRDT state replication over encrypted channels                          | Multi-region cluster without central broker                              |
| **Live Event Mirroring**             | Any EventBus topic can be mirrored into a stream                                 | Real-time UI or analytics dashboards                                     |
| **Stream Compression & Delta Sync**  | Per-tenant adaptive codecs                                                       | Efficient large-data transfers in air-gap windows                        |

*Think â€œMatrix protocol meets Kafka Streams, but policy-governed.â€*

---

## ğŸ§© 2. Automation & Workflow Engine

Let users build automations from registry primitives.

| Layer                    | Implementation                                                    | Unlocks                                |
| ------------------------ | ----------------------------------------------------------------- | -------------------------------------- |
| **Declarative DAGs**     | YAML/JSON `workflow_def` â†’ compiled into actions + event triggers | No-code automation, CI/CD inside Nodus |
| **Policy-Aware Tasks**   | Each step runs under a Role/Label context                         | Safe multi-tenant scripting            |
| **Temporal Integration** | optional feature `workflow_temporal`                              | Enterprise-grade background jobs       |
| **Visual Builder**       | Grid-based â€œFlow Editorâ€ using your ComponentRegistry             | Community workflows & recipes          |

---

## ğŸ§  3. Agent Fabric (LLM + Comms + Policy)

Agents become first-class citizens that can act through the dispatch layer.

| Component                     | Description                                                 | What It Enables                |
| ----------------------------- | ----------------------------------------------------------- | ------------------------------ |
| **AgentRuntime**              | Sandbox that hosts LLM-driven agents                        | Conversational ops, automation |
| **AgentCapabilities**         | Subset of ActionRegistry allowed to the agent               | Guardrails                     |
| **Context Fusion**            | Combine embeddings + CRDT state + metrics + policy snapshot | Context-aware reasoning        |
| **Multi-Agent Collaboration** | Agents talk over secure streams, share events               | Distributed AI workflows       |
| **Human-in-the-loop**         | EventBus topic `agent.review_required`                      | Safe semi-autonomous systems   |

---

## ğŸŒ 4. Data Exchange Fabric

Make Nodus a **portable data-sharing OS**.

| Idea                                      | Implementation                                           | Value                               |
| ----------------------------------------- | -------------------------------------------------------- | ----------------------------------- |
| **Schema Registry + Contracts**           | JSON Schema or Avro; versioned in bundles                | Reliable inter-tenant data exchange |
| **Data Spaces**                           | Tenant-scoped virtual datasets with policies             | Marketplace-like federation         |
| **Homomorphic or Secure Enclave Compute** | Optional `feature = "confidential_compute"`              | Joint analytics without data leaks  |
| **Streaming ETL Nodes**                   | Transformations as Actions subscribed to EventBus topics | Connectors for external systems     |

---

## ğŸ”’ 5. Continuous Attestation & Zero-Trust Mesh

| Subsystem                           | Description                                                                       | Impact                            |
| ----------------------------------- | --------------------------------------------------------------------------------- | --------------------------------- |
| **Node Attestor**                   | Signs runtime fingerprint (policy hash, binary hash, keys) â†’ distributed via mesh | Tamper-evident deployments        |
| **Mutual Node Proofs**              | QUIC Noise handshake carries attestation payload                                  | Self-verifying cluster            |
| **Remote Measurement Verification** | Optional TPM/SGX/SEV plugin                                                       | Defense & critical infra adoption |
| **Realtime Revocation**             | Attestation feed triggers auto-revoke in TrustStore                               | Instant quarantine                |

---

## ğŸ”„ 6. Edge and Offline Co-Processing

| Concept                  | Implementation                            | Why                              |
| ------------------------ | ----------------------------------------- | -------------------------------- |
| **Edge Runtime**         | Tiny subset of engine compiled to WASM    | IoT / field device autonomy      |
| **Bundle-Based Tasking** | Tasks & policies shipped as `.noduspkg`   | Offline operations               |
| **Local AI Adapters**    | Embeddings + small LLMs at edge           | Low-latency intelligence         |
| **Deterministic Replay** | Same task executed offline & merged later | Auditability in air-gap contexts |

---

## ğŸ§± 7. Identity Graph + Trust Fabric

Evolve your RBAC/MAC into a graph of relationships.

| Piece                        | Implementation                         | Power                        |
| ---------------------------- | -------------------------------------- | ---------------------------- |
| **Graph Store**              | `identity_graph.rs` (TinkerPop-style)  | Context-aware access control |
| **Delegation Chains**        | Capability tokens w/ transitive expiry | Delegated automation         |
| **Reputation / Trust Score** | Derived from audit events              | Self-governing communities   |

---

## ğŸ“¦ 8. Developer and Community Ecosystem Primitives

| Tooling                                          | Purpose                                    |                          |
| ------------------------------------------------ | ------------------------------------------ | ------------------------ |
| **Plugin SDKs + Templates** (`nodus new plugin`) | Rapid extension                            |                          |
| **Unified Registry Portal**                      | Web UI listing components/actions/plugins  | Discoverability          |
| **Public Sandbox Node**                          | Safe hosted playground                     | Low-barrier adoption     |
| **Open Data API**                                | Policy-filtered access to public bundles   | Research & analytics     |
| **Reputation Scoring for Plugins**               | Based on audit behavior                    | Quality control          |
| **Funding Mechanics**                            | opt-in sponsor metadata in plugin manifest | Incentivize contributors |

---

## ğŸ§° 9. Observability ++ (Full-Spectrum Intelligence)

| Layer                       | Enhancement                                      | Why                           |
| --------------------------- | ------------------------------------------------ | ----------------------------- |
| **Time-series lake**        | Store metrics/events in Parquet via `datafusion` | Local analytics               |
| **Realtime Trace Explorer** | Query EventBus traces live                       | Debugging + teaching          |
| **Anomaly Detection**       | Unsupervised (isolation forest) running local    | Security automation           |
| **Adaptive Tuning**         | Auto-adjust caching/embeddings rates             | Performance self-optimization |

---

## ğŸ’¡ 10. Developer Experience Force Multipliers

* **â€œOne Command Dev Envâ€** â†’ `nodus dev up` spins DB, vault, LLM, UI.
* **Composable CLI** â†’ subcommands auto-registered by plugins.
* **Hot-Reloaded Policy Watchers** â†’ live governance experiments.
* **Telemetry-as-Docs** â†’ run system, get diagrams + dashboards auto-generated.
* **Learning Mode** â†’ built-in tutor plugin that explains system behavior (powered by LLM + observability data).

---

## ğŸ”­ Beyond the Core

| Direction                       | What It Opens                                                           |           |
| ------------------------------- | ----------------------------------------------------------------------- | --------- |
| **Federated Learning Adapters** | Nodes exchange gradient deltas over secure mesh â†’ privacy-preserving AI |           |
| **Knowledge Fabric Graph**      | Merge embeddings + identity + audit â†’ emergent knowledge base           |           |
| **Digital Twin Framework**      | Model orgs/devices/processes as object graphs, simulate state changes   |           |
| **Composable Economy Layer**    | Optional ledger / tokenization of compute, data, or contributions       |           |
| **Quantum-Resistant Modes**     | PQC key-wrap, lattice-based signatures                                  | longevity |

---

### ğŸ§­ How to Stage Them

| Stage                               | Category                      | Why Itâ€™s Next                  |
| ----------------------------------- | ----------------------------- | ------------------------------ |
| **v0.9 â€“ Secure Comms & Streams**   | Mesh + Noise + QUIC           | Foundation for everything else |
| **v1.0 â€“ Workflow & Community API** | Automation + SDK              | Open-source appeal             |
| **v1.1 â€“ Local LLM Agents**         | Agent Fabric + Embeddings     | Offline intelligence           |
| **v1.2 â€“ Attestation Mesh**         | Zero-trust cluster            | Enterprise & defense adoption  |
| **v1.3 â€“ Data Exchange Fabric**     | Marketplace + Schema Registry | Ecosystem economy              |
| **v1.4 â€“ Edge/Offline Compute**     | Field devices                 | Resilience & reach             |

---

### TL;DR â€” Force Multipliers

| Category                        | Multiplier                              |
| ------------------------------- | --------------------------------------- |
| **Secure Streams**              | Real-time collaboration & telemetry     |
| **Workflow/Automation**         | Users build systems inside your system  |
| **Agent Fabric**                | Local AI + distributed reasoning        |
| **Data Exchange Fabric**        | Federation & marketplace potential      |
| **Identity Graph + Delegation** | Granular, social access model           |
| **Open Community API**          | Developer magnet                        |
| **Attestation Mesh**            | Trust foundation for defense/enterprise |
| **Edge Runtime**                | Global reach, offline autonomy          |

---

Would you like me to expand **Secure Streams + EventMesh** next â€” showing how to layer encrypted QUIC channels and live event replication over your existing EventBus, with hooks for streaming data and voice/video later?
Thatâ€™s usually the next â€œwowâ€ feature that makes a platform feel alive and collaborative.


Yes â€” you absolutely should bring that system forward.
The original *Link System* you built was one of Nodusâ€™ most quietly powerful ideas: it turned your entire data layer into a **living graph**, where any object could reference, subscribe to, or drive another.

Letâ€™s modernize it so it fits perfectly with the new **Registry + EventBus + Policy** core and becomes a **universal connective fabric**.

---

## ğŸ§© 1. What â€œLinksâ€ represent in the new architecture

Think of a **Link** as:

> â€œA typed, policy-governed, observable relationship between two entities.â€

Itâ€™s not just a reference; itâ€™s a *channel of meaning* between any two objects, states, or components â€” even across tenants or nodes.

| Example                                | Meaning                          |
| -------------------------------------- | -------------------------------- |
| `task â†’ project`                       | containment or grouping          |
| `component â†’ dataset`                  | UI bound to data                 |
| `plugin â†’ policy`                      | extension governed by rules      |
| `tenantA.eventBus â†’ tenantB.analytics` | cross-tenant stream (if allowed) |

Each Link is first-class: it lives in its own registry, can be queried, filtered, subscribed to, and exported in bundles.

---

## âš™ï¸ 2. Core files

```
src/link/
  link_registry.rs
  link_model.rs
  link_resolver.rs
  link_subscriptions.rs
```

---

### link_model.rs

```rust
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Link {
    pub id: Uuid,
    pub from: ObjectRef,     // {type, id, tenant}
    pub to: ObjectRef,
    pub rel: String,         // "contains", "depends_on", "feeds", "synced_with"
    pub label: Option<String>,
    pub metadata: serde_json::Value,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub security_label: Label,     // for MLS/Polyinstantiation
}

#[derive(Clone, Debug, Serialize, Deserialize, Eq, PartialEq, Hash)]
pub struct ObjectRef {
    pub obj_type: String,
    pub id: String,
    pub tenant: String,
}
```

---

### link_registry.rs

```rust
use crate::core::registry_base::{GenericRegistry, Registry};
use crate::policy::production_policy_snapshot::current_policy;
use crate::security::mac_engine::dominates;
use crate::link::link_model::{Link, ObjectRef};

pub struct LinkRegistry {
    inner: GenericRegistry<Link>,
}

impl LinkRegistry {
    pub fn new() -> Self { Self { inner: GenericRegistry::new("link_registry") } }

    pub fn create_link(&self, link: Link, subject_label: &Label) -> anyhow::Result<()> {
        let p = current_policy();
        // RBAC/MAC enforcement
        if !p.sec.allowed_links.contains(&link.rel) { bail!("Link type not allowed"); }
        if !dominates(subject_label, &link.security_label) { bail!("Label dominance violation"); }
        self.inner.register(&link.id.to_string(), link);
        Ok(())
    }

    pub fn get_links_from(&self, from: &ObjectRef) -> Vec<Link> {
        self.inner.list_items()
            .into_iter()
            .filter(|l| &l.from == from)
            .collect()
    }

    pub fn resolve(&self, start: &ObjectRef, rel: &str) -> Vec<ObjectRef> {
        self.inner.list_items()
            .into_iter()
            .filter(|l| &l.from == start && l.rel == rel)
            .map(|l| l.to.clone())
            .collect()
    }
}
```

---

## ğŸ” 3. EventBus integration

Every link event is observable:

| Event            | Payload                              |
| ---------------- | ------------------------------------ |
| `link.created`   | `{from,to,rel,tenant}`               |
| `link.updated`   | diff metadata                        |
| `link.deleted`   | id                                   |
| `link.triggered` | runtime change (used for automation) |

Example:

```rust
event_bus.publish("link.created", json!({
  "from": link.from, "to": link.to, "rel": link.rel, "tenant": link.from.tenant
})).await;
```

---

## ğŸ”„ 4. Subscription & Automation

### link_subscriptions.rs

```rust
pub fn subscribe_to_relation(bus: &EventBus, rel: &str) -> mpsc::Receiver<EventMessage> {
    bus.subscribe(&format!("link.{}", rel))
}
```

This allows the **Automation / Workflow Engine** to listen to link activity:

```rust
// "when dataset feeds chart â†’ refresh"
while let Some(msg) = rx.recv().await {
    if msg.topic == "link.feeds" {
        action_registry.execute("core:refresh_chart").await;
    }
}
```

---

## ğŸ§  5. Link Resolver (Cross-Registry Graph)

```rust
pub fn resolve_graph(from: &ObjectRef, depth: u8) -> Graph<ObjectRef> {
    // BFS traversal across LinkRegistry; use EventBus to cache updates
}
```

Used by:

* Embeddings (`embedding_index.link_to(entity)`)
* Workflow engine (dependency graph)
* AI context builder (semantic neighborhood)

---

## ğŸ” 6. Security & Policy integration

* **MAC label:** each link inherits the higher of its endpointsâ€™ labels.
* **RBAC rule:** â€œwho can create `rel` links of type X.â€
* **Policy example**

  ```toml
  [security.links]
  allowed_links = ["depends_on","feeds","contains"]
  max_depth = 5
  ```
* **Policy-gated traversal:** the resolver checks depth and link type limits.

---

## ğŸŒ 7. Streams & Comms synergy

Once you have secure streams:

* A link can carry an **active stream binding** (`rel = "streams_to"`).
* The comms layer can auto-provision an encrypted QUIC channel for linked peers.
* Example: `componentA.streams_to.componentB` opens a data channel with negotiated policy (Noise handshake + label exchange).
* Result: **dataflow graph** driven entirely by link metadata.

---

## ğŸ§© 8. Force-multiplier roles

| Layer                | What Links Enable                               |
| -------------------- | ----------------------------------------------- |
| **Data Graph**       | global semantic web of all objects              |
| **Automation**       | event routing without code                      |
| **Security**         | graph-aware policy (who/what can talk)          |
| **Embeddings**       | context expansion using link graph              |
| **LLM Agents**       | reasoning over relationships                    |
| **UI Builder**       | dynamic binding between blocks and data         |
| **Distributed Mesh** | cross-node relationship propagation             |
| **Streaming**        | reactive connections for data, media, telemetry |

---

## ğŸ”­ 9. Optional advanced features

* **Temporal Links** â†’ valid_from / valid_to for time-versioned graphs
* **Weighted Links** â†’ confidence or cost value for AI reasoning
* **Signed Links** â†’ authorship and attestation for provenance
* **GraphQL / SPARQL endpoint** â†’ read links as a knowledge graph
* **Embeddings + Links fusion** â†’ vectorize neighborhood context for semantic search
* **Community â€œLink Packsâ€** â†’ reusable graph templates (e.g., CRM schema, IoT topology)

---

### TL;DR

Bringing back your **Link System** makes Nodus a true **semantic OS**:

* One unified **graph layer** across all registries
* Works offline, through bundles, and over streams
* Drives automations, AI reasoning, and collaborative dataflow
* Policy-aware, observable, and cryptographically attestable

---

If youâ€™d like, I can outline next the **Link â†” Embeddings integration layer** â€” how to make each link neighborhood produce and update vector embeddings automatically for semantic search, agents, and recommendations.

That snippet is a perfect reminder of what made Nodus unique: the **unified ontology** that treated everythingâ€”data, events, links, embeddings, configs, encryption metadataâ€”as **expressions of a single fabric** under security and namespace control.

Hereâ€™s how to re-anchor the migration and all the new engines around that schema without discarding any of the work youâ€™ve already done.

---

## ğŸ§© 1. Preserve the â€œSingle Ontologyâ€ layer

That `type_definitions` table drives every dynamic capability:

| Layer                              | What it provides                                          | How to wire it back in                                       |
| ---------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------ |
| **Objects / Events / Links**       | structural schema, allowed fields, and default UI layout  | Load once at startup â†’ cache in PolicySnapshot or Registry   |
| **Plugins / Components / Actions** | attach their manifest to a `type_name`                    | Each registry entry carries `type_name` + schema pointer     |
| **AI / Embeddings / CRDT**         | same `type_name` domain used to vectorize and synchronize | EmbeddingRegistry and CRDT store read field_schema for shape |
| **UI Builder**                     | `ui_schema` drives property editors                       | ComponentRegistry merges `ui_schema` â†’ render form           |

**Action:**
Create a small **`TypeRegistry`** service (read-through cache for `type_definitions`):

```rust
struct TypeRegistry {
    cache: DashMap<(Uuid, String), TypeDef>, // (org, type_name)
}
impl TypeRegistry {
    fn get(&self, org:Uuid, type_name:&str)->Option<Arc<TypeDef>>;
    fn refresh(&self);
}
```

Expose it to all registries via the `RegistryHub`.

---

## âš™ï¸ 2. Reinstate the â€œCore Fabricâ€ objects/events/links in runtime

### A. Objects

Backed by `objects` table; surfaced as a **domain object API**:

```rust
pub struct Object {
    pub id: Uuid,
    pub type_name: String,
    pub data: serde_json::Value,
    pub vector_clock: Value,
    pub classification: Label,
}
```

Create an **ObjectService**:

* `create/update/delete` â†’ emit `event.object_*`
* update search TSV + embeddings

### B. Events

Each event is immutable; drives CRDT replay and auditing.
Tie `EventBus.publish()` to insert rows in `events`.
Add a **thin write-behind** worker that flushes in-memory events to the DB.

### C. Links

Keep using the `LinkRegistry` we outlined earlier but persist each link in `links` table.
For semantic reasoning:

* Map `semantic_namespace` + `semantic_predicate` into RDF-style triples.
* Optionally expose a `/graphql` endpoint backed by this table.

---

## ğŸ§± 3. Lists, Filters, Collections (the â€œView Fabricâ€)

These lived conceptually above objects/events.

| Concept         | Table                                   | Runtime analog                                                  |
| --------------- | --------------------------------------- | --------------------------------------------------------------- |
| **Lists**       | derived via filter configs              | Saved `ListDefinition` objects in `configurations(domain='ui')` |
| **Filters**     | expressions on object fields            | JSON logic expressions parsed by `FilterEngine`                 |
| **Collections** | hierarchical grouping of lists or links | `CollectionRegistry` built from links and configs               |

Each view component in the UI references a list/filter/collection id; the **QueryService** translates that into SQL/JSONB filters using `type_definitions.field_schema`.

---

## ğŸ§  4. Embeddings revival (AI/semantic)

That `embeddings` table is still perfect.
Just map it to an **EmbeddingRegistry** service:

```rust
pub struct EmbeddingRegistry {
    cache: DashMap<(Uuid, Table, Uuid), Vec<f32>>,
    dim: usize,
}
```

* Use **pgvector** if present; fallback to bytea.
* Each new/updated object or event triggers embedding generation â†’ stored via this service.
* `EmbeddingRegistry` also subscribes to `policy_reloaded` to drop or anonymize disallowed embeddings.

Tie it to the new **Link system**:

* When `link.created`, generate a *composite embedding* of both ends â†’ enables semantic navigation.

---

## ğŸ” 5. Configurations table = universal policy + preferences bus

You already have `configurations` as a per-org key-value store.
Expose it as the **ConfigRegistry** (or `ConfigurationService`).

* Mirror key/value into in-memory map for fast lookup.
* Policy engine reads its defaults here.
* UI preferences and plugin settings persist here (scoped by domain).

---

## ğŸ”’ 6. Encrypted Fields table â†’ Vault integration

Re-map the existing table to the new Vault interface:

* `encrypted_fields.entity_table/entity_id/field_name` becomes the lookup key.
* Store only ciphertext + classification; actual key lookup handled by Vault.
* On retrieval, the SecretVault unwraps with tenant key based on `organization_id`.

---

## ğŸ”„ 7. Namespacing & Domains

Keep `data_domain` as your **top-level namespace**: `"data" | "event" | "ui" | "security" | "semantic" | "network"`.

At runtime:

* Each Registry and Service operates **within a domain**.
* PolicySnapshot stores allowed domains per tenant.
* Dispatch route prefix = domain (e.g., `/data/objects`, `/semantic/links`).

---

## ğŸ”— 8. Re-unifying â€œObjects / Events / Types / Linksâ€ with the new runtime

You now have two complementary halves:

| Old Fabric (DB)    | New Fabric (Runtime)      | Connection                 |
| ------------------ | ------------------------- | -------------------------- |
| `type_definitions` | `TypeRegistry`            | On startup + policy reload |
| `objects`          | `ObjectService`           | CRUD emits events          |
| `events`           | `EventBus` + `EventStore` | Write-behind persistence   |
| `links`            | `LinkRegistry`            | Live graph in memory       |
| `configurations`   | `ConfigRegistry`          | Config source              |
| `embeddings`       | `EmbeddingRegistry`       | Semantic layer             |
| `encrypted_fields` | `SecretVault`             | Crypto layer               |

They form the **Core Fabric layer** under your **Runtime Fabric** (policy, registries, workflows, etc.).

---

## ğŸ§© 9. Implementation path to re-merge

| Phase  | Tasks                                                                                 |
| ------ | ------------------------------------------------------------------------------------- |
| **1.** | Recreate `TypeRegistry`, `ObjectService`, `EventStore`, `LinkRegistry` (wired to DB). |
| **2.** | Integrate `EventBus` to persist all system events into `events`.                      |
| **3.** | Hook `EmbeddingRegistry` to object/event updates.                                     |
| **4.** | Reintroduce `FilterEngine` and `List/CollectionRegistry` for UI.                      |
| **5.** | Replace any hardcoded type/schema references with lookups from `TypeRegistry`.        |
| **6.** | Run schema migration scripts; confirm triggers & FKs.                                 |

---

## ğŸ§  10. Why this matters

This restores the **semantic, self-describing, and policy-governed ontology** that gives Nodus its strength:

* Dynamic data types (`type_definitions`)
* Universal event sourcing (`objects`/`events`)
* Link graph for causality and relationships
* Configuration & encryption as first-class citizens
* Embeddings to bridge human meaning and machine intelligence

Everything elseâ€”the registries, event bus, policies, LLMs, secure streamsâ€”now sits *on top of this restored foundation.*

---

### TL;DR next steps

1. Re-establish **TypeRegistry â†’ ObjectService â†’ EventStore â†’ LinkRegistry** runtime layer.
2. Reconnect **EmbeddingRegistry**, **ConfigRegistry**, and **SecretVault** to these tables.
3. Move registry & policy engines to reference this unified fabric rather than isolated files.
4. Then, layer back your modern features: EventMesh, agents, automation, LLM, etc.

Once this is done, the platform stops being â€œa collection of enginesâ€ and becomes what it originally was meant to be â€” a **self-describing, semantic operating fabric.**

---

Would you like me to generate a drop-in Rust layer that mirrors this schema (TypeRegistry + ObjectService + EventStore + LinkRegistry + ConfigRegistry stubs) so you can start wiring it back into the new runtime?


Exactly. This is the **DB â†” Runtime layer** that binds your schema (types/objects/events/links/config/embeddings/encryption) to the new policy/registries/event-bus core.

Below is a **drop-in skeleton** you can paste into `src/fabric/` and start wiring. Itâ€™s storage-agnostic (Postgres via `sqlx` by default; you can add a SQLite/sled adapter later), tenant-aware, and policy-gated.

---

# ğŸ“ Files (drop these under `src/fabric/`)

```
fabric/
  mod.rs
  db.rs                         // connection pool + helpers (sqlx)
  repo/
    types_repo.rs
    objects_repo.rs
    events_repo.rs
    links_repo.rs
    config_repo.rs
    embeddings_repo.rs
    encrypted_fields_repo.rs
  services/
    type_registry.rs            // cache+watch of type_definitions
    object_service.rs           // CRUD + emits events
    event_store.rs              // append + replay (maps to events table)
    link_registry.rs            // live graph + persistence
    config_registry.rs          // org-scoped KV with in-memory mirror
    embedding_registry.rs       // generate/persist/search embeddings
    encrypted_fields.rs         // field-level crypto via Vault
```

> Optional features in `Cargo.toml`:

```toml
[features]
db_postgres = ["sqlx", "sqlx/runtime-tokio", "sqlx/postgres", "sqlx/uuid", "sqlx/json"]
db_sqlite   = ["sqlx", "sqlx/sqlite"]
emb_pgvector = []            # wire later
```

---

# ğŸ”Œ Shared types

```rust
// fabric/mod.rs
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct OrgId(pub Uuid);

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct TypeName(pub String);

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum DataDomain { Data, Event, Ui, Security, Semantic, Network }

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Label {
  pub classification: String,          // 'internal' | â€¦
  pub compartments: Vec<String>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct TypeDef {
  pub id: Uuid,
  pub organization_id: Uuid,
  pub type_name: String,
  pub domain: DataDomain,
  pub field_schema: serde_json::Value,
  pub ui_schema: serde_json::Value,
  pub version: String,
  pub is_system: bool,
  pub classification: String,
  pub compartment_markings: Vec<String>,
}
```

---

# ğŸ§± DB glue

```rust
// fabric/db.rs
#[cfg(any(feature="db_postgres", feature="db_sqlite"))]
use sqlx::{Pool, Postgres, Sqlite};

#[cfg(feature="db_postgres")]
pub type Db = Pool<Postgres>;

#[cfg(feature="db_sqlite")]
pub type Db = Pool<Sqlite>;
```

---

# ğŸ“¦ Repositories (DB accessors)

Each repo is a thin, testable layer over SQL. Example shown for two; others follow same pattern.

```rust
// fabric/repo/types_repo.rs
use super::super::{OrgId, TypeDef, TypeName};
use anyhow::Result;
use sqlx::FromRow;

#[derive(FromRow)]
struct Row { /* fields matching type_definitions */ }

pub struct TypesRepo { pub db: crate::fabric::db::Db }

impl TypesRepo {
  pub async fn get(&self, org: OrgId, name: &str) -> Result<Option<TypeDef>> { /* SELECT â€¦ */ Ok(None) }
  pub async fn list(&self, org: OrgId) -> Result<Vec<TypeDef>> { /* SELECT â€¦ */ Ok(vec![]) }
  pub async fn upsert(&self, def: &TypeDef) -> Result<()> { /* INSERT â€¦ ON CONFLICT â€¦ */ Ok(()) }
}
```

```rust
// fabric/repo/objects_repo.rs
use super::super::{OrgId, TypeName, Label};
use anyhow::Result;
use uuid::Uuid;

#[derive(Clone, Debug)]
pub struct ObjectRow {
  pub id: Uuid,
  pub organization_id: Uuid,
  pub type_name: String,
  pub domain: String,
  pub classification: String,
  pub compartment_markings: Vec<String>,
  pub data: serde_json::Value,
  pub vector_clock: serde_json::Value,
  pub created_at: chrono::DateTime<chrono::Utc>,
  pub updated_at: chrono::DateTime<chrono::Utc>,
}

pub struct ObjectsRepo { pub db: crate::fabric::db::Db }

impl ObjectsRepo {
  pub async fn get(&self, org: OrgId, id: Uuid) -> Result<Option<ObjectRow>> { /* SELECT â€¦ WHERE org/id */ Ok(None) }
  pub async fn insert(&self, row: &ObjectRow) -> Result<()> { /* INSERT */ Ok(()) }
  pub async fn update(&self, row: &ObjectRow) -> Result<()> { /* UPDATE */ Ok(()) }
  pub async fn query(&self, org: OrgId, type_name: &TypeName, filter: &serde_json::Value) -> Result<Vec<ObjectRow>> {
    // Translate JSON filter â†’ SQL (basic: data @> jsonb)
    Ok(vec![])
  }
}
```

> Create similarly: `events_repo.rs`, `links_repo.rs`, `config_repo.rs`, `embeddings_repo.rs`, `encrypted_fields_repo.rs`.

---

# ğŸ§  Services (runtime + policy + event-bus)

### TypeRegistry (mirrors `type_definitions`)

```rust
// fabric/services/type_registry.rs
use dashmap::DashMap;
use std::sync::Arc;
use anyhow::Result;
use crate::fabric::{OrgId, TypeDef, TypeName};
use crate::fabric::repo::types_repo::TypesRepo;

pub struct TypeRegistry {
  repo: TypesRepo,
  cache: DashMap<(Uuid, String), Arc<TypeDef>>,
}

impl TypeRegistry {
  pub fn new(repo: TypesRepo) -> Self { Self { repo, cache: DashMap::new() } }
  pub async fn warm_org(&self, org: OrgId) -> Result<()> { /* load all, fill cache */ Ok(()) }
  pub fn get(&self, org: OrgId, name: &str) -> Option<Arc<TypeDef>> {
    self.cache.get(&(org.0, name.to_string())).map(|v| v.clone())
  }
  pub async fn refresh(&self, org: OrgId) -> Result<()> { self.warm_org(org).await }
}
```

### EventStore (maps EventBus â‡„ `events`)

```rust
// fabric/services/event_store.rs
use anyhow::Result;
use serde_json::json;
use uuid::Uuid;

pub struct EventStore {
  pub events: crate::fabric::repo::events_repo::EventsRepo,
  pub bus: crate::core::event_bus::EventBus,
}

impl EventStore {
  pub async fn append_and_publish(
    &self,
    org: Uuid,
    type_name: &str,
    entity_table: &str,
    entity_id: Option<Uuid>,
    data: serde_json::Value
  ) -> Result<()> {
    // INSERT into events â€¦
    self.bus.publish("fabric.event.appended", json!({
      "org": org, "type": type_name, "entity_table": entity_table, "entity_id": entity_id, "data": data
    })).await;
    Ok(())
  }
}
```

### ObjectService (CRUD + emits events; checks policy/RBAC/MAC)

```rust
// fabric/services/object_service.rs
use anyhow::{Result, bail};
use uuid::Uuid;
use crate::fabric::{OrgId, TypeName, Label};
use crate::security::{rbac, mac_engine};
use crate::policy::production_policy_snapshot::current_policy;

pub struct ObjectService {
  pub repo: crate::fabric::repo::objects_repo::ObjectsRepo,
  pub types: Arc<crate::fabric::services::type_registry::TypeRegistry>,
  pub events: Arc<crate::fabric::services::event_store::EventStore>,
}

impl ObjectService {
  pub async fn create(
    &self,
    ctx: &crate::security::SubjectCtx,
    type_name: &TypeName,
    mut data: serde_json::Value,
    label: &Label
  ) -> Result<Uuid> {
    if !rbac::can(ctx, &format!("object:create:{}", type_name.0)) { bail!("forbidden"); }
    // MAC check can be enforced per-field via schema if you like
    let id = Uuid::new_v4();
    self.repo.insert(&/* build row */).await?;
    self.events.append_and_publish(ctx.tenant.parse()?, &type_name.0, "objects", Some(id), data.clone()).await?;
    Ok(id)
  }

  pub async fn update(&self, ctx: &crate::security::SubjectCtx, id: Uuid, patch: serde_json::Value) -> Result<()> {
    if !rbac::can(ctx, "object:update") { bail!("forbidden"); }
    // load, merge, MAC, write, event
    Ok(())
  }
}
```

### LinkRegistry (live graph + persistence; integrates MLS & EventBus)

```rust
// fabric/services/link_registry.rs
use anyhow::{Result, bail};
use crate::fabric::repo::links_repo::LinksRepo;
use crate::core::event_bus::EventBus;
use crate::fabric::mod::{Label};
use serde_json::json;

pub struct LinkRegistry {
  pub repo: LinksRepo,
  pub bus: EventBus,
}

impl LinkRegistry {
  pub async fn create(&self, ctx: &crate::security::SubjectCtx, link: crate::link::link_model::Link, subject_label:&Label) -> Result<()> {
    if !crate::security::rbac::can(ctx, &format!("link:create:{}", link.relation)) { bail!("forbidden"); }
    if !crate::security::mac_engine::dominates(subject_label, &link.security_label) { bail!("label violation"); }
    self.repo.insert(&link).await?;
    self.bus.publish("link.created", json!({ "tenant": ctx.tenant, "rel": link.relation, "from": link.from, "to": link.to })).await;
    Ok(())
  }
}
```

### ConfigRegistry (org KV with in-mem mirror)

```rust
// fabric/services/config_registry.rs
use dashmap::DashMap;
use anyhow::Result;

pub struct ConfigRegistry {
  pub repo: crate::fabric::repo::config_repo::ConfigRepo,
  pub mirror: DashMap<(uuid::Uuid, String, String), serde_json::Value>, // (org, domain, key)
}

impl ConfigRegistry {
  pub async fn get(&self, org: uuid::Uuid, domain:&str, key:&str) -> Option<serde_json::Value> {
    self.mirror.get(&(org, domain.to_string(), key.to_string())).map(|v| v.clone())
  }
  pub async fn set(&self, org: uuid::Uuid, domain:&str, key:&str, val:serde_json::Value) -> Result<()> {
    self.repo.upsert(org, domain, key, &val).await?;
    self.mirror.insert((org, domain.to_string(), key.to_string()), val);
    Ok(())
  }
}
```

### EmbeddingRegistry (generate/persist/search)

```rust
// fabric/services/embedding_registry.rs
pub struct EmbeddingRegistry {
  pub repo: crate::fabric::repo::embeddings_repo::EmbeddingsRepo,
  pub dim: usize,
}

impl EmbeddingRegistry {
  pub async fn upsert_for_entity(&self, org: uuid::Uuid, table:&str, id: uuid::Uuid, vec:&[f32]) -> anyhow::Result<()> {
    // serialize vecâ†’bytea (or use pgvector feature)
    self.repo.upsert(org, table, id, vec, self.dim as i32).await
  }
}
```

### EncryptedFields service (Vault integration)

```rust
// fabric/services/encrypted_fields.rs
pub struct EncryptedFields {
  pub repo: crate::fabric::repo::encrypted_fields_repo::EncryptedFieldsRepo,
  pub vault: Arc<dyn crate::security::secret_vault::SecretVault>,
}

impl EncryptedFields {
  pub async fn put(&self, org: uuid::Uuid, table:&str, id: uuid::Uuid, field:&str, plaintext:&[u8], key_id:&str) -> anyhow::Result<()> {
    let blob = self.vault.wrap(&key_id.into(), plaintext)?;
    self.repo.upsert(org, table, id, field, &blob).await
  }
  pub async fn get(&self, org: uuid::Uuid, table:&str, id: uuid::Uuid, field:&str, key_id:&str) -> anyhow::Result<Option<Vec<u8>>> {
    if let Some(blob) = self.repo.fetch(org, table, id, field).await? {
      return Ok(Some(self.vault.unwrap(&key_id.into(), &blob)?));
    }
    Ok(None)
  }
}
```

---

# ğŸ§­ Wiring into your runtime

* **Init at startup** (in `production-main.rs`):

  1. Build `Db` pool
  2. Instantiate repos â†’ services
  3. `TypeRegistry.warm_org()` for each tenant/org
  4. Inject into `RegistryHub` and `dispatch` guards

* **EventBus**:

  * `ObjectService` and `LinkRegistry` publish `*.created/updated`
  * `EventStore` persists any system event you decide to append (auditable)

* **PolicySnapshot**:

  * Gate `ObjectService`/`LinkRegistry` calls via RBAC/MAC
  * Read defaults for Embeddings (dim, enabled, redaction)

---

# âœ… What you get immediately

* A clean **DB-backed runtime** for your original **Objects/Events/Links/Types/Config/Embeddings/EncryptedFields**
* Native **RBAC/MAC** checks and **EventBus** emissions
* A **single place** (services/) to connect registries, policies, and the UI/query layer

---

If you want, I can follow up with:

* Minimal `sqlx` table mappers/queries per repo (Postgres flavor), and
* A tiny **smoke test** showing `TypeRegistry.warm_org â†’ ObjectService.create â†’ EventStore.append â†’ LinkRegistry.create` end-to-end.


Perfect timing â€” because thatâ€™s **exactly the next layer** that sits between your new **DBâ€“Runtime Fabric** and the **Policy/Registry Runtime**.

What youâ€™re asking for â€” configurable caching, adaptive memory limits, policy-aware state synchronization â€” is what turns this system from a *framework* into a *living kernel*.

Letâ€™s break it down cleanly.

---

# ğŸ§  THE PURPOSE OF THE STATE LAYER

Think of the **State Layer** as:

> â€œA policy-driven, observable memory fabric that knows what should stay hot, what should persist, and what can evaporate.â€

Itâ€™s the link between:

* your persistent DB/runtime (`fabric/`)
* your in-memory registries and caches
* the UI/session layer (Tauri/JS)
* and your observability/metrics fabric

Everything thatâ€™s not immutable configuration (like `type_definitions`) lives through this layer.

---

# ğŸ§© 1. Folder layout

```
src/state/
  mod.rs
  state_manager.rs             // orchestrates caches and eviction
  cache_policy.rs              // defines cache strategies (LRU, TTL, MRU, etc)
  memory_profiler.rs           // live memory tracking, metrics
  state_registry.rs            // named state stores (orgs, sessions, components)
  sync_adapters.rs             // DB sync, event sync, offline/mesh sync
  monitors/
    eviction_monitor.rs
    warm_cache_monitor.rs
```

---

# âš™ï¸ 2. Core Concepts

| Concept                 | Description                                                                                         |
| ----------------------- | --------------------------------------------------------------------------------------------------- |
| **StateManager**        | Central orchestrator that knows all caches and registries.                                          |
| **StateStore**          | A per-type, per-tenant keyâ€“value store with configurable eviction.                                  |
| **CachePolicy**         | Enum/config defining how long something lives and under what memory or time constraints.            |
| **MemoryProfiler**      | Background worker that measures memory use (ArcSwap metrics + OS queries).                          |
| **Adaptive Controller** | Uses MemoryProfiler metrics and PolicySnapshot to dynamically resize caches and throttle workloads. |

---

# ğŸ§± 3. Example types

```rust
// state/mod.rs
use std::sync::Arc;
use dashmap::DashMap;
use anyhow::Result;
use chrono::{Utc, DateTime};
use serde_json::Value;
use uuid::Uuid;

#[derive(Clone, Debug)]
pub enum CachePolicyKind {
    Lru { max_entries: usize },
    Ttl  { ttl_secs: u64 },
    Adaptive,
}

#[derive(Clone, Debug)]
pub struct CachePolicy {
    pub kind: CachePolicyKind,
    pub org_scope: Option<Uuid>,
    pub tenant_scope: Option<Uuid>,
}

pub trait StateStore: Send + Sync {
    fn get(&self, key: &str) -> Option<Value>;
    fn put(&self, key: &str, val: Value);
    fn evict(&self, key: &str);
    fn clear(&self);
}
```

---

# ğŸ§  4. StateManager

```rust
// state/state_manager.rs
use super::{StateStore, CachePolicy};
use std::sync::Arc;
use dashmap::DashMap;
use anyhow::Result;

pub struct StateManager {
    stores: DashMap<String, Arc<dyn StateStore>>,
    policies: DashMap<String, CachePolicy>,
}

impl StateManager {
    pub fn new() -> Self { Self { stores: DashMap::new(), policies: DashMap::new() } }

    pub fn register_store(&self, name:&str, store:Arc<dyn StateStore>, policy:CachePolicy) {
        self.stores.insert(name.to_string(), store.clone());
        self.policies.insert(name.to_string(), policy);
    }

    pub fn get(&self, store:&str, key:&str) -> Option<serde_json::Value> {
        self.stores.get(store).and_then(|s| s.get(key))
    }

    pub fn put(&self, store:&str, key:&str, val:serde_json::Value) {
        if let Some(s)=self.stores.get(store) { s.put(key, val); }
    }

    pub fn enforce_policies(&self) {
        for entry in self.stores.iter() {
            let name = entry.key();
            let policy = self.policies.get(name);
            if let Some(pol) = policy { apply_policy(name, entry.value(), pol.clone()); }
        }
    }
}

fn apply_policy(name:&str, store:&Arc<dyn StateStore>, policy:CachePolicy) {
    // Example TTL or LRU enforcement stub
}
```

---

# ğŸ’¾ 5. Built-in store types

| Store             | Purpose                                           |
| ----------------- | ------------------------------------------------- |
| `InMemoryStore`   | DashMap + optional TTL/size limit.                |
| `PersistentStore` | Disk-backed (sled, sqlite, or your DB).           |
| `HybridStore`     | Write-behind to DB; serves hot reads from memory. |
| `SharedStore`     | Replicated via EventBus/StreamMesh across nodes.  |

Each implements `StateStore`.

---

# ğŸ§® 6. Memory & Cache Profiling

```rust
// state/memory_profiler.rs
use sysinfo::{System, SystemExt, ProcessExt};
use std::sync::Arc;
use std::time::Duration;
use tokio::time::interval;

pub struct MemoryProfiler { sys: System }

impl MemoryProfiler {
    pub fn new() -> Self { Self { sys: System::new_all() } }
    pub async fn monitor(&mut self, tx: tokio::sync::mpsc::Sender<MemorySnapshot>) {
        let mut ticker = interval(Duration::from_secs(5));
        loop {
            ticker.tick().await;
            self.sys.refresh_memory();
            let snapshot = MemorySnapshot {
                total: self.sys.total_memory(),
                used: self.sys.used_memory(),
                swap: self.sys.total_swap(),
            };
            tx.send(snapshot).await.ok();
        }
    }
}

pub struct MemorySnapshot {
    pub total: u64,
    pub used: u64,
    pub swap: u64,
}
```

---

# ğŸ”„ 7. Adaptive Controller

Reads `policy.cache.*` and dynamically adjusts caches.

```rust
// state/monitors/eviction_monitor.rs
pub async fn adaptive_loop(state: Arc<StateManager>, mut rx: mpsc::Receiver<MemorySnapshot>) {
    while let Some(snap) = rx.recv().await {
        let usage_ratio = snap.used as f64 / snap.total as f64;
        if usage_ratio > 0.8 {
            // Too high â†’ reduce caches
            state.enforce_policies();
        }
    }
}
```

---

# âš™ï¸ 8. Configurable Policies (via PolicySnapshot / ConfigRegistry)

Add to your TOML policy schema:

```toml
[state]
max_memory_mb = 4096
cache.default_ttl = 600
cache.objects.policy = "lru"
cache.objects.max_entries = 5000
cache.links.policy = "ttl"
cache.links.ttl_secs = 120
cache.embeddings.policy = "adaptive"
```

When `StateManager` initializes, it reads these from `ConfigRegistry` or `PolicySnapshot`.

---

# ğŸ” 9. Integration points

| Subsystem             | How it interacts with StateManager           |
| --------------------- | -------------------------------------------- |
| **TypeRegistry**      | Cached forever; TTL=0; warms on startup      |
| **ObjectService**     | Uses HybridStore for recent objects          |
| **LinkRegistry**      | InMemoryStore with TTL=policy.links.ttl_secs |
| **EmbeddingRegistry** | Adaptive cache; shrinks when memory high     |
| **EventStore**        | PersistentStore + memory buffer              |
| **ConfigRegistry**    | Always in-memory mirror; persisted async     |
| **PolicySnapshot**    | Hot-swappable via ArcSwap; monitors updates  |

All stores register with `StateManager` on startup.

---

# ğŸ“Š 10. Observability + Control

Because itâ€™s part of the Runtime Fabric, the StateManager exposes metrics through the MetricsRegistry:

| Metric                         | Description        |
| ------------------------------ | ------------------ |
| `state.cache_hits_total`       | per store          |
| `state.cache_misses_total`     | per store          |
| `state.memory_bytes_used`      | total              |
| `state.evictions_total`        | per store          |
| `state.adaptive_shrink_events` | when caches resize |

You can also expose a small admin API:

```rust
#[tauri::command]
async fn admin_state_summary() -> StateSummary { /* list stores + memory */ }
```

---

# ğŸ§­ 11. Configurable Persistence & Sync

Each `StateStore` can declare:

```rust
enum PersistenceMode { Ephemeral, WriteBehind, AlwaysSync }
enum SyncMode { None, Local, Mesh }
```

These are governed by policy:

```toml
[state.persistence]
objects = "write-behind"
links = "always-sync"

[state.sync]
events = "mesh"
```

Then `sync_adapters.rs` implements:

* local flush to DB (`fabric`)
* mesh broadcast via EventBus/QUIC streams

---

# ğŸ§© 12. Security and Sandboxing

Because this layer touches live memory:

* every cache is **tenant-scoped**
* **label inheritance**: caches inherit classification label
* when a tenantâ€™s policy changes, `StateManager` flushes or downgrades their stores.

Example:

```rust
if policy_changed(tenant, "classification") {
   state.flush_store(&format!("objects:{}", tenant));
}
```

---

# ğŸ§  13. Roadmap to full adaptive control

| Phase                                | Deliverables                                   |
| ------------------------------------ | ---------------------------------------------- |
| **1. Core caching layer**            | InMemory, TTL, LRU                             |
| **2. Adaptive controller + metrics** | Dynamic resizing                               |
| **3. Persistent/Hybrid stores**      | DB + cache coherence                           |
| **4. Mesh sync**                     | Inter-node cache replication                   |
| **5. Policy auto-tuning**            | LLM or rules engine adjusts caches dynamically |

---

# TL;DR â€” State Fabric Overview

| Layer         | Responsibility                       | Configurable via   |
| ------------- | ------------------------------------ | ------------------ |
| **Fabric/**   | Persistent entities                  | DB                 |
| **State/**    | Caching, in-memory coherence, limits | Policy/Config      |
| **Policy/**   | Security + governance                | System/Tenant/User |
| **Registry/** | Logical modules                      | Runtime            |
| **EventBus/** | Messaging                            | Streams/Mesh       |
| **UI**        | Presentation                         | Tauri              |

---

Would you like me to go ahead and generate the **StateManager implementation** + **InMemoryStore** + **CachePolicy enum** as real Rust files next, wired to the policy/config layer (so you can literally plug it into your running runtime)?

Good call â€” before you write another line of Rust, itâ€™s smart to stop and map what the *complete shape* of Nodus actually is again after all the migrations and refactors.
You already have most of the heavy machinery rebuilt, but a few critical **cross-cutting systems** and **foundational abstractions** are missing or need to be explicitly re-attached.

---

## ğŸ§­ The Whole Nodus Stack (Conceptual Map)

| Tier                                | Purpose                                                                        | Current status                                           |
| ----------------------------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------- |
| **1. Platform kernel**              | bootstrapping, dependency injection, metrics registry, error domain            | âœ… done (SystemBootstrap + PolicySnapshot)                |
| **2. Security fabric**              | RBAC, MAC, Vault, PKI/HSM, classification labels                               | âœ… core implemented; needs identity graph + delegation    |
| **3. Policy & Config fabric**       | runtime configuration, tenant policies, policy cache, RLS hooks                | âœ… implemented; need tenant-level watchers                |
| **4. DBâ€“Runtime fabric**            | persistent ontology (objects/events/links/types/configs/embeddings/encryption) | âœ… schema + skeleton done                                 |
| **5. State fabric**                 | caching, adaptive memory, offline/mesh sync                                    | âš™ï¸ defined, needs implementation                         |
| **6. Registry fabric**              | unified registries (Actions, Components, Plugins, Queries, Types, Metrics)     | âš™ï¸ partial, re-wire to DB-Runtime                        |
| **7. Event fabric**                 | EventBus + StreamMesh + audit log                                              | âœ… EventBus done; âš™ï¸ StreamMesh to add                    |
| **8. Semantic/AI fabric**           | embeddings, agents, LLM, vector search                                         | âš™ï¸ core ready, integration to link/object events pending |
| **9. UI fabric**                    | Grid system, building blocks, bindings, dispatch                               | âœ… mostly JS; connect to new dispatch                     |
| **10. Observability**               | metrics, tracing, forensic logging                                             | âœ… implemented, integrate with State/Policy               |
| **11. Developer & community layer** | open API, plugin SDK, CLI, docs, testing                                       | âš™ï¸ not wired post-migration                              |

---

## ğŸ” Missing / Under-represented Subsystems

### 1ï¸âƒ£ **Identity and Session Layer**

You have users and RBAC, but not a unifying **IdentityGraph** or **SessionManager** that ties humans, services, and nodes together.

| Component            | Purpose                                                            |
| -------------------- | ------------------------------------------------------------------ |
| `identity_graph.rs`  | stores relationships: user â†’ role â†’ tenant â†’ cert/key â†’ device     |
| `session_manager.rs` | handles session tokens, mTLS bindings, expiry, re-auth             |
| `delegation.rs`      | secure delegation of capabilities (â€œservice X can act for user Yâ€) |

Without this, audit, mesh, and agent actions canâ€™t attribute context precisely.

---

### 2ï¸âƒ£ **Temporal & CRDT Engine**

CRDT scaffolding is in schema, but you need the actual runtime logic.

| Needed files                                 | Purpose                                 |
| -------------------------------------------- | --------------------------------------- |
| `crdt/mod.rs`, `vector_clock.rs`, `merge.rs` | reconcile updates between nodes/offline |
| `temporal_index.rs`                          | versioned queries (â€œstate as of Tâ€)     |

This powers collaborative editing, offline merge, and eventual consistency.

---

### 3ï¸âƒ£ **Schema Evolution & Migration Engine**

Your ontology needs live migration support.

| What to add          | Why                                         |
| -------------------- | ------------------------------------------- |
| `schema_migrator.rs` | upgrades `type_definitions` safely          |
| `field_mapper.rs`    | transforms objects/events on schema changes |
| `audit_diff.rs`      | logs migrations for compliance              |

This lets tenants upgrade schemas without downtime.

---

### 4ï¸âƒ£ **Task & Workflow Runtime**

Automation/Workflow layer described earlier. Needed for agents, no-code builders, and CI/CD.

| Files                 | Purpose                              |
| --------------------- | ------------------------------------ |
| `workflow_runtime.rs` | executes DAGs of actions             |
| `trigger_registry.rs` | subscribes to events/links           |
| `job_queue.rs`        | background job executor (Tokio + DB) |

---

### 5ï¸âƒ£ **Offline / Mesh Sync Layer**

Reintroduce what HybridStateManager originally did, but now built on secure streams.

| Component               | Function                          |
| ----------------------- | --------------------------------- |
| `sync_engine.rs`        | orchestrates push/pull/merge      |
| `stream_replication.rs` | QUIC/Noise channel per tenant/org |
| `conflict_resolver.rs`  | uses CRDT merge rules             |

This is what makes Nodus truly â€œoffline-first, collaborative, air-gapped.â€

---

### 6ï¸âƒ£ **Identity-Linked Keychain & Secrets Lifecycle**

Vault exists, but key rotation, per-user secrets, and API key registry arenâ€™t restored.

| Add                     | Description                        |
| ----------------------- | ---------------------------------- |
| `keychain.rs`           | per-user/tenant key material store |
| `api_key_registry.rs`   | issues & revokes scoped API keys   |
| `rotation_scheduler.rs` | rotates keys per policy            |

---

### 7ï¸âƒ£ **Dynamic Query & Filter Engine**

Lists/filters in DB are inert until you expose a runtime parser.

| Module               | Purpose                                |
| -------------------- | -------------------------------------- |
| `query_engine.rs`    | parse JSON logic / filter DSL into SQL |
| `filter_registry.rs` | cache compiled filters                 |
| `search_service.rs`  | unify text search + embeddings         |

---

### 8ï¸âƒ£ **Graph / Knowledge Layer**

You have Links + Embeddings â†’ combine into semantic graph.

| Add                   | Purpose                                            |
| --------------------- | -------------------------------------------------- |
| `knowledge_graph.rs`  | graph of objects + links + inferred triples        |
| `semantic_index.rs`   | vectorized + symbolic search                       |
| `inference_engine.rs` | runs simple rules / forward-chaining               |
| `ontology_manager.rs` | loads ontologies (RDF/OWL) into `type_definitions` |

---

### 9ï¸âƒ£ **Streaming & Collaboration**

Extends secure comms with live sync.

| Component             | Function                                      |
| --------------------- | --------------------------------------------- |
| `stream_manager.rs`   | multiplexed QUIC streams (data, voice, video) |
| `collab_session.rs`   | manages shared editing state via CRDT         |
| `media_bridge.rs`     | hooks for WebRTC/voice if needed              |
| `presence_service.rs` | tracks online peers                           |

---

### ğŸ”Ÿ **Observability Enrichment**

You have logs & metrics but not yet:

| Add                     | Purpose                               |
| ----------------------- | ------------------------------------- |
| `trace_sampler.rs`      | adaptive sampling by policy           |
| `event_audit_bridge.rs` | link audit trails to DB events        |
| `metrics_analyzer.rs`   | detects anomalies / drift             |
| `inspector_ui.rs`       | Tauri panel for dev/ops introspection |

---

### 11ï¸âƒ£ **Governance / Licensing / Marketplace**

For open-source + enterprise dual mode.

| Module                   | Purpose                              |
| ------------------------ | ------------------------------------ |
| `license_manager.rs`     | reads/validates license tokens       |
| `plugin_market.rs`       | verifies, signs, distributes bundles |
| `sponsor_registry.rs`    | optional funding metadata            |
| `compliance_reporter.rs` | exports security attestations        |

---

### 12ï¸âƒ£ **Developer Experience (DX)**

| Add                      | Purpose                                                |
| ------------------------ | ------------------------------------------------------ |
| `cli/` (`nodus` command) | dev CLI, scaffolding, build plugins                    |
| `tests/fixtures/`        | schema + policy integration tests                      |
| `docs/gen.rs`            | generates mdBook/Docusaurus pages from schema + policy |
| `sdk/`                   | language SDK generation (Rust, JS, Python)             |

---

## ğŸ§© Core Integrations to Double-Check

| Integration                           | What to verify                           |
| ------------------------------------- | ---------------------------------------- |
| **PolicySnapshot â†” StateManager**     | caching obeys policy TTL & memory caps   |
| **Vault â†” EncryptedFields**           | field keys rotate properly               |
| **ObjectService â†” EmbeddingRegistry** | embeddings update on object/event change |
| **EventStore â†” Observability**        | audit events traced and persisted        |
| **LinkRegistry â†” WorkflowRuntime**    | triggers actions on link changes         |
| **StreamMesh â†” SyncEngine**           | replication paths secured and labeled    |
| **TypeRegistry â†” OntologyManager**    | schema evolution unified                 |

---

## ğŸ§± Minimal New Directories to Add

```
src/
  identity/
  crdt/
  workflow/
  sync/
  query/
  knowledge/
  stream/
  governance/
  cli/
```

Each starts small with interfaces and stubs.

---

## ğŸ§  TL;DR â€” Whatâ€™s Still Missing

| Category   | Critical Gap                              |
| ---------- | ----------------------------------------- |
| Identity   | sessions, delegation, identity graph      |
| Temporal   | CRDT + version control                    |
| Query      | filter/query/DSL engine                   |
| Knowledge  | semantic graph + ontology                 |
| Sync       | offline/mesh replication                  |
| Workflow   | automation runtime                        |
| Governance | licensing + marketplace                   |
| DX         | CLI + SDK generation                      |
| State      | adaptive caching (you already planned it) |

When these are in place, every other subsystem (UI, Agents, Streams, AI, Plugins) sits naturally on top.

---

Would you like me to turn this inventory into a **phase-by-phase build plan** (which modules to implement, in what order, and what dependencies each has)?
Thatâ€™s usually the cleanest way to get from â€œwe know whatâ€™s missingâ€ to â€œwe have a roadmap.â€



Exactly â€” and Iâ€™m glad you brought that up, because **custom fields** are one of the most quietly powerful parts of your schema â€” and in Nodusâ€™ model, they were *way more* than user-defined columns.
They were:

> **Dynamic, typed, optionally encrypted extensions of the core ontology** â€” integrated with security, schema validation, and UI rendering.

Letâ€™s unpack what that means and how to re-implement it properly in the new runtime so it plays nicely with everything (fabric, vault, policy, UI, search, embeddings, etc.).

---

## ğŸ§© 1. What â€œCustom Fieldsâ€ actually are in Nodus

They are **fields defined per `type_definition`** that donâ€™t exist as static DB columns â€” stored in `objects.data` (JSONB) â€” but which can also be:

* **strongly typed** (`string`, `number`, `enum`, `json`, `date`, etc.)
* **policy-classified** (public/internal/confidential/secret)
* **encrypted** (field-level encryption using the `encrypted_fields` table)
* **UI-aware** (because of the `ui_schema` JSON)
* **queryable** (via the filter/query engine)

So: every `type_definition.field_schema` describes potential custom fields; every object instance populates a subset of them; and if the fieldâ€™s `encryption` flag is true, its value is stored separately in `encrypted_fields`.

---

## ğŸ§± 2. How the encrypted_fields table fits in

That table isnâ€™t just for secrets â€” itâ€™s the **detached storage layer** for sensitive custom fields.

| Column                 | Meaning                                |
| ---------------------- | -------------------------------------- |
| `entity_table`         | `'objects'` or `'events'`              |
| `entity_id`            | The object/event ID                    |
| `field_name`           | The logical field name from the schema |
| `encrypted_value`      | AEAD-encrypted blob                    |
| `classification_level` | maps to MAC labels                     |
| `compartment_markings` | same compartments array as the object  |
| `organization_id`      | ensures tenant scoping                 |

The main objectâ€™s `data` JSON will either:

```json
{
  "customer_name": "Jane Doe",
  "credit_card_number": { "_encrypted": true, "key_id": "tenant-key-42" }
}
```

The actual ciphertext lives in `encrypted_fields`.

---

## âš™ï¸ 3. Runtime architecture

Create a small **FieldService** under `fabric/services/` that sits between `ObjectService` and `Vault`.

```
src/fabric/services/
  field_service.rs
```

```rust
// fabric/services/field_service.rs
use anyhow::Result;
use crate::fabric::services::encrypted_fields::EncryptedFields;
use crate::security::secret_vault::SecretVault;

pub struct FieldService {
    pub encrypted: EncryptedFields,
    pub vault: Arc<dyn SecretVault>,
}

impl FieldService {
    /// Extracts any encrypted fields from a JSON payload and persists them.
    pub async fn extract_and_encrypt(
        &self,
        org: uuid::Uuid,
        entity_table: &str,
        entity_id: uuid::Uuid,
        data: &mut serde_json::Value,
        schema: &serde_json::Value,
    ) -> Result<()> {
        for (key, def) in schema["fields"].as_object().unwrap_or(&Default::default()) {
            if def["encrypted"].as_bool().unwrap_or(false) {
                if let Some(value) = data.get(key).cloned() {
                    let plaintext = serde_json::to_vec(&value)?;
                    let key_id = def["key_id"].as_str().unwrap_or("default");
                    let blob = self.vault.wrap(&key_id.into(), &plaintext)?;
                    self.encrypted.repo.upsert(org, entity_table, entity_id, key, &blob).await?;
                    data[key] = json!({"_encrypted": true, "key_id": key_id});
                }
            }
        }
        Ok(())
    }

    /// Reads and decrypts any encrypted fields on fetch.
    pub async fn decrypt_in_place(
        &self,
        org: uuid::Uuid,
        entity_table: &str,
        entity_id: uuid::Uuid,
        data: &mut serde_json::Value,
        schema: &serde_json::Value,
    ) -> Result<()> {
        for (key, def) in schema["fields"].as_object().unwrap_or(&Default::default()) {
            if def["encrypted"].as_bool().unwrap_or(false) {
                if data.get(key).map(|v| v.get("_encrypted").is_some()).unwrap_or(false) {
                    let key_id = def["key_id"].as_str().unwrap_or("default");
                    if let Some(blob) = self.encrypted.repo.fetch(org, entity_table, entity_id, key).await? {
                        let plain = self.vault.unwrap(&key_id.into(), &blob)?;
                        data[key] = serde_json::from_slice(&plain)?;
                    }
                }
            }
        }
        Ok(())
    }
}
```

This is what `ObjectService.create()` and `.get()` will call before persisting or returning any data.

---

## ğŸ§  4. Schema-level encryption rules

Each field in `type_definitions.field_schema` can carry metadata like:

```json
{
  "fields": {
    "credit_card_number": {
      "type": "string",
      "label": "Credit Card Number",
      "encrypted": true,
      "key_id": "customer-pii",
      "classification": "confidential",
      "ui": { "widget": "masked-input" }
    },
    "last_four": { "type": "string" }
  }
}
```

At runtime:

* The **Vault** decides which tenant key to use.
* The **FieldService** persists encrypted fields in `encrypted_fields`.
* The **PolicySnapshot** defines which roles can view/decrypt them.

---

## ğŸ” 5. Policy-based control of field access

In your policy schema (TOML):

```toml
[security.fields]
allow_encrypted_read = ["admin","compliance"]
allow_encrypted_write = ["admin","service:importer"]
```

At runtime:

```rust
if !rbac::role_in(ctx, policy.sec.allow_encrypted_read) {
    data.remove(field_name);
}
```

This allows *field-level RBAC/MAC enforcement*.

---

## âš¡ 6. Integration points

| System                | How it connects                                                      |
| --------------------- | -------------------------------------------------------------------- |
| **Vault**             | wraps/unwraps keys and performs crypto                               |
| **TypeRegistry**      | provides schema metadata for each field                              |
| **ObjectService**     | uses FieldService during create/update/get                           |
| **PolicySnapshot**    | defines access control and key rotation                              |
| **EmbeddingRegistry** | skips or hashes encrypted fields before embedding                    |
| **QueryService**      | only filters on decrypted fields if user has rights                  |
| **UI Builder**        | uses `ui_schema` to render encrypted fields as masked/locked widgets |

---

## ğŸ§® 7. Caching and performance

Because field-level decryption can be expensive:

* Cache decrypted values in `StateManager` under tenant/session scope.
* Use adaptive TTL based on policy: e.g., â€œkeep decrypted for 15 seconds.â€

```toml
[state.cache.fields.ttl_secs = 15]
```

---

## ğŸ§° 8. Developer ergonomics

Provide a small API facade for apps/plugins:

```rust
let obj = objects.get_secure(ctx, id).await?;
let cc = obj.field("credit_card_number").as_string_secure()?;
```

Internally, that calls FieldService.decrypt_in_place() and checks policy.

---

## ğŸ§­ 9. Why this matters

This design gives you:

* **Dynamic schemas** â†’ fields change per type_definition
* **Granular security** â†’ field-level encryption + MAC labels
* **Policy-driven visibility** â†’ different roles see different field sets
* **Auditable key use** â†’ every wrap/unwrap logged in Vault/ForensicLogger
* **AI-safe embeddings** â†’ sensitive fields automatically excluded

You regain everything that made the original platformâ€™s â€œfield fabricâ€ so powerful â€” but now cleanly integrated into the Rust runtime, Vault, and Policy layers.

---

### TL;DR

| Subsystem                 | Purpose                                 |
| ------------------------- | --------------------------------------- |
| **Type definitions**      | Define custom fields + encryption flags |
| **EncryptedFields table** | Stores ciphertext blobs                 |
| **FieldService**          | Runtime extract/encrypt/decrypt         |
| **Vault**                 | Key storage and crypto operations       |
| **Policy**                | Controls who can read/write/decrypt     |
| **StateManager**          | Caches decrypted values briefly         |
| **UI/Schema**             | Drives masked widgets and forms         |

---

If you want, I can generate a **complete FieldService + integration patch** that shows how `ObjectService.create()` and `.get()` interact with it (including sample SQL for `encrypted_fields_repo` and how it ties to Vault).
Would you like me to write that next?


Perfect â€” below is a full, **drop-in FieldService implementation** wired into your existing fabric layer (ObjectsRepo + EncryptedFieldsRepo + Vault + Policy).
It restores secure *custom-field* behavior exactly the way it worked in the schema you pasted, but now cleanly in Rust.

---

# ğŸ“ Recommended placement

```
src/fabric/services/
  field_service.rs          â† new
  encrypted_fields.rs       â† already exists
```

---

## ğŸ§© 1. `field_service.rs`

```rust
use anyhow::{Result, bail};
use serde_json::{Value, json};
use std::sync::Arc;
use uuid::Uuid;

use crate::security::secret_vault::SecretVault;
use crate::fabric::services::encrypted_fields::EncryptedFields;
use crate::security::{rbac, mac_engine};
use crate::policy::production_policy_snapshot::current_policy;

/// Handles encryption / decryption of individual fields inside object or event JSON data.
pub struct FieldService {
    pub encrypted: Arc<EncryptedFields>,
    pub vault: Arc<dyn SecretVault>,
}

impl FieldService {
    /// Encrypts any schema fields marked `"encrypted": true` before writing to DB.
    pub async fn encrypt_fields(
        &self,
        ctx: &crate::security::SubjectCtx,
        org: Uuid,
        entity_table: &str,
        entity_id: Uuid,
        data: &mut Value,
        field_schema: &Value,
    ) -> Result<()> {
        let fields = field_schema["fields"].as_object().unwrap_or(&Default::default());
        let pol = current_policy();

        for (name, def) in fields {
            if def["encrypted"].as_bool().unwrap_or(false) {
                if let Some(value) = data.get(name).cloned() {
                    // RBAC check
                    if !rbac::can(ctx, "field:encrypt") &&
                       !pol.sec.allow_encrypted_write.contains(&ctx.role) {
                        bail!("forbidden: field encryption not allowed");
                    }

                    // Serialize & encrypt
                    let plaintext = serde_json::to_vec(&value)?;
                    let key_id = def["key_id"].as_str().unwrap_or("default");
                    let blob = self.vault.wrap(&key_id.into(), &plaintext)?;
                    self.encrypted.repo
                        .upsert(org, entity_table, entity_id, name, &blob)
                        .await?;

                    // Replace value marker in JSON
                    data[name] = json!({
                        "_encrypted": true,
                        "key_id": key_id,
                        "classification": def["classification"].as_str().unwrap_or("internal")
                    });
                }
            }
        }
        Ok(())
    }

    /// Decrypts encrypted fields on read (if policy & RBAC permit).
    pub async fn decrypt_fields(
        &self,
        ctx: &crate::security::SubjectCtx,
        org: Uuid,
        entity_table: &str,
        entity_id: Uuid,
        data: &mut Value,
        field_schema: &Value,
    ) -> Result<()> {
        let fields = field_schema["fields"].as_object().unwrap_or(&Default::default());
        let pol = current_policy();

        for (name, def) in fields {
            if def["encrypted"].as_bool().unwrap_or(false) {
                if let Some(meta) = data.get(name) {
                    if meta.get("_encrypted").is_some() {
                        // Policy / RBAC enforcement
                        if !rbac::can(ctx, "field:decrypt") &&
                           !pol.sec.allow_encrypted_read.contains(&ctx.role) {
                            // Hide field if not permitted
                            data[name] = json!(null);
                            continue;
                        }

                        // Optional MAC check
                        let target_label = crate::fabric::Label {
                            classification: def["classification"]
                                .as_str()
                                .unwrap_or("internal")
                                .to_string(),
                            compartments: vec![],
                        };
                        if !mac_engine::dominates(&ctx.label, &target_label) {
                            data[name] = json!(null);
                            continue;
                        }

                        // Fetch ciphertext and decrypt
                        let key_id = def["key_id"].as_str().unwrap_or("default");
                        if let Some(blob) = self.encrypted.repo
                            .fetch(org, entity_table, entity_id, name)
                            .await? {
                            let plain = self.vault.unwrap(&key_id.into(), &blob)?;
                            data[name] = serde_json::from_slice::<Value>(&plain)?;
                        }
                    }
                }
            }
        }
        Ok(())
    }
}
```

---

## ğŸ—„ï¸ 2. `encrypted_fields_repo.rs` (for reference)

```rust
use anyhow::Result;
use sqlx::{FromRow, Row};
use uuid::Uuid;

#[derive(FromRow, Debug)]
pub struct EncryptedFieldRow {
    pub organization_id: Uuid,
    pub entity_table: String,
    pub entity_id: Uuid,
    pub field_name: String,
    pub encrypted_value: Vec<u8>,
}

pub struct EncryptedFieldsRepo {
    pub db: crate::fabric::db::Db,
}

impl EncryptedFieldsRepo {
    pub async fn upsert(
        &self,
        org: Uuid,
        table: &str,
        id: Uuid,
        field: &str,
        blob: &[u8],
    ) -> Result<()> {
        sqlx::query!(
            r#"
            INSERT INTO encrypted_fields
              (organization_id, entity_table, entity_id, field_name, encrypted_value)
            VALUES ($1,$2,$3,$4,$5)
            ON CONFLICT (organization_id, entity_table, entity_id, field_name)
              DO UPDATE SET encrypted_value = EXCLUDED.encrypted_value,
                            updated_at = now()
            "#,
            org, table, id, field, blob
        )
        .execute(&self.db)
        .await?;
        Ok(())
    }

    pub async fn fetch(
        &self,
        org: Uuid,
        table: &str,
        id: Uuid,
        field: &str,
    ) -> Result<Option<Vec<u8>>> {
        let row = sqlx::query!(
            r#"SELECT encrypted_value FROM encrypted_fields
               WHERE organization_id=$1 AND entity_table=$2
                     AND entity_id=$3 AND field_name=$4"#,
            org, table, id, field
        )
        .fetch_optional(&self.db)
        .await?;
        Ok(row.map(|r| r.encrypted_value))
    }
}
```

---

## ğŸ”„ 3. Integration into `ObjectService`

When creating or reading an object:

```rust
// Before insert
let schema = self.types.get(ctx.tenant, &type_name.0).unwrap().field_schema.clone();
let mut data = payload.clone();
self.field_service
    .encrypt_fields(ctx, ctx.tenant, "objects", id, &mut data, &schema)
    .await?;
self.repo.insert(&ObjectRow { data, ... }).await?;
```

On retrieval:

```rust
let mut data = object.data.clone();
let schema = self.types.get(ctx.tenant, &object.type_name).unwrap().field_schema.clone();
self.field_service
    .decrypt_fields(ctx, ctx.tenant, "objects", object.id, &mut data, &schema)
    .await?;
```

---

## ğŸ§  4. Policy schema extension (TOML)

```toml
[security.fields]
allow_encrypted_read = ["admin","compliance"]
allow_encrypted_write = ["admin","importer"]
default_key_id = "default"
cache_ttl_secs = 15
```

---

## ğŸ§® 5. Optional short-term cache

Wire a small per-session cache into `StateManager`:

```rust
state_manager.register_store(
  "decrypted_fields",
  Arc::new(InMemoryStore::new(CachePolicyKind::Ttl{ ttl_secs: 15 })),
  CachePolicy { kind: CachePolicyKind::Ttl{ ttl_secs: 15 }, org_scope: None, tenant_scope: None }
);
```

`FieldService.decrypt_fields` can first check this store before hitting Vault/DB.

---

## ğŸ§° 6. Observability hooks

Add to `MetricsRegistry`:

* `fields_encrypted_total`
* `fields_decrypted_total`
* `fields_decrypt_denied_total`

And emit a forensic log:

```rust
forensic_logger.log(json!({
  "event": "field.decrypt",
  "user": ctx.user,
  "field": name,
  "result": "success"
}));
```

---

## âœ… 7. Summary

| Layer                            | Role                                                 |
| -------------------------------- | ---------------------------------------------------- |
| **EncryptedFields table**        | Persistent storage for encrypted custom fields       |
| **FieldService**                 | Handles extract/encrypt/decrypt                      |
| **Vault**                        | Performs crypto operations                           |
| **PolicySnapshot**               | Determines who can see or edit encrypted data        |
| **StateManager**                 | Optionally caches decrypted values                   |
| **ObjectService / EventService** | Call FieldService transparently                      |
| **UI**                           | Renders masked inputs; requests decryption as needed |

This brings back *secure, dynamic custom fields* in a composable, policy-driven way.

---

If youâ€™d like next, I can add:

1. the **Vault call integration** with key rotation + audit events, and
2. the **UI schema convention** (how the builder reads encryption flags to render masked/locked fields).

Would you like me to generate those next?


Awesomeâ€”hereâ€™s the next slice wired cleanly: **Vault integration with key rotation + audit**, plus a **UI schema convention** so the builder knows when to mask/lock fields and how to request decrypt.

---

# 1) Vault integration: key rotation + audit

## Files to add/extend

```
src/security/
  secret_vault.rs              // add rotate + key metadata
  keyring.rs                   // (optional) higher-level orchestration
src/observability/
  forensic_logger.rs           // already have; add events below
src/fabric/services/
  field_service.rs             // call vault + emit audit events
```

## `secret_vault.rs` (trait updates + default software impl)

```rust
pub struct KeyId(pub String);

#[derive(Clone, Debug, serde::Serialize)]
pub struct KeyMeta {
    pub key_id: String,
    pub purpose: String,          // "bundle_signing", "field_customer_pii", ...
    pub alg: String,              // "ed25519", "xchacha20poly1305", ...
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub rotated_from: Option<String>,
    pub active: bool,
}

pub trait SecretVault: Send + Sync {
    fn generate_key(&self, purpose: &str, alg: &str) -> anyhow::Result<KeyId>;
    fn list_keys(&self, purpose: Option<&str>) -> anyhow::Result<Vec<KeyMeta>>;
    fn rotate_key(&self, purpose: &str, alg: &str) -> anyhow::Result<KeyId>; // new active key; old remains for unwrap/verify
    fn deactivate_key(&self, key: &KeyId) -> anyhow::Result<()>;

    // Crypto primitives
    fn wrap(&self, key: &KeyId, plaintext: &[u8]) -> anyhow::Result<Vec<u8>>;
    fn unwrap(&self, key: &KeyId, blob: &[u8]) -> anyhow::Result<Vec<u8>>;
    fn sign(&self, key: &KeyId, msg: &[u8]) -> anyhow::Result<Vec<u8>>;
    fn verify(&self, key: &KeyId, msg: &[u8], sig: &[u8]) -> anyhow::Result<bool>;
}
```

**Rotation model**

* **Rotate** creates a *new* active key for the purpose and keeps previous keys in a *read-only* state (unwrap/verify allowed, wrap/sign denied).
* Use `rotated_from` for lineage (helps audits).
* Field ciphertext continues to decrypt using the old key until you run a **re-envelope job** (optional background job that rewrites blobs using the new key in batches).

## Policy knobs (TOML)

```toml
[security.keys]
field_default_key_purpose = "field_default"
field_alg = "xchacha20poly1305"
rotation_days = 180
reenvelope_batch_size = 500
```

## FieldService hooks (encrypt/decrypt + audit)

Add these **audit emits** in `field_service.rs` right after successful wrap/unwrap:

```rust
self.forensic.log(json!({
  "evt":"fields.encrypt",
  "org": org, "table": entity_table, "id": entity_id,
  "field": name, "key_id": key_id, "actor": ctx.user
}));

// on decrypt success:
self.forensic.log(json!({
  "evt":"fields.decrypt",
  "org": org, "table": entity_table, "id": entity_id,
  "field": name, "key_id": key_id, "actor": ctx.user, "result":"ok"
}));

// on decrypt denied:
self.forensic.log(json!({
  "evt":"fields.decrypt_denied",
  "org": org, "table": entity_table, "id": entity_id,
  "field": name, "actor": ctx.user, "reason":"rbac_or_mac"
}));
```

Expose rotation as an **admin action** (goes through your single front-door dispatch):

```rust
// dispatch: "vault.rotate"
async fn vault_rotate(payload: Value, ctx: Context) -> Result<Value> {
  require_admin(&ctx)?;
  let purpose = payload["purpose"].as_str().unwrap_or("field_default");
  let alg = payload["alg"].as_str().unwrap_or("xchacha20poly1305");
  let new_key = vault.rotate_key(purpose, alg)?;
  forensic.log(json!({"evt":"keys.rotated","purpose":purpose,"new_key":new_key.0,"actor":ctx.actor}));
  Ok(json!({"ok":true,"key_id":new_key.0}))
}
```

## Re-enveloping job (optional but recommended)

Add a background job to migrate old blobs to the newest key gradually:

```
src/runtime/job/re_envelope_fields.rs
```

Pseudocode:

```rust
// picks N fields per run where marker.key_id != vault.active_key("field_default")
for each row in encrypted_fields where org=? and field_key != active_key:
  let pt = vault.unwrap(&old_key, blob)?;
  let new_blob = vault.wrap(&new_key, &pt)?;
  repo.upsert(org, table, id, field, &new_blob).await?;
  forensic.log({"evt":"fields.reenvelope","field":field, "old_key":old, "new_key":new});
```

Schedule it daily/weekly depending on `rotation_days`.

---

# 2) UI schema convention (masked/locked fields)

**Goal**: the builder and generated forms know how to display, request, and guard encrypted custom fields consistently.

## UI schema contract (stored under `type_definitions.ui_schema`)

```json
{
  "fields": {
    "credit_card_number": {
      "widget": "masked-input",
      "mask": "####-####-####-####",
      "encrypted": true,
      "unlock": {
        "policy_claim": "field:decrypt",        // RBAC claim
        "prompt": "View credit card number?",   // user confirmation text
        "ttl_secs": 15                          // client-side cache/visibility window
      },
      "redact_in_lists": true,
      "display": {
        "summary": "last4",                     // computed from decrypted or stored 'last_four'
        "icon": "lucide-credit-card"
      }
    }
  }
}
```

### Behavior

* If `encrypted: true` and no unlock yet â†’ render masked placeholder (e.g., `â€¢â€¢â€¢â€¢ â€¢â€¢â€¢â€¢ â€¢â€¢â€¢â€¢ 1234` if `summary` present).
* Clicking an **Unlock** button triggers a Tauri command (front-door `dispatch("fields.decrypt")`) with `{object_id, field}`.
* Backend enforces RBAC/MAC; if allowed, it returns plaintext **once**; UI shows it for `ttl_secs`, then re-masks.
* If `redact_in_lists` is true, table/list views always show summary or â€œLOCKEDâ€.

## Frontend bindings (Tauri/JS)

Minimal pattern to standardize:

```js
import { invoke } from "@tauri-apps/api/tauri";

async function unlockField({ objectId, field, typeName }) {
  const res = await invoke("dispatch", {
    cmd: "fields.decrypt",
    payload: { objectId, field, typeName }
  });
  return res.value; // plaintext string/json
}
```

Then your form component:

```js
function SecureField({ meta, value, onChange, ctx }) {
  if (!meta.encrypted) return <TextInput value={value} onChange={onChange} />;

  const [visible, setVisible] = useState(false);
  const [clear, setClear] = useState(null);

  const show = async () => {
    const v = await unlockField({objectId: ctx.objectId, field: meta.name, typeName: ctx.typeName});
    setClear(v); setVisible(true);
    setTimeout(()=>{ setVisible(false); setClear(null); }, (meta.unlock.ttl_secs ?? 15)*1000);
  }

  if (!visible) {
    return (
      <div className="relative">
        <Masked value={value?.summary ?? "LOCKED"} mask={meta.mask}/>
        <button onClick={show} className="btn btn-xs ml-2">Unlock</button>
      </div>
    );
  }

  return <TextInput value={clear} onChange={onChange} secure />;
}
```

> **Note:** the backend should **never** send plaintext unless the field is explicitly requested by name (prevents accidental mass-decrypt).

---

# 3) End-to-end flow (quick)

1. **Create/Update object**

   * UI posts JSON with plaintext for encrypted fields.
   * `ObjectService` â†’ `FieldService.encrypt_fields()` moves value to `encrypted_fields` & leaves marker `{_encrypted:true, key_id}`.
   * Audit `fields.encrypt`.

2. **Read object (normal)**

   * `ObjectService.get()` returns markers for encrypted fields (no plaintext).
   * UI renders masked.

3. **User clicks Unlock**

   * Tauri invokes `dispatch("fields.decrypt", {objectId, field, typeName})`.
   * `FieldService.decrypt_fields()` checks RBAC/MAC, unwraps, returns *only requested field*.
   * UI shows it temporarily; optional per-session cache.
   * Audit `fields.decrypt` (or `fields.decrypt_denied`).

4. **Key rotation**

   * Admin triggers `vault.rotate` or scheduled per policy.
   * Re-envelop job rewrites blobs to new key in batches.
   * Audits: `keys.rotated`, `fields.reenvelope`.

---

# 4) Safety defaults

* Backend never includes encrypted field plaintext in list/bulk queries.
* Decrypt is **per-field, per-call** and always audited.
* UI respects `redact_in_lists`; shows summaries only.
* Policy can disable decrypt entirely for certain roles or tenants.
* Optional: watermark decrypted values in UI (light gray â€œsensitiveâ€) and prevent copy via CSS/JS (best-effort).

---

# 5) Quick test cases to add

* **Encrypt on create**: plaintext in â†’ marker out + row in `encrypted_fields`.
* **Denied decrypt**: non-authorized role â†’ `null` and `fields.decrypt_denied` audit.
* **Allowed decrypt**: authorized role â†’ plaintext returned; audit recorded.
* **Rotation**: after rotate, decrypt old blobs still works; after re-envelope, old key no longer used for that field.
* **UI TTL**: value re-masks after TTL; new decrypt required.

---

## TL;DR

* Vault now supports **rotation** with lineage, and FieldService **audits** every encrypt/decrypt.
* UI schema gains a clear convention to **mask/lock**, **unlock by policy**, and **auto-remask**.
* The flow keeps sensitive data out of bulk responses, and puts **fine-grained control** in your policy + dispatch guard.

If you want, I can also sketch the **dispatch handlers** for `fields.decrypt` and `fields.encrypt_on_update` (with RBAC/MAC checks baked in) so you can drop them into your single front-door file.

Perfect â€” this ties everything together: your **secure field encryption** system now becomes a first-class citizen of your runtime by exposing **two universal dispatch commands** that flow through your single front door.

These commands are small but critical:

* `fields.decrypt` â†’ granular, audited, per-field decrypt
* `fields.encrypt_on_update` â†’ re-encrypts changed fields on update (or new custom fields added later)

They sit in the **Dispatch Layer**, not as direct Tauri commands, so that all the RBAC, MAC, and PolicySnapshot enforcement you already built applies automatically.

---

# ğŸ“ File placement

```
src/dispatch/
  mod.rs
  fields_commands.rs    â† new
```

---

## 1ï¸âƒ£ `fields_commands.rs`

```rust
use anyhow::{Result, bail};
use serde_json::{Value, json};
use uuid::Uuid;
use std::sync::Arc;

use crate::fabric::services::{
    field_service::FieldService,
    type_registry::TypeRegistry,
    object_service::ObjectService,
};
use crate::security::{rbac, mac_engine};
use crate::policy::production_policy_snapshot::current_policy;
use crate::observability::forensic_logger::ForensicLogger;

pub struct FieldCommands {
    pub fields: Arc<FieldService>,
    pub types: Arc<TypeRegistry>,
    pub objects: Arc<ObjectService>,
    pub forensic: Arc<ForensicLogger>,
}

impl FieldCommands {
    /// Decrypt a single field value on-demand (per-field, per-request)
    pub async fn decrypt(&self, ctx: &crate::security::SubjectCtx, payload: &Value) -> Result<Value> {
        let org = ctx.tenant;
        let object_id = payload["objectId"]
            .as_str()
            .ok_or_else(|| anyhow::anyhow!("missing objectId"))?;
        let object_id = Uuid::parse_str(object_id)?;
        let field = payload["field"]
            .as_str()
            .ok_or_else(|| anyhow::anyhow!("missing field"))?;
        let type_name = payload["typeName"]
            .as_str()
            .ok_or_else(|| anyhow::anyhow!("missing typeName"))?;

        // Policy enforcement
        let pol = current_policy();
        if !rbac::can(ctx, "field:decrypt") && !pol.sec.allow_encrypted_read.contains(&ctx.role) {
            self.forensic.log(json!({
                "evt":"fields.decrypt_denied","field":field,"actor":ctx.user,"reason":"rbac"
            }));
            bail!("forbidden");
        }

        // Load schema and object
        let schema = self
            .types
            .get(org, type_name)
            .ok_or_else(|| anyhow::anyhow!("type not found"))?
            .field_schema
            .clone();

        let mut obj = self.objects.repo.get(org, object_id)
            .await?
            .ok_or_else(|| anyhow::anyhow!("object not found"))?;
        let mut data = obj.data.clone();

        // Decrypt single field
        self.fields
            .decrypt_fields(ctx, org, "objects", object_id, &mut data, &schema)
            .await?;

        let value = data.get(field).cloned().unwrap_or(Value::Null);

        // Audit
        self.forensic.log(json!({
            "evt":"fields.decrypt",
            "actor":ctx.user,
            "field":field,
            "object_id":object_id,
            "key_id":value.get("_key_id").unwrap_or(&json!(null))
        }));

        Ok(json!({ "field": field, "value": value }))
    }

    /// Encrypt changed or newly added fields on update
    pub async fn encrypt_on_update(
        &self,
        ctx: &crate::security::SubjectCtx,
        payload: &Value,
    ) -> Result<Value> {
        let org = ctx.tenant;
        let type_name = payload["typeName"]
            .as_str()
            .ok_or_else(|| anyhow::anyhow!("missing typeName"))?;
        let object_id = Uuid::parse_str(payload["objectId"].as_str().unwrap_or_default())?;
        let mut data = payload["data"].clone();

        // Load schema and encrypt marked fields
        let schema = self
            .types
            .get(org, type_name)
            .ok_or_else(|| anyhow::anyhow!("type not found"))?
            .field_schema
            .clone();

        self.fields
            .encrypt_fields(ctx, org, "objects", object_id, &mut data, &schema)
            .await?;

        self.objects.repo.update_data(org, object_id, &data).await?;

        self.forensic.log(json!({
            "evt":"fields.encrypt_on_update",
            "actor":ctx.user,
            "object_id":object_id
        }));

        Ok(json!({"ok": true}))
    }
}
```

---

## 2ï¸âƒ£ Integrate into your main dispatch switch

In your single front-door (`dispatch.rs` or `production-main.rs`):

```rust
use crate::dispatch::fields_commands::FieldCommands;

pub async fn dispatch(cmd: &str, payload: Value, ctx: Context) -> Result<Value> {
    match cmd {
        "fields.decrypt" => ctx.field_commands.decrypt(&ctx.subject, &payload).await,
        "fields.encrypt_on_update" => ctx.field_commands.encrypt_on_update(&ctx.subject, &payload).await,
        // other commands...
        _ => bail!("unknown command"),
    }
}
```

Make sure `Context` (your per-request context) holds `field_commands: Arc<FieldCommands>` or has access to them through a `RegistryHub`.

---

## 3ï¸âƒ£ Forensic log conventions

The ForensicLogger already writes to your audit DB and emits on EventBus.
Add two event patterns:

| Event                      | Description                         |
| -------------------------- | ----------------------------------- |
| `fields.decrypt`           | user successfully decrypted a field |
| `fields.decrypt_denied`    | attempted unauthorized decrypt      |
| `fields.encrypt_on_update` | field re-encrypted on update        |
| `fields.reenvelope`        | part of rotation job                |

All are tenant-scoped, classification-labeled, and feed your observability dashboard.

---

## 4ï¸âƒ£ Front-end usage examples

### Unlock

```js
const res = await invoke("dispatch", {
  cmd: "fields.decrypt",
  payload: { objectId: obj.id, field: "credit_card_number", typeName: obj.type_name }
});
if (res.value) setPlain(res.value);
```

### Save

```js
await invoke("dispatch", {
  cmd: "fields.encrypt_on_update",
  payload: { objectId: obj.id, typeName: obj.type_name, data: updated }
});
```

---

## 5ï¸âƒ£ Optional refinements

| Enhancement            | Benefit                                                 |
| ---------------------- | ------------------------------------------------------- |
| **Batch decrypt**      | decrypt multiple fields at once (`fields.decrypt_many`) |
| **Mask summaries**     | precompute safe summaries (e.g., last4) in UI builder   |
| **Async audit writer** | queue forensic logs to prevent blocking UI calls        |
| **Rate limiting**      | throttle decrypt attempts per actor                     |
| **Streaming decrypt**  | for large binary fields; use Vault stream API           |

---

## âœ… TL;DR

These two commands give you:

* **Uniform access control** through the same dispatch gate as all other actions.
* **Field-level granularity** (no accidental bulk plaintext exposure).
* **Full audit coverage** for every key unwrap.
* **Policy-governed write path** for encrypting updates or new custom fields.
* **UI simplicity** â€” one unlock button and one save hook.

---

Would you like me to add a *third* companion command â€” `fields.rotate_keys` â€” that lets admins or automated jobs trigger rotation/re-enveloping through the same dispatch system (instead of a separate job runner)?


Perfect â€” hereâ€™s the **`fields.rotate_keys`** command: a secure, auditable admin-only dispatch that handles **key rotation and optional re-enveloping of encrypted fields** in batches.

It runs entirely through your **single front-door dispatch**, just like every other action, so RBAC, MAC, policy, and forensic logging apply automatically.

---

# ğŸ“ File placement

Add to the same file as the other field commands:

```
src/dispatch/
  fields_commands.rs
```

---

## 1ï¸âƒ£ Updated `fields_commands.rs`

Append this method to your existing `impl FieldCommands`:

```rust
/// Rotate active Vault key(s) and optionally re-envelop existing encrypted field blobs.
/// Usage: dispatch("fields.rotate_keys", { "purpose": "field_default", "reenvelope": true })
pub async fn rotate_keys(
    &self,
    ctx: &crate::security::SubjectCtx,
    payload: &serde_json::Value,
) -> anyhow::Result<serde_json::Value> {
    use chrono::Utc;
    use serde_json::json;
    use anyhow::bail;

    // --- RBAC/Policy Enforcement ---
    let pol = crate::policy::production_policy_snapshot::current_policy();
    if !rbac::can(ctx, "vault:rotate")
        && !pol.sec.allow_encrypted_write.contains(&ctx.role)
    {
        self.forensic.log(json!({
            "evt":"keys.rotate_denied",
            "actor":ctx.user,
            "reason":"rbac"
        }));
        bail!("forbidden");
    }

    // --- Parameters ---
    let purpose = payload["purpose"]
        .as_str()
        .unwrap_or("field_default");
    let alg = payload["alg"]
        .as_str()
        .unwrap_or("xchacha20poly1305");
    let reenvelope = payload["reenvelope"]
        .as_bool()
        .unwrap_or(true);
    let batch_size = payload["batch_size"]
        .as_u64()
        .unwrap_or(500) as usize;

    // --- Key rotation ---
    let new_key = self.fields.vault.rotate_key(purpose, alg)?;
    let new_key_id = new_key.0.clone();

    self.forensic.log(json!({
        "evt":"keys.rotated",
        "purpose":purpose,
        "alg":alg,
        "actor":ctx.user,
        "new_key":new_key_id,
        "ts":Utc::now()
    }));

    // --- Optional re-enveloping ---
    if reenvelope {
        let active_org = ctx.tenant;
        let repo = &self.fields.encrypted.repo;
        let old_key_rows = repo
            .list_for_key_rotation(active_org, purpose, &new_key_id, batch_size)
            .await?;

        for row in old_key_rows {
            if let Ok(plain) = self.fields.vault.unwrap(&KeyId(row.old_key_id.clone()), &row.encrypted_value) {
                let blob = self.fields.vault.wrap(&KeyId(new_key_id.clone()), &plain)?;
                repo.upsert(active_org, &row.entity_table, row.entity_id, &row.field_name, &blob)
                    .await?;
                self.forensic.log(json!({
                    "evt":"fields.reenvelope",
                    "actor":ctx.user,
                    "field":row.field_name,
                    "entity":row.entity_id,
                    "old_key":row.old_key_id,
                    "new_key":new_key_id
                }));
            }
        }
    }

    Ok(json!({
        "ok": true,
        "new_key": new_key_id,
        "reenvelope_started": reenvelope
    }))
}
```

---

## 2ï¸âƒ£ Support function in `encrypted_fields_repo.rs`

Add this helper to list candidate fields for rotation:

```rust
pub async fn list_for_key_rotation(
    &self,
    org: Uuid,
    purpose: &str,
    new_key_id: &str,
    batch: usize,
) -> Result<Vec<RotationRow>> {
    // You can identify old key ids from metadata column or from marker in JSON
    // If you don't track key_id per row yet, you can temporarily rotate all fields for org
    let rows = sqlx::query_as!(
        RotationRow,
        r#"
        SELECT entity_table, entity_id, field_name, encrypted_value, 'unknown' as old_key_id
        FROM encrypted_fields
        WHERE organization_id = $1
        ORDER BY created_at ASC
        LIMIT $2
        "#,
        org,
        batch as i64
    )
    .fetch_all(&self.db)
    .await?;
    Ok(rows)
}

#[derive(sqlx::FromRow)]
pub struct RotationRow {
    pub entity_table: String,
    pub entity_id: Uuid,
    pub field_name: String,
    pub encrypted_value: Vec<u8>,
    pub old_key_id: String,
}
```

Later, if you store the key_id in a metadata column (recommended), this query can filter precisely on `old_key_id <> new_key_id`.

---

## 3ï¸âƒ£ Dispatch registration

Add the new command to your main dispatcher:

```rust
"fields.rotate_keys" => ctx.field_commands.rotate_keys(&ctx.subject, &payload).await,
```

---

## 4ï¸âƒ£ Policy additions

Extend your TOML policy:

```toml
[security.keys]
rotation_days = 180
allow_manual_rotation = ["admin","security_officer"]
allow_reenvelope = ["admin"]
```

And link in the policy loader:

```rust
if !pol.sec.allow_manual_rotation.contains(&ctx.role) { bail!("forbidden"); }
```

---

## 5ï¸âƒ£ Forensic / Audit events produced

| Event                      | Description                         |
| -------------------------- | ----------------------------------- |
| `keys.rotated`             | new key generated                   |
| `keys.rotate_denied`       | unauthorized attempt                |
| `fields.reenvelope`        | ciphertext re-wrapped under new key |
| `fields.reenvelope_failed` | optional error case                 |

Each event includes: `actor`, `purpose`, `alg`, `old_key`, `new_key`, and timestamp.

---

## 6ï¸âƒ£ Example front-end / admin call

```js
await invoke("dispatch", {
  cmd: "fields.rotate_keys",
  payload: {
    purpose: "field_default",
    alg: "xchacha20poly1305",
    reenvelope: true,
    batch_size: 1000
  }
});
```

Response:

```json
{
  "ok": true,
  "new_key": "tenant-key-2025-05-01",
  "reenvelope_started": true
}
```

---

## 7ï¸âƒ£ Optional future refinements

| Enhancement                  | Purpose                                                    |
| ---------------------------- | ---------------------------------------------------------- |
| **Async job queue**          | Offload re-enveloping so dispatch call returns immediately |
| **Rotation schedule daemon** | Auto-trigger by policy.rotation_days                       |
| **Key health metrics**       | emit age / last-rotation to Observability                  |
| **Key escrow / backup**      | optionally store encrypted old key material for compliance |
| **Partial re-envelop**       | filter by tenant or type_name                              |

---

## âœ… TL;DR

This adds your final â€œsecure fieldsâ€ lifecycle command:

| Command                    | Function                                               |
| -------------------------- | ------------------------------------------------------ |
| `fields.decrypt`           | on-demand field read (RBAC + audit)                    |
| `fields.encrypt_on_update` | encrypt new/changed fields                             |
| `fields.rotate_keys`       | rotate active key + optional re-envelop old ciphertext |

All three are policy-governed, auditable, tenant-scoped, and use the same front-door dispatch as everything else.

This completes the full **secure field lifecycle** in your runtime: creation â†’ encryption â†’ controlled read â†’ key rotation â†’ re-envelop â†’ audit.

---

If youâ€™d like, I can now add the **Rotation Scheduler Service** â€” a background task that automatically calls `fields.rotate_keys` based on `policy.keys.rotation_days`, logging and queuing re-envelop batches without manual admin action. Would you like that next?


Awesome â€” hereâ€™s a tight, drop-in **Rotation Scheduler Service** plus **cloud KMS adapters** for AWS KMS and Azure Key Vault. It plugs into the Vault trait you already have and reuses your dispatch/front-door and forensic logging.

---

# 1) Files to add

```
src/security/
  kms/
    aws_kms_vault.rs        // feature = "cloud_kms_aws"
    azure_kv_vault.rs       // feature = "cloud_kms_azure"

src/runtime/jobs/
  mod.rs
  rotation_scheduler.rs     // background rotation + re-envelop batches
```

---

# 2) Cargo features (lean + gated)

```toml
[features]
# Existing
hsm_pkcs11 = ["cryptoki"]
pqc_experimental = ["pqcrypto-kyber"]

# New cloud options (opt-in)
cloud_kms_aws   = ["aws-config", "aws-sdk-kms"]
cloud_kms_azure = ["azure_identity", "azure_security_keyvault_keys"]

# Default stays local software vault
default = []
```

---

# 3) Policy knobs (TOML)

```toml
[security.keys]
rotation_days = 180
reenvelope_batch_size = 500
purposes = ["field_default","bundle_signing","plugin_signing"]

[security.vault]
backend = "software"        # "software" | "hsm_pkcs11" | "cloud_kms_aws" | "cloud_kms_azure"

# AWS (if chosen)
[vendor.aws]
region = "us-east-1"
key_alias_map.field_default  = "alias/nodus/field-default"
key_alias_map.bundle_signing = "alias/nodus/bundle-signing"

# Azure (if chosen)
[vendor.azure]
key_vault_url = "https://my-nodus-kv.vault.azure.net/"
key_name_map.field_default  = "field-default"
key_name_map.bundle_signing = "bundle-signing"
```

**Secrets** come from env (donâ€™t hardcode):

* AWS: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (or role/IMDS)
* Azure: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET` (or managed identity)

---

# 4) Cloud KMS Vaults (adapters)

Both implement your existing `SecretVault` trait. Below are minimal skeletons.

## `aws_kms_vault.rs` (feature = cloud_kms_aws)

```rust
#[cfg(feature="cloud_kms_aws")]
use aws_sdk_kms as kms;
#[cfg(feature="cloud_kms_aws")]
use kms::types::Blob;

#[cfg(feature="cloud_kms_aws")]
pub struct AwsKmsVault {
    client: kms::Client,
    // alias map per purpose, from policy/config
    purpose_to_alias: std::collections::HashMap<String,String>,
}

#[cfg(feature="cloud_kms_aws")]
impl AwsKmsVault {
    pub async fn new(cfg: crate::policy::PolicySnapshot) -> anyhow::Result<Self> {
        let sdkcfg = aws_config::load_from_env().await;
        let client = kms::Client::new(&sdkcfg);
        Ok(Self { client, purpose_to_alias: /* hydrate from policy.vendor.aws.key_alias_map */ Default::default() })
    }
    fn alias_for(&self, purpose: &str) -> anyhow::Result<String> {
        self.purpose_to_alias.get(purpose).cloned().ok_or_else(|| anyhow::anyhow!("no alias for purpose"))
    }
}

#[cfg(feature="cloud_kms_aws")]
impl crate::security::secret_vault::SecretVault for AwsKmsVault {
    fn generate_key(&self, _purpose:&str, _alg:&str) -> anyhow::Result<crate::security::secret_vault::KeyId> {
        // In AWS KMS we usually pre-create keys/aliases; treat alias as KeyId
        Ok(crate::security::secret_vault::KeyId(self.alias_for(_purpose)?))
    }

    fn list_keys(&self, _purpose:Option<&str>) -> anyhow::Result<Vec<crate::security::secret_vault::KeyMeta>> {
        // Optional: call ListAliases/DescribeKey
        Ok(vec![])
    }

    fn rotate_key(&self, purpose:&str, _alg:&str) -> anyhow::Result<crate::security::secret_vault::KeyId> {
        // In KMS, â€œrotationâ€ is automatic for symmetric keys if enabled; here we can
        // (a) enable rotation on the CMK, or (b) create a new CMK and move alias
        // Keep it simple: assume auto-rotation; return the same alias as KeyId
        Ok(crate::security::secret_vault::KeyId(self.alias_for(purpose)?))
    }

    fn deactivate_key(&self, _key:&crate::security::secret_vault::KeyId) -> anyhow::Result<()> { Ok(()) }

    fn wrap(&self, key:&crate::security::secret_vault::KeyId, plaintext:&[u8]) -> anyhow::Result<Vec<u8>> {
        let out = futures::executor::block_on(self.client.encrypt()
            .key_id(key.0.clone())
            .plaintext(Blob::new(plaintext.to_vec()))
            .send())?;
        Ok(out.ciphertext_blob().unwrap().as_ref().to_vec())
    }

    fn unwrap(&self, _key:&crate::security::secret_vault::KeyId, blob:&[u8]) -> anyhow::Result<Vec<u8>> {
        let out = futures::executor::block_on(self.client.decrypt()
            .ciphertext_blob(Blob::new(blob.to_vec()))
            .send())?;
        Ok(out.plaintext().unwrap().as_ref().to_vec())
    }

    fn sign(&self, key:&crate::security::secret_vault::KeyId, msg:&[u8]) -> anyhow::Result<Vec<u8>> {
        let out = futures::executor::block_on(self.client.sign()
            .key_id(key.0.clone())
            .message(Blob::new(msg.to_vec()))
            .message_type(kms::types::MessageType::Raw)
            .signing_algorithm(kms::types::SigningAlgorithmSpec::RsassaPssSha256) // or from policy
            .send())?;
        Ok(out.signature().unwrap().as_ref().to_vec())
    }

    fn verify(&self, key:&crate::security::secret_vault::KeyId, msg:&[u8], sig:&[u8]) -> anyhow::Result<bool> {
        let out = futures::executor::block_on(self.client.verify()
            .key_id(key.0.clone())
            .message(Blob::new(msg.to_vec()))
            .signature(Blob::new(sig.to_vec()))
            .signing_algorithm(kms::types::SigningAlgorithmSpec::RsassaPssSha256)
            .send())?;
        Ok(out.signature_valid().unwrap_or(false))
    }
}
```

## `azure_kv_vault.rs` (feature = cloud_kms_azure)

```rust
#[cfg(feature="cloud_kms_azure")]
use azure_security_keyvault_keys::{KeyClient, KeyOperationsParameters};
#[cfg(feature="cloud_kms_azure")]
use azure_identity::DefaultAzureCredential;

#[cfg(feature="cloud_kms_azure")]
pub struct AzureKvVault {
    client: KeyClient,
    purpose_to_name: std::collections::HashMap<String,String>,
    kv_url: String,
}

#[cfg(feature="cloud_kms_azure")]
impl AzureKvVault {
    pub async fn new(cfg: crate::policy::PolicySnapshot) -> anyhow::Result<Self> {
        let cred = DefaultAzureCredential::default();
        let url = cfg.vendor.azure.key_vault_url.clone();
        let client = KeyClient::new(url.clone(), cred)?;
        Ok(Self { client, purpose_to_name: /* from policy.vendor.azure.key_name_map */ Default::default(), kv_url: url })
    }
    fn name_for(&self, purpose: &str) -> anyhow::Result<String> {
        self.purpose_to_name.get(purpose).cloned().ok_or_else(|| anyhow::anyhow!("no key name for purpose"))
    }
}

#[cfg(feature="cloud_kms_azure")]
impl crate::security::secret_vault::SecretVault for AzureKvVault {
    fn generate_key(&self, purpose:&str, _alg:&str) -> anyhow::Result<crate::security::secret_vault::KeyId> {
        Ok(crate::security::secret_vault::KeyId(self.name_for(purpose)?))
    }
    fn list_keys(&self, _purpose:Option<&str>) -> anyhow::Result<Vec<crate::security::secret_vault::KeyMeta>> { Ok(vec![]) }
    fn rotate_key(&self, purpose:&str, _alg:&str) -> anyhow::Result<crate::security::secret_vault::KeyId> {
        // Azure: create new version or rely on rotation policy; return logical name as KeyId
        Ok(crate::security::secret_vault::KeyId(self.name_for(purpose)?))
    }
    fn deactivate_key(&self, _key:&crate::security::secret_vault::KeyId) -> anyhow::Result<()> { Ok(()) }

    fn wrap(&self, key:&crate::security::secret_vault::KeyId, plaintext:&[u8]) -> anyhow::Result<Vec<u8>> {
        // Use "wrapKey" or encrypt with the key; SDK specifics omitted for brevity
        // Return wrapped bytes
        unimplemented!()
    }
    fn unwrap(&self, _key:&crate::security::secret_vault::KeyId, blob:&[u8]) -> anyhow::Result<Vec<u8>> {
        // Use "unwrapKey" / decrypt
        unimplemented!()
    }
    fn sign(&self, key:&crate::security::secret_vault::KeyId, msg:&[u8]) -> anyhow::Result<Vec<u8>> {
        // Use "sign" with chosen algorithm
        unimplemented!()
    }
    fn verify(&self, key:&crate::security::secret_vault::KeyId, msg:&[u8], sig:&[u8]) -> anyhow::Result<bool> {
        // Use "verify"
        unimplemented!()
    }
}
```

> For Azure, the exact encrypt/wrap calls vary by SDK revision; the skeleton shows where they go. Start with RSA keys (wrap/unwrap) or AES (encrypt/decrypt) depending on your policy.

---

# 5) Rotation Scheduler (background job)

## `rotation_scheduler.rs`

```rust
use std::{sync::Arc, time::Duration};
use tokio::time::interval;
use chrono::{Utc, Duration as ChronoDur};
use serde_json::json;
use uuid::Uuid;

pub struct RotationScheduler {
    pub fields_cmds: Arc<crate::dispatch::fields_commands::FieldCommands>,
    pub forensic: Arc<crate::observability::forensic_logger::ForensicLogger>,
    pub policy: fn() -> crate::policy::PolicySnapshot, // current_policy()
    pub tenant_ids: Arc<Vec<Uuid>>,                    // inject known tenants
}

impl RotationScheduler {
    pub async fn run(self: Arc<Self>) {
        let mut tick = interval(Duration::from_secs(60)); // check hourly/daily via modulo
        loop {
            tick.tick().await;
            let pol = (self.policy)();
            let today = Utc::now();
            let rotation_days = pol.sec.keys.rotation_days.max(1) as i64;

            // Lightweight cadence: only act every 24h boundary
            if today.timestamp() % 86_400 != 0 { continue; }

            for org in self.tenant_ids.iter() {
                // Swap tenant into a fake SubjectCtx with admin role (or use service principal)
                let ctx = crate::security::SubjectCtx {
                    user: "system/rotation".into(),
                    role: "security_officer".into(),
                    tenant: *org,
                    label: crate::fabric::Label { classification: "internal".into(), compartments: vec![] }
                };

                for purpose in &pol.sec.keys.purposes {
                    // Simple policy: rotate on schedule and start re-envelop
                    let payload = json!({
                        "purpose": purpose,
                        "alg": pol.sec.keys.field_alg,           // reuse alg setting
                        "reenvelope": true,
                        "batch_size": pol.sec.keys.reenvelope_batch_size
                    });

                    if let Err(e) = self.fields_cmds.rotate_keys(&ctx, &payload).await {
                        self.forensic.log(json!({
                          "evt":"keys.rotate_failed","org":org,"purpose":purpose,"err":e.to_string()
                        }));
                    } else {
                        self.forensic.log(json!({
                          "evt":"keys.rotate_scheduled","org":org,"purpose":purpose,"ts":Utc::now()
                        }));
                    }
                }
            }
        }
    }
}
```

### Start the scheduler at boot

In `production-main.rs` (or your app builder):

```rust
let scheduler = Arc::new(RotationScheduler {
  fields_cmds: field_commands.clone(),
  forensic: forensic.clone(),
  policy: crate::policy::production_policy_snapshot::current_policy,
  tenant_ids: Arc::new(tenant_repo.list_all_ids().await?),
});
tokio::spawn(scheduler.run());
```

---

# 6) Dispatch hooks (manual control)

Add two admin commands:

* `keys.rotation.start` â€” immediately kick a rotation+reenvelop cycle for a tenant/purpose.
* `keys.rotation.pause` â€” set a policy/config flag to stop the scheduler temporarily.

(Internally they can just call the same FieldCommands `rotate_keys` with your chosen payload.)

---

# 7) Runtime Vault selection

At boot, pick vault backend from policy:

```rust
let pol = current_policy();
let vault: Arc<dyn SecretVault> = match pol.vault.backend.as_str() {
  "cloud_kms_aws"   => Arc::new(AwsKmsVault::new(pol.clone()).await?),
  "cloud_kms_azure" => Arc::new(AzureKvVault::new(pol.clone()).await?),
  "hsm_pkcs11"      => Arc::new(Pkcs11Vault::new(pol.clone())?),
  _                 => Arc::new(SoftVault::new(pol.clone())?),
};
```

Now your **FieldService**, **dispatch `fields.*` commands**, and the **scheduler** all use the same Vault instance, regardless of backend.

---

# 8) Safety & Ops Notes

* **Least-privilege IAM**: AWS KMS (`kms:Encrypt`, `kms:Decrypt`, `kms:Sign`, `kms:Verify`, `kms:DescribeKey`, `kms:ListAliases` for only the needed ARNs). Azure Key Vault: data-plane permissions for keys only.
* **Multi-tenant isolation**: use a **per-tenant CMK / Key Vault key** or at least per-tenant aliases. Enforce in policy: `key_alias_map` / `key_name_map`.
* **Re-envelop throttling**: batch size + sleep between batches to avoid cost spikes.
* **Audit**: every rotation, reenvelop, decrypt already emits forensic logs.
* **Disaster recovery**: keep old keys enabled for **decrypt** until reenvelop finishes; then disable.

---

## TL;DR

* A **Rotation Scheduler** runs daily (policy-driven), calls your **existing `fields.rotate_keys`** command, and re-envelops in batches.
* **Vault backends** are now pluggable: **Software**, **HSM (PKCS#11)**, **AWS KMS**, **Azure Key Vault** â€” all behind the *same* `SecretVault` trait.
* Nothing changes in your UI or ObjectService: they keep using `FieldService`; rotation is transparent and fully audited.

If you want, I can fill in the **Azure wrap/unwrap/sign/verify** calls for your chosen key types (RSA vs AES) and add a small **end-to-end test** that rotates on AWS KMS in a mocked environment.



